{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwFeRi_OjHzB",
    "outputId": "4bfea26a-41e2-41a1-833e-7314cb8bb917"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-svj3C4JrAFi"
   },
   "source": [
    "# Preparing the data\n",
    "\n",
    "We will:\n",
    "- Load the ground truth, which contains a subset of semantic joins, a subset of syntactic joins and a sample of the rest of joins.\n",
    "- Merge it with the distances. That is, for each selected pair \"add\" the distances between the metrics of their respective profiles\n",
    "- Remove unnecessary columns for the models (e.g. dataset and attribute names)\n",
    "- Transform categorical variables into dummies\n",
    "\n",
    "**Important**: the `ground_truth_models.csv` file contains all the semantic and syntactic joins detected in the data lake + a sample of joins that do not have a relationship (i.e. containment < 0.1 and no semantic link), indicated with a *null* value in the relationships cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "error",
     "timestamp": 1753704098205,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "FZN3haMUrqTs",
    "outputId": "7aed1fd3-ce0c-4996-fcd5-a3110d5929a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of syntactic joins: 2703\n",
      "Number of semantic joins: 1701\n",
      "Number of unrelated pairs: 18206\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>containment</th>\n",
       "      <th>cardinality_proportion</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>multiset_jaccard</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22610.000000</td>\n",
       "      <td>2.261000e+04</td>\n",
       "      <td>22610.000000</td>\n",
       "      <td>22610.000000</td>\n",
       "      <td>22610.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.107225</td>\n",
       "      <td>2.154501e-01</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.004229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.242172</td>\n",
       "      <td>2.893796e-01</td>\n",
       "      <td>0.149750</td>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.033486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.251912e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.015228e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.189559e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>3.333333e-01</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.494872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        containment  cardinality_proportion       jaccard  multiset_jaccard  \\\n",
       "count  22610.000000            2.261000e+04  22610.000000      22610.000000   \n",
       "mean       0.107225            2.154501e-01      0.043204          0.010597   \n",
       "std        0.242172            2.893796e-01      0.149750          0.050149   \n",
       "min        0.000000            4.251912e-07      0.000000          0.000000   \n",
       "25%        0.000000            1.015228e-02      0.000000          0.000000   \n",
       "50%        0.000000            6.189559e-02      0.000000          0.000000   \n",
       "75%        0.045455            3.333333e-01      0.002380          0.000219   \n",
       "max        1.000000            1.000000e+00      1.000000          0.500000   \n",
       "\n",
       "            quality  \n",
       "count  22610.000000  \n",
       "mean       0.004229  \n",
       "std        0.033486  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000002  \n",
       "max        0.494872  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.read_csv('C:/Projects/freyja_repo/data/ground_truth_models.csv')\n",
    "ground_truth['relationship'] = ground_truth['relationship'].fillna('unrelated') # Pairs that are neither semantic or syntactic have a NaN. We change it by unrelated to prevent problems.\n",
    "\n",
    "count_syntactic = (ground_truth['relationship'] == 'syntactic').sum()\n",
    "count_semantic = (ground_truth['relationship'] == 'semantic').sum()\n",
    "count_unrelated = (ground_truth['relationship'] == 'unrelated').sum()\n",
    "\n",
    "print(f\"Number of syntactic joins: {count_syntactic}\")\n",
    "print(f\"Number of semantic joins: {count_semantic}\")\n",
    "print(f\"Number of unrelated pairs: {count_unrelated}\")\n",
    "\n",
    "ground_truth.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_name</th>\n",
       "      <th>att_name</th>\n",
       "      <th>ds_name_2</th>\n",
       "      <th>att_name_2</th>\n",
       "      <th>relationship</th>\n",
       "      <th>containment</th>\n",
       "      <th>cardinality_proportion</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>multiset_jaccard</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.757322</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>1.381513e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distributions_data_2016.csv</td>\n",
       "      <td>demographics</td>\n",
       "      <td>Tech_sector_diversity_demographics_2016.csv</td>\n",
       "      <td>raceEthnicity</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>1.041278e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA_cars_datasets.csv</td>\n",
       "      <td>country</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>4.659192e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World_countries_env_vars.csv</td>\n",
       "      <td>Country</td>\n",
       "      <td>world_city.csv</td>\n",
       "      <td>District</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.177892</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>6.364830e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>books_updated.csv</td>\n",
       "      <td>languageCode</td>\n",
       "      <td>countries_metadatacountries.csv</td>\n",
       "      <td>CountryCode</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>1.381148e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22605</th>\n",
       "      <td>pte_sulfo.csv</td>\n",
       "      <td>Set</td>\n",
       "      <td>AdventureWorks2014_shift.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22606</th>\n",
       "      <td>dataSpotifyClass.csv</td>\n",
       "      <td>song_title</td>\n",
       "      <td>netflix_titles.csv</td>\n",
       "      <td>description</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22607</th>\n",
       "      <td>pte_methoxy.csv</td>\n",
       "      <td>Arg0</td>\n",
       "      <td>countries_data.csv</td>\n",
       "      <td>1997</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22608</th>\n",
       "      <td>student-mat.csv</td>\n",
       "      <td>internet</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22609</th>\n",
       "      <td>ipums_la_98-small.csv</td>\n",
       "      <td>schltype</td>\n",
       "      <td>financial_order.csv</td>\n",
       "      <td>ksymbol</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>4.544962e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22610 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ds_name      att_name  \\\n",
       "0      AdventureWorks2014_stateprovince.csv          Name   \n",
       "1               Distributions_data_2016.csv  demographics   \n",
       "2                     USA_cars_datasets.csv       country   \n",
       "3              World_countries_env_vars.csv       Country   \n",
       "4                         books_updated.csv  languageCode   \n",
       "...                                     ...           ...   \n",
       "22605                         pte_sulfo.csv           Set   \n",
       "22606                  dataSpotifyClass.csv    song_title   \n",
       "22607                       pte_methoxy.csv          Arg0   \n",
       "22608                       student-mat.csv      internet   \n",
       "22609                 ipums_la_98-small.csv      schltype   \n",
       "\n",
       "                                         ds_name_2     att_name_2  \\\n",
       "0                                world_country.csv           Name   \n",
       "1      Tech_sector_diversity_demographics_2016.csv  raceEthnicity   \n",
       "2                                world_country.csv           Name   \n",
       "3                                   world_city.csv       District   \n",
       "4                  countries_metadatacountries.csv    CountryCode   \n",
       "...                                            ...            ...   \n",
       "22605                 AdventureWorks2014_shift.csv           Name   \n",
       "22606                           netflix_titles.csv    description   \n",
       "22607                           countries_data.csv           1997   \n",
       "22608         AdventureWorks2014_stateprovince.csv           Name   \n",
       "22609                          financial_order.csv        ksymbol   \n",
       "\n",
       "      relationship  containment  cardinality_proportion   jaccard  \\\n",
       "0        unrelated     0.044199                0.757322  0.019417   \n",
       "1        syntactic     0.230769                0.461538  0.187500   \n",
       "2         semantic     0.500000                0.008368  0.004167   \n",
       "3        unrelated     0.053498                0.177892  0.006250   \n",
       "4        syntactic     0.360000                0.101215  0.034091   \n",
       "...            ...          ...                     ...       ...   \n",
       "22605    unrelated     0.000000                0.120000  0.000000   \n",
       "22606    unrelated     0.000000                0.313414  0.000000   \n",
       "22607    unrelated     0.000000                0.004015  0.000000   \n",
       "22608    unrelated     0.000000                0.011050  0.000000   \n",
       "22609    unrelated     0.000000                0.750000  0.125000   \n",
       "\n",
       "       multiset_jaccard       quality  \n",
       "0              0.019048  1.381513e-03  \n",
       "1              0.000186  1.041278e-05  \n",
       "2              0.000365  4.659192e-07  \n",
       "3              0.002314  6.364830e-05  \n",
       "4              0.000878  1.381148e-05  \n",
       "...                 ...           ...  \n",
       "22605          0.000000  0.000000e+00  \n",
       "22606          0.000000  0.000000e+00  \n",
       "22607          0.000000  0.000000e+00  \n",
       "22608          0.000000  0.000000e+00  \n",
       "22609          0.000072  4.544962e-06  \n",
       "\n",
       "[22610 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Projects/freyja_repo/data/distances.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m distances\n",
      "File \u001b[1;32mc:\\Users\\marcm\\miniconda3\\envs\\freyja_paper\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marcm\\miniconda3\\envs\\freyja_paper\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marcm\\miniconda3\\envs\\freyja_paper\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\marcm\\miniconda3\\envs\\freyja_paper\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "distances = pd.read_csv('C:/Projects/freyja_repo/data/distances.csv')\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 19450,
     "status": "ok",
     "timestamp": 1736505249702,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "iXd8o58DTDUz",
    "outputId": "6edf9e8f-797c-4c1c-d4ae-477c74f60857"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_name</th>\n",
       "      <th>att_name</th>\n",
       "      <th>ds_name_2</th>\n",
       "      <th>att_name_2</th>\n",
       "      <th>relationship</th>\n",
       "      <th>containment</th>\n",
       "      <th>cardinality_proportion</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>multiset_jaccard</th>\n",
       "      <th>quality</th>\n",
       "      <th>...</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>pct_date_time</th>\n",
       "      <th>uniqueness</th>\n",
       "      <th>frequency_5qo</th>\n",
       "      <th>frequency_7qo</th>\n",
       "      <th>words_cnt_min</th>\n",
       "      <th>words_cnt_avg</th>\n",
       "      <th>frequency_sd</th>\n",
       "      <th>pct_phones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.757322</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>1.381513e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>-0.071857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.371837</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distributions_data_2016.csv</td>\n",
       "      <td>demographics</td>\n",
       "      <td>Tech_sector_diversity_demographics_2016.csv</td>\n",
       "      <td>raceEthnicity</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>1.041278e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>Distributions_data_2016.csv</td>\n",
       "      <td>0.143498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.135553</td>\n",
       "      <td>-0.104895</td>\n",
       "      <td>-0.104895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.294709</td>\n",
       "      <td>-0.389624</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA_cars_datasets.csv</td>\n",
       "      <td>country</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>4.659192e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>USA_cars_datasets.csv</td>\n",
       "      <td>-1.538034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999200</td>\n",
       "      <td>0.993015</td>\n",
       "      <td>0.993015</td>\n",
       "      <td>-0.409673</td>\n",
       "      <td>-0.830778</td>\n",
       "      <td>3.014091</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books_updated.csv</td>\n",
       "      <td>languageCode</td>\n",
       "      <td>countries_metadatacountries.csv</td>\n",
       "      <td>CountryCode</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>1.381148e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>books_updated.csv</td>\n",
       "      <td>-1.979207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.997500</td>\n",
       "      <td>-0.003349</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.363103</td>\n",
       "      <td>2.702889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cars.csv</td>\n",
       "      <td>color</td>\n",
       "      <td>colors.csv</td>\n",
       "      <td>name</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>3.200301e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>cars.csv</td>\n",
       "      <td>-0.860310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999689</td>\n",
       "      <td>0.089943</td>\n",
       "      <td>0.170423</td>\n",
       "      <td>-0.182574</td>\n",
       "      <td>-1.696849</td>\n",
       "      <td>-0.202640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22605</th>\n",
       "      <td>us_companies.csv</td>\n",
       "      <td>fulltimeemployees</td>\n",
       "      <td>genes_interactions.csv</td>\n",
       "      <td>Type</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>genes_interactions.csv</td>\n",
       "      <td>-0.478577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.013717</td>\n",
       "      <td>0.215019</td>\n",
       "      <td>0.385905</td>\n",
       "      <td>0.311248</td>\n",
       "      <td>0.456492</td>\n",
       "      <td>1.088152</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22606</th>\n",
       "      <td>pte_sulfo.csv</td>\n",
       "      <td>Set</td>\n",
       "      <td>AdventureWorks2014_shift.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>AdventureWorks2014_shift.csv</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22607</th>\n",
       "      <td>pte_methoxy.csv</td>\n",
       "      <td>Arg0</td>\n",
       "      <td>countries_data.csv</td>\n",
       "      <td>1997</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>countries_data.csv</td>\n",
       "      <td>1.633221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.148744</td>\n",
       "      <td>-0.033283</td>\n",
       "      <td>-0.033283</td>\n",
       "      <td>-0.133631</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>-1.312925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22608</th>\n",
       "      <td>student-mat.csv</td>\n",
       "      <td>internet</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>1.290401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994937</td>\n",
       "      <td>-0.827387</td>\n",
       "      <td>-0.827387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.788854</td>\n",
       "      <td>-1.897268</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22609</th>\n",
       "      <td>ipums_la_98-small.csv</td>\n",
       "      <td>schltype</td>\n",
       "      <td>financial_order.csv</td>\n",
       "      <td>ksymbol</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>4.544962e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>financial_order.csv</td>\n",
       "      <td>-0.921915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.077803</td>\n",
       "      <td>0.300168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.430816</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22610 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ds_name           att_name  \\\n",
       "0      AdventureWorks2014_stateprovince.csv               Name   \n",
       "1               Distributions_data_2016.csv       demographics   \n",
       "2                     USA_cars_datasets.csv            country   \n",
       "3                         books_updated.csv       languageCode   \n",
       "4                                  cars.csv              color   \n",
       "...                                     ...                ...   \n",
       "22605                      us_companies.csv  fulltimeemployees   \n",
       "22606                         pte_sulfo.csv                Set   \n",
       "22607                       pte_methoxy.csv               Arg0   \n",
       "22608                       student-mat.csv           internet   \n",
       "22609                 ipums_la_98-small.csv           schltype   \n",
       "\n",
       "                                         ds_name_2     att_name_2  \\\n",
       "0                                world_country.csv           Name   \n",
       "1      Tech_sector_diversity_demographics_2016.csv  raceEthnicity   \n",
       "2                                world_country.csv           Name   \n",
       "3                  countries_metadatacountries.csv    CountryCode   \n",
       "4                                       colors.csv           name   \n",
       "...                                            ...            ...   \n",
       "22605                       genes_interactions.csv           Type   \n",
       "22606                 AdventureWorks2014_shift.csv           Name   \n",
       "22607                           countries_data.csv           1997   \n",
       "22608         AdventureWorks2014_stateprovince.csv           Name   \n",
       "22609                          financial_order.csv        ksymbol   \n",
       "\n",
       "      relationship  containment  cardinality_proportion   jaccard  \\\n",
       "0        unrelated     0.044199                0.757322  0.019417   \n",
       "1        syntactic     0.230769                0.461538  0.187500   \n",
       "2         semantic     0.500000                0.008368  0.004167   \n",
       "3        syntactic     0.360000                0.101215  0.034091   \n",
       "4         semantic     0.750000                0.088889  0.065217   \n",
       "...            ...          ...                     ...       ...   \n",
       "22605    unrelated     0.000000                0.333333  0.000000   \n",
       "22606    unrelated     0.000000                0.120000  0.000000   \n",
       "22607    unrelated     0.000000                0.004015  0.000000   \n",
       "22608    unrelated     0.000000                0.011050  0.000000   \n",
       "22609    unrelated     0.000000                0.750000  0.125000   \n",
       "\n",
       "       multiset_jaccard       quality  ...  \\\n",
       "0              0.019048  1.381513e-03  ...   \n",
       "1              0.000186  1.041278e-05  ...   \n",
       "2              0.000365  4.659192e-07  ...   \n",
       "3              0.000878  1.381148e-05  ...   \n",
       "4              0.000233  3.200301e-06  ...   \n",
       "...                 ...           ...  ...   \n",
       "22605          0.000000  0.000000e+00  ...   \n",
       "22606          0.000000  0.000000e+00  ...   \n",
       "22607          0.000000  0.000000e+00  ...   \n",
       "22608          0.000000  0.000000e+00  ...   \n",
       "22609          0.000072  4.544962e-06  ...   \n",
       "\n",
       "                               dataset_name  cardinality  pct_date_time  \\\n",
       "0      AdventureWorks2014_stateprovince.csv    -0.071857            0.0   \n",
       "1               Distributions_data_2016.csv     0.143498            0.0   \n",
       "2                     USA_cars_datasets.csv    -1.538034            0.0   \n",
       "3                         books_updated.csv    -1.979207            0.0   \n",
       "4                                  cars.csv    -0.860310            0.0   \n",
       "...                                     ...          ...            ...   \n",
       "22605                genes_interactions.csv    -0.478577            0.0   \n",
       "22606          AdventureWorks2014_shift.csv    -0.707107            0.0   \n",
       "22607                    countries_data.csv     1.633221            0.0   \n",
       "22608  AdventureWorks2014_stateprovince.csv     1.290401            0.0   \n",
       "22609                   financial_order.csv    -0.921915            0.0   \n",
       "\n",
       "      uniqueness  frequency_5qo  frequency_7qo words_cnt_min  words_cnt_avg  \\\n",
       "0       0.000000       0.001341       0.001341      0.000000       1.371837   \n",
       "1      -0.135553      -0.104895      -0.104895      0.000000      -1.294709   \n",
       "2      -0.999200       0.993015       0.993015     -0.409673      -0.830778   \n",
       "3      -0.997500      -0.003349       0.002351      0.000000      -0.363103   \n",
       "4      -0.999689       0.089943       0.170423     -0.182574      -1.696849   \n",
       "...          ...            ...            ...           ...            ...   \n",
       "22605  -0.013717       0.215019       0.385905      0.311248       0.456492   \n",
       "22606   0.000000       0.293333       0.293333      0.000000       0.000000   \n",
       "22607  -0.148744      -0.033283      -0.033283     -0.133631      -0.147237   \n",
       "22608   0.994937      -0.827387      -0.827387      0.000000       1.788854   \n",
       "22609   0.000217       0.077803       0.300168      0.000000       0.000000   \n",
       "\n",
       "       frequency_sd  pct_phones  \n",
       "0          0.004977         0.0  \n",
       "1         -0.389624         0.0  \n",
       "2          3.014091         0.0  \n",
       "3          2.702889         0.0  \n",
       "4         -0.202640         0.0  \n",
       "...             ...         ...  \n",
       "22605      1.088152         0.0  \n",
       "22606      0.707107         0.0  \n",
       "22607     -1.312925         0.0  \n",
       "22608     -1.897268         0.0  \n",
       "22609      2.430816         0.0  \n",
       "\n",
       "[22610 rows x 74 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = pd.merge(ground_truth, distances, left_on=['ds_name', 'ds_name_2', 'att_name', 'att_name_2'], right_on=['dataset_name', 'dataset_name_2', 'attribute_name', 'attribute_name_2'])\n",
    "joined_2 = pd.merge(ground_truth, distances, left_on=['ds_name', 'ds_name_2', 'att_name', 'att_name_2'], right_on=['dataset_name_2', 'dataset_name', 'attribute_name_2', 'attribute_name'])\n",
    "\n",
    "merged = pd.concat([joined, joined_2], ignore_index=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1736505249702,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "QK2djoYXzHGg",
    "outputId": "9e06a7cb-e845-425b-f520-bcddf3c1a12e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ds_name', 'att_name', 'ds_name_2', 'att_name_2', 'relationship',\n",
      "       'containment', 'cardinality_proportion', 'jaccard', 'multiset_jaccard',\n",
      "       'quality', 'pct_alphabetic', 'pct_time', 'len_avg_word',\n",
      "       'dataset_name_2', 'pct_email', 'is_empty_2', 'attribute_name_2',\n",
      "       'pct_username', 'val_pct_max', 'pct_ip', 'pct_others', 'last_word',\n",
      "       'constancy', 'pct_alphanumeric', 'len_max_word', 'frequency_iqr',\n",
      "       'pct_date', 'datatype_2', 'cardinalityRaw', 'frequency_2qo', 'pct_url',\n",
      "       'frequency_avg', 'frequency_min', 'frequency_4qo', 'val_pct_min',\n",
      "       'frequency_6qo', 'pct_general', 'freq_word_containment', 'number_words',\n",
      "       'cardinalityRaw_2', 'val_pct_std', 'freq_word_soundex_containment',\n",
      "       'words_cnt_sd', 'frequency_max', 'K', 'pct_phrases', 'first_word',\n",
      "       'entropy', 'pct_date_time_specific', 'datatype', 'len_min_word',\n",
      "       'attribute_name', 'pct_non_alphanumeric', 'is_empty', 'name_dist',\n",
      "       'pct_unknown', 'pct_numeric', 'is_binary', 'frequency_1qo',\n",
      "       'specific_type', 'specific_type_2', 'words_cnt_max', 'incompleteness',\n",
      "       'frequency_3qo', 'dataset_name', 'cardinality', 'pct_date_time',\n",
      "       'uniqueness', 'frequency_5qo', 'frequency_7qo', 'words_cnt_min',\n",
      "       'words_cnt_avg', 'frequency_sd', 'pct_phones'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged.columns)\n",
    "\n",
    "merged.drop(['ds_name', 'ds_name_2', 'att_name', 'att_name_2', \n",
    "             'relationship', 'containment', 'cardinality_proportion', 'jaccard', 'multiset_jaccard', \n",
    "             'dataset_name', 'attribute_name', 'dataset_name_2', 'attribute_name_2', \n",
    "             'cardinalityRaw','cardinalityRaw_2', 'K'],  \n",
    "             axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45MyaIh-05s8"
   },
   "outputs": [],
   "source": [
    "# 2 merged, one with the dummies of the datatypes (merged_all), another without them (merged_no_dummies)\n",
    "merged_all = merged\n",
    "merged_all = pd.concat([merged_all.drop('datatype', axis=1), pd.get_dummies(merged_all['datatype'], prefix='datatype_', dtype=int)], axis=1)\n",
    "merged_all = pd.concat([merged_all.drop('datatype_2', axis=1), pd.get_dummies(merged_all['datatype_2'], prefix='datatype_2_', dtype=int)], axis=1)\n",
    "merged_all = pd.concat([merged_all.drop('specific_type', axis=1), pd.get_dummies(merged_all['specific_type'], prefix='specific_type_', dtype=int)], axis=1)\n",
    "merged_all = pd.concat([merged_all.drop('specific_type_2', axis=1), pd.get_dummies(merged_all['specific_type_2'], prefix='specific_type_2_', dtype=int)], axis=1)\n",
    "\n",
    "merged_no_dummies = merged.drop(['datatype', \"datatype_2\", \"specific_type\", \"specific_type_2\"],  axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-hEqpoxmPwp"
   },
   "source": [
    "# Model selection\n",
    "\n",
    "Our goal is to define the best regressor model that can approximate the true value of the joinability metric defined (MJ & K) by using profiles.\n",
    "\n",
    "We define four base models to do so, whose metrics vary. The first point of variation is the inclusion of \"datatypes\" metrics (i.e. semantic types/characteristics of each column: names, URIs, etc. / alphabetical, numerical etc. ). These metrics are the most time consuming to compute, which implies that the already lightweight profile-based approach can be made much faster. The second point of variation is the execution (or lack thereof) of feature selection tasks, which further reduce the number of features while, ideally, keeping, or improving, the evaluation scores.\n",
    "\n",
    "**Result**: for all models, the best performing regressor has been the Random Forest. Nonetheless, further testing with the benchmarks has shown that the Gradient Booster predictor (with no fine-tuning) works best. This might be due to overfitting produced by the Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWZf8rrtp6oH"
   },
   "source": [
    "### Model evaluation methodology\n",
    "\n",
    "We want to define a regression model. To do so, we will employ 12 base regressors (listed below) evaluated over a 10-split CV (test size = 30%) and 4 different metrics, of which we will primarily focus on the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSdayOnY9GuS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "split = ShuffleSplit(n_splits=10, test_size=0.3, random_state=211199)\n",
    "\n",
    "def scoring(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return {\"R2\": r2_score(y, y_pred),\n",
    "            \"MAE\": mean_absolute_error(y, y_pred),\n",
    "            \"RMSE\": math.sqrt(mean_squared_error(y, y_pred)),\n",
    "            \"MedAE\": median_absolute_error(y, y_pred),}\n",
    "\n",
    "names = [\n",
    "    \"Linear Regression\",\n",
    "    \"Ridge Regression\",\n",
    "    \"Lasso Regression\",\n",
    "    \"ElasticNet Regression\",\n",
    "    \"Random Forests\",\n",
    "    \"Gradient Boosting\",\n",
    "    \"AdaBoost\",\n",
    "    \"Extra Trees\",\n",
    "    \"Histogram Gradient Boosting\",\n",
    "    \"XGBoosting\",\n",
    "    \"Light GBM\",\n",
    "    \"CatBoost\",\n",
    "    \"MLP Relu\",\n",
    "    \"MLP logistic\",\n",
    "    \"MLP Tanh\",\n",
    "    \"SVR Poly\",\n",
    "    \"SVR Rbf\"\n",
    "]\n",
    "\n",
    "regressors = [\n",
    "    LinearRegression(),\n",
    "    Ridge(random_state=211199),\n",
    "    Lasso(random_state=211199),\n",
    "    ElasticNet(random_state=211199),\n",
    "    RandomForestRegressor(random_state=211199, n_estimators=50),\n",
    "    GradientBoostingRegressor(random_state=211199, n_estimators=50),\n",
    "    AdaBoostRegressor(random_state=211199, n_estimators=50),\n",
    "    ExtraTreesRegressor(random_state=211199, n_estimators=50),\n",
    "    HistGradientBoostingRegressor(random_state=211199),\n",
    "    XGBRegressor(random_state=211199, n_estimators=50),\n",
    "    LGBMRegressor(random_state=211199, n_estimators=50, verbose=0),\n",
    "    CatBoostRegressor(random_state=211199, verbose=0),\n",
    "    MLPRegressor(random_state=211199, activation = 'relu'),\n",
    "    MLPRegressor(random_state=211199, activation = 'logistic'),\n",
    "    MLPRegressor(random_state=211199, activation = 'tanh'),\n",
    "    SVR(kernel=\"poly\"),\n",
    "    SVR(kernel=\"rbf\"),\n",
    "]\n",
    "\n",
    "def test_list_of_regressors(predictors, target, names, regressors):\n",
    "  for name, regressor in zip(names, regressors):\n",
    "    print(name)\n",
    "    test_regressor(regressor, predictors, target)\n",
    "\n",
    "def test_regressor(regressor, predictors, target):\n",
    "  scores = cross_validate(regressor, predictors, target, cv=split, scoring=scoring, verbose=0)\n",
    "  print(f\"Fit time: {scores['fit_time'].mean():.6f} | \"\n",
    "      f\"Score time: {scores['score_time'].mean():.6f}\")\n",
    "\n",
    "  print(f\"R2: {scores['test_R2'].mean():.6f} | \"\n",
    "      f\"MAE: {scores['test_MAE'].mean():.6f} | \"\n",
    "      f\"RMSE: {scores['test_RMSE'].mean():.6f} | \"\n",
    "      f\"MedAE: {scores['test_MedAE'].mean():.6f}\")\n",
    "  print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_names = [\n",
    "        \"gradient_boosting\",\n",
    "        \"extra_trees\",\n",
    "        \"xgboosting\",\n",
    "        \"catboost\"\n",
    "    ]\n",
    "\n",
    "best_regressors = [\n",
    "    GradientBoostingRegressor(random_state=211199, n_estimators=50),\n",
    "    ExtraTreesRegressor(random_state=211199, n_estimators=50),\n",
    "    XGBRegressor(random_state=211199, n_estimators=50),\n",
    "    CatBoostRegressor(random_state=211199, verbose=0),\n",
    "]\n",
    "\n",
    "models_path = Path(f'C:/Projects/freyja_repo/data/models/')\n",
    "\n",
    "def store_models(target, predictors, model_typology):\n",
    "    for name, regressor in tqdm(zip(best_names, best_regressors), total=len(best_names)):\n",
    "        regressor.fit(predictors, target)\n",
    "        folder_path = models_path / model_typology\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(regressor, folder_path / f\"{name}_{model_typology}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvxm9qE3qJGI"
   },
   "source": [
    "### Model 1: All metrics\n",
    "\n",
    "Model that includes all metrics (likely to overfit and contain redundancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 884473,
     "status": "ok",
     "timestamp": 1719936430793,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "65h-CwQxEtkh",
    "outputId": "5ad7db70-3b28-486e-b966-da5e0ff13482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Fit time: 0.046146 | Score time: 0.004562\n",
      "R2: 0.238525 | MAE: 0.009050 | RMSE: 0.029562 | MedAE: 0.003237\n",
      "------------------------------\n",
      "Ridge Regression\n",
      "Fit time: 0.020905 | Score time: 0.004700\n",
      "R2: 0.238603 | MAE: 0.009024 | RMSE: 0.029561 | MedAE: 0.003208\n",
      "------------------------------\n",
      "Lasso Regression\n",
      "Fit time: 0.021186 | Score time: 0.004564\n",
      "R2: -0.000082 | MAE: 0.007977 | RMSE: 0.033888 | MedAE: 0.004208\n",
      "------------------------------\n",
      "ElasticNet Regression\n",
      "Fit time: 0.025123 | Score time: 0.004704\n",
      "R2: -0.000082 | MAE: 0.007977 | RMSE: 0.033888 | MedAE: 0.004208\n",
      "------------------------------\n",
      "Random Forests\n",
      "Fit time: 37.921386 | Score time: 0.056000\n",
      "R2: 0.908023 | MAE: 0.001247 | RMSE: 0.009982 | MedAE: 0.000001\n",
      "------------------------------\n",
      "Gradient Boosting\n",
      "Fit time: 6.471234 | Score time: 0.008255\n",
      "R2: 0.769585 | MAE: 0.002931 | RMSE: 0.016195 | MedAE: 0.000048\n",
      "------------------------------\n",
      "AdaBoost\n",
      "Fit time: 1.352709 | Score time: 0.034578\n",
      "R2: 0.627723 | MAE: 0.006443 | RMSE: 0.020509 | MedAE: 0.002111\n",
      "------------------------------\n",
      "Extra Trees\n",
      "Fit time: 5.041172 | Score time: 0.057152\n",
      "R2: 0.910918 | MAE: 0.000746 | RMSE: 0.009650 | MedAE: 0.000000\n",
      "------------------------------\n",
      "Histogram Gradient Boosting\n",
      "Fit time: 0.323675 | Score time: 0.011601\n",
      "R2: 0.889177 | MAE: 0.001496 | RMSE: 0.011020 | MedAE: 0.000046\n",
      "------------------------------\n",
      "XGBoosting\n",
      "Fit time: 0.178690 | Score time: 0.023117\n",
      "R2: 0.923968 | MAE: 0.001054 | RMSE: 0.008856 | MedAE: 0.000058\n",
      "------------------------------\n",
      "Light GBM\n",
      "Fit time: 0.101294 | Score time: 0.006799\n",
      "R2: 0.888351 | MAE: 0.001597 | RMSE: 0.011105 | MedAE: 0.000059\n",
      "------------------------------\n",
      "CatBoost\n",
      "Fit time: 3.730366 | Score time: 0.007252\n",
      "R2: 0.935972 | MAE: 0.001220 | RMSE: 0.008206 | MedAE: 0.000239\n",
      "------------------------------\n",
      "MLP Relu\n",
      "Fit time: 2.231105 | Score time: 0.006738\n",
      "R2: -111.709678 | MAE: 0.082438 | RMSE: 0.238994 | MedAE: 0.045153\n",
      "------------------------------\n",
      "MLP logistic\n",
      "Fit time: 1.593536 | Score time: 0.031899\n",
      "R2: 0.256685 | MAE: 0.011557 | RMSE: 0.029207 | MedAE: 0.006165\n",
      "------------------------------\n",
      "MLP Tanh\n",
      "Fit time: 1.169218 | Score time: 0.010756\n",
      "R2: 0.015939 | MAE: 0.020915 | RMSE: 0.033536 | MedAE: 0.014561\n",
      "------------------------------\n",
      "SVR Poly\n",
      "Fit time: 0.425491 | Score time: 0.040745\n",
      "R2: -7.929163 | MAE: 0.099229 | RMSE: 0.101006 | MedAE: 0.100050\n",
      "------------------------------\n",
      "SVR Rbf\n",
      "Fit time: 0.324277 | Score time: 0.146695\n",
      "R2: -5.354481 | MAE: 0.081794 | RMSE: 0.085169 | MedAE: 0.085150\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_MJ = merged_all['quality']\n",
    "predictors = merged_all.drop(columns=['quality'], axis=1)\n",
    "\n",
    "test_list_of_regressors(predictors, y_MJ, names, regressors)\n",
    "\n",
    "# Gradient Boosting -> 0.016195\n",
    "# Extra Trees       -> 0.009650\n",
    "# XGBoosting        -> 0.008856\n",
    "# CatBoost          -> 0.008206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24557,
     "status": "ok",
     "timestamp": 1736505300961,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "UoOTDQT3RCD8",
    "outputId": "0c7de7ff-81e0-4ed7-9a5d-7078fdcaed1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:20<00:00,  5.16s/it]\n"
     ]
    }
   ],
   "source": [
    "target = merged_all['quality']\n",
    "predictors = merged_all.drop(columns=['quality'], axis=1)\n",
    "\n",
    "store_models(target, predictors, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2PKzyskqn8O"
   },
   "source": [
    "### Model 2: All metrics + Feature Selection\n",
    "\n",
    "Model 1 + feature selection process to reduce overfitting and redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1736505312623,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "sYV37nOWt30H",
    "outputId": "00ad5623-af82-483e-9c2c-bf8b143a942d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features for gradient_boosting -> Original = 81, new = 39\n",
      "gradient_boosting\n",
      "Fit time: 4.815652 | Score time: 0.006177\n",
      "R2: 0.772057 | MAE: 0.002926 | RMSE: 0.016114 | MedAE: 0.000055\n",
      "------------------------------\n",
      "Number of features for extra_trees -> Original = 81, new = 54\n",
      "extra_trees\n",
      "Fit time: 4.250001 | Score time: 0.055315\n",
      "R2: 0.919671 | MAE: 0.000720 | RMSE: 0.009142 | MedAE: 0.000000\n",
      "------------------------------\n",
      "Number of features for xgboosting -> Original = 81, new = 50\n",
      "xgboosting\n",
      "Fit time: 0.137683 | Score time: 0.016001\n",
      "R2: 0.926385 | MAE: 0.001030 | RMSE: 0.008714 | MedAE: 0.000054\n",
      "------------------------------\n",
      "Number of features for catboost -> Original = 81, new = 58\n",
      "catboost\n",
      "Fit time: 3.690403 | Score time: 0.006600\n",
      "R2: 0.938836 | MAE: 0.001200 | RMSE: 0.007993 | MedAE: 0.000238\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, regressor in zip(best_names, best_regressors):\n",
    "  model_all = joblib.load(models_path / \"all\" / f\"{name}_all.pkl\")\n",
    "\n",
    "  feature_importances = model_all.feature_importances_\n",
    "  predictors = merged_all.drop(columns=['quality'], axis=1)\n",
    "\n",
    "  # Match feature importances with corresponding feature names\n",
    "  feature_names = list(predictors.columns)\n",
    "  feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "  # Sort the feature importances in descending order\n",
    "  sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  selected_metrics_fs_all = []\n",
    "  for feature, importance in sorted_feature_importances:\n",
    "    if importance > 0.0001:\n",
    "      selected_metrics_fs_all.append(feature)\n",
    "\n",
    "  target = merged_all['quality']\n",
    "  predictors = merged_all[selected_metrics_fs_all]\n",
    "  print(f\"Number of features for {name} -> Original = {len(feature_names)}, new = {len(selected_metrics_fs_all)}\")\n",
    "  test_list_of_regressors(predictors, target, [name], [regressor])\n",
    "\n",
    "  # selected_metrics_fs_all = ['freq_word_containment', 'frequency_7qo', 'uniqueness', 'frequency_min', 'first_word', 'frequency_4qo', 'constancy', 'name_dist', 'frequency_iqr',\n",
    "  #                     'specific_type_2__pct_username', 'words_cnt_min', 'pct_alphabetic', 'freq_word_soundex_containment', 'len_max_word', 'pct_username', 'last_word',\n",
    "  #                     'frequency_max', 'pct_general', 'specific_type__pct_username', 'datatype_2__pct_alphanumeric', 'val_pct_min', 'frequency_2qo', 'len_min_word',\n",
    "  #                     'pct_phrases', 'frequency_sd', 'words_cnt_max', 'val_pct_max', 'frequency_avg', 'len_avg_word', 'frequency_6qo', 'cardinality', 'frequency_3qo',\n",
    "  #                     'frequency_5qo', 'incompleteness', 'datatype_2__pct_alphabetic', 'pct_unknown', 'specific_type_2__pct_phrases', 'datatype__pct_unknown', 'pct_non_alphanumeric']\n",
    "\n",
    "  model = regressor.fit(predictors, target)\n",
    "  folder_path = models_path / \"all_fs\"\n",
    "  folder_path.mkdir(parents=True, exist_ok=True)\n",
    "  joblib.dump(regressor, folder_path / f\"{name}_all_fs.pkl\")\n",
    "\n",
    "\n",
    "# Gradient Boosting -> from 0.016195 to 0.016114, 81 to 39 features\n",
    "# Extra Trees       -> from 0.009650 to 0.009142, 81 to 54 features\n",
    "# XGBoosting        -> from 0.008856 to 0.008714, 81 to 50 features\n",
    "# CatBoost          -> from 0.008206 to 0.007993, 81 to 58 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRw306YMr59N"
   },
   "source": [
    "### Model 3: No datatypes (i.e. no datatypes, specific datatypes or datatype percentages)\n",
    "\n",
    "Removal of \"datatypes\" metrics, which are highly costly and their contribution to the predictions of the model is not ensured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7519,
     "status": "error",
     "timestamp": 1736506086619,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "ex5dVXuCvzsy",
    "outputId": "31e047db-3f5d-494c-c863-8e8b44528911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Fit time: 0.014426 | Score time: 0.003031\n",
      "R2: 0.230721 | MAE: 0.008271 | RMSE: 0.029715 | MedAE: 0.002194\n",
      "------------------------------\n",
      "Ridge Regression\n",
      "Fit time: 0.009501 | Score time: 0.003205\n",
      "R2: 0.230752 | MAE: 0.008253 | RMSE: 0.029714 | MedAE: 0.002175\n",
      "------------------------------\n",
      "Lasso Regression\n",
      "Fit time: 0.009896 | Score time: 0.003100\n",
      "R2: -0.000082 | MAE: 0.007977 | RMSE: 0.033888 | MedAE: 0.004208\n",
      "------------------------------\n",
      "ElasticNet Regression\n",
      "Fit time: 0.010391 | Score time: 0.003209\n",
      "R2: -0.000082 | MAE: 0.007977 | RMSE: 0.033888 | MedAE: 0.004208\n",
      "------------------------------\n",
      "Random Forests\n",
      "Fit time: 28.625246 | Score time: 0.052337\n",
      "R2: 0.915626 | MAE: 0.001210 | RMSE: 0.009588 | MedAE: 0.000001\n",
      "------------------------------\n",
      "Gradient Boosting\n",
      "Fit time: 5.295446 | Score time: 0.005917\n",
      "R2: 0.767448 | MAE: 0.002973 | RMSE: 0.016299 | MedAE: 0.000062\n",
      "------------------------------\n",
      "AdaBoost\n",
      "Fit time: 0.912972 | Score time: 0.015812\n",
      "R2: 0.647918 | MAE: 0.005539 | RMSE: 0.020040 | MedAE: 0.001255\n",
      "------------------------------\n",
      "Extra Trees\n",
      "Fit time: 3.112378 | Score time: 0.054575\n",
      "R2: 0.930103 | MAE: 0.000688 | RMSE: 0.008569 | MedAE: 0.000000\n",
      "------------------------------\n",
      "Histogram Gradient Boosting\n",
      "Fit time: 0.231634 | Score time: 0.010201\n",
      "R2: 0.897959 | MAE: 0.001512 | RMSE: 0.010658 | MedAE: 0.000047\n",
      "------------------------------\n",
      "XGBoosting\n",
      "Fit time: 0.126117 | Score time: 0.011499\n",
      "R2: 0.936776 | MAE: 0.001056 | RMSE: 0.008252 | MedAE: 0.000057\n",
      "------------------------------\n",
      "Light GBM\n",
      "Fit time: 0.065062 | Score time: 0.005200\n",
      "R2: 0.897935 | MAE: 0.001586 | RMSE: 0.010689 | MedAE: 0.000064\n",
      "------------------------------\n",
      "CatBoost\n",
      "Fit time: 3.229941 | Score time: 0.006200\n",
      "R2: 0.941062 | MAE: 0.001204 | RMSE: 0.008011 | MedAE: 0.000221\n",
      "------------------------------\n",
      "MLP Relu\n",
      "Fit time: 1.610571 | Score time: 0.005900\n",
      "R2: -20.102407 | MAE: 0.065631 | RMSE: 0.133085 | MedAE: 0.042988\n",
      "------------------------------\n",
      "MLP logistic\n",
      "Fit time: 1.276452 | Score time: 0.030719\n",
      "R2: 0.159309 | MAE: 0.015402 | RMSE: 0.030966 | MedAE: 0.010478\n",
      "------------------------------\n",
      "MLP Tanh\n",
      "Fit time: 1.104087 | Score time: 0.009900\n",
      "R2: -0.216745 | MAE: 0.023330 | RMSE: 0.037309 | MedAE: 0.015779\n",
      "------------------------------\n",
      "SVR Poly\n",
      "Fit time: 0.334913 | Score time: 0.032223\n",
      "R2: -7.925377 | MAE: 0.099203 | RMSE: 0.100986 | MedAE: 0.100028\n",
      "------------------------------\n",
      "SVR Rbf\n",
      "Fit time: 0.234396 | Score time: 0.122674\n",
      "R2: -5.769220 | MAE: 0.085092 | RMSE: 0.087885 | MedAE: 0.087800\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "quality = merged_no_dummies['quality']\n",
    "predictors = merged_no_dummies.drop(columns=['quality', \"pct_numeric\", \"pct_alphanumeric\", \"pct_alphabetic\", \"pct_non_alphanumeric\", \"pct_date_time\", \"pct_unknown\",\n",
    "                                      \"pct_phones\", \"pct_email\", \"pct_url\", \"pct_ip\", \"pct_username\", \"pct_phrases\", \"pct_general\", \"pct_date\", \"pct_time\",\n",
    "                                      \"pct_date_time_specific\", \"pct_others\"], axis=1)\n",
    "\n",
    "test_list_of_regressors(predictors, quality, names, regressors)\n",
    "\n",
    "# Gradient Boosting -> models 1-2 -> from 0.016195 to 0.016114, 81 to 39 features\n",
    "#                   -> model 3         -> 0.016299, 53 features\n",
    "# Extra Trees       -> models 1-2 -> from 0.009650 to 0.009142, 81 to 54 features\n",
    "#                   -> model 3         -> 0.008569, 53 features\n",
    "# XGBoosting        -> models 1-2 -> from 0.008856 to 0.008714, 81 to 50 features\n",
    "#                   -> model 3         -> 0.008252, 53 features\n",
    "# CatBoost          -> models 1-2 -> from 0.008206 to 0.007993, 81 to 58 features\n",
    "#                   -> model 3         -> 0.008011, 53 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28442,
     "status": "ok",
     "timestamp": 1736506126822,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "CblWuM9SHd_b",
    "outputId": "36025157-2683-4aef-ae3b-5a69d5b30556"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:15<00:00,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "target = merged_no_dummies['quality']\n",
    "predictors = merged_no_dummies.drop(columns=['quality', \"pct_numeric\", \"pct_alphanumeric\", \"pct_alphabetic\", \"pct_non_alphanumeric\", \"pct_date_time\", \"pct_unknown\",\n",
    "                                      \"pct_phones\", \"pct_email\", \"pct_url\", \"pct_ip\", \"pct_username\", \"pct_phrases\", \"pct_general\", \"pct_date\", \"pct_time\",\n",
    "                                      \"pct_date_time_specific\", \"pct_others\"], axis=1)\n",
    "\n",
    "store_models(target, predictors, \"no_syntactic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0QZUn1DwXoE"
   },
   "source": [
    "### Model 4: No Syntactic + Feature Selection\n",
    "\n",
    "Model 3 + feature selection process to further trim down the group of metrics, which might cause underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1736506302468,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "ZZKP1Ualwkkp",
    "outputId": "518e5f45-5669-46da-ecce-45aa68d452e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['len_avg_word', 'is_empty_2', 'val_pct_max', 'last_word', 'constancy', 'len_max_word', 'frequency_iqr', 'frequency_2qo', 'frequency_avg', 'frequency_min', 'frequency_4qo', 'val_pct_min', 'frequency_6qo', 'freq_word_containment', 'number_words', 'val_pct_std', 'freq_word_soundex_containment', 'words_cnt_sd', 'frequency_max', 'first_word', 'entropy', 'len_min_word', 'is_empty', 'name_dist', 'is_binary', 'frequency_1qo', 'words_cnt_max', 'incompleteness', 'frequency_3qo', 'cardinality', 'uniqueness', 'frequency_5qo', 'frequency_7qo', 'words_cnt_min', 'words_cnt_avg', 'frequency_sd']\n",
      "Number of features for gradient_boosting -> Original = 36, new = 30\n",
      "gradient_boosting\n",
      "Fit time: 4.792233 | Score time: 0.005807\n",
      "R2: 0.767436 | MAE: 0.002981 | RMSE: 0.016298 | MedAE: 0.000064\n",
      "------------------------------\n",
      "['len_avg_word', 'is_empty_2', 'val_pct_max', 'last_word', 'constancy', 'len_max_word', 'frequency_iqr', 'frequency_2qo', 'frequency_avg', 'frequency_min', 'frequency_4qo', 'val_pct_min', 'frequency_6qo', 'freq_word_containment', 'number_words', 'val_pct_std', 'freq_word_soundex_containment', 'words_cnt_sd', 'frequency_max', 'first_word', 'entropy', 'len_min_word', 'is_empty', 'name_dist', 'is_binary', 'frequency_1qo', 'words_cnt_max', 'incompleteness', 'frequency_3qo', 'cardinality', 'uniqueness', 'frequency_5qo', 'frequency_7qo', 'words_cnt_min', 'words_cnt_avg', 'frequency_sd']\n",
      "Number of features for extra_trees -> Original = 36, new = 33\n",
      "extra_trees\n",
      "Fit time: 3.044768 | Score time: 0.054328\n",
      "R2: 0.930739 | MAE: 0.000697 | RMSE: 0.008568 | MedAE: 0.000000\n",
      "------------------------------\n",
      "['len_avg_word', 'is_empty_2', 'val_pct_max', 'last_word', 'constancy', 'len_max_word', 'frequency_iqr', 'frequency_2qo', 'frequency_avg', 'frequency_min', 'frequency_4qo', 'val_pct_min', 'frequency_6qo', 'freq_word_containment', 'number_words', 'val_pct_std', 'freq_word_soundex_containment', 'words_cnt_sd', 'frequency_max', 'first_word', 'entropy', 'len_min_word', 'is_empty', 'name_dist', 'is_binary', 'frequency_1qo', 'words_cnt_max', 'incompleteness', 'frequency_3qo', 'cardinality', 'uniqueness', 'frequency_5qo', 'frequency_7qo', 'words_cnt_min', 'words_cnt_avg', 'frequency_sd']\n",
      "Number of features for xgboosting -> Original = 36, new = 32\n",
      "xgboosting\n",
      "Fit time: 0.110423 | Score time: 0.010235\n",
      "R2: 0.937814 | MAE: 0.001052 | RMSE: 0.008197 | MedAE: 0.000057\n",
      "------------------------------\n",
      "['len_avg_word', 'is_empty_2', 'val_pct_max', 'last_word', 'constancy', 'len_max_word', 'frequency_iqr', 'frequency_2qo', 'frequency_avg', 'frequency_min', 'frequency_4qo', 'val_pct_min', 'frequency_6qo', 'freq_word_containment', 'number_words', 'val_pct_std', 'freq_word_soundex_containment', 'words_cnt_sd', 'frequency_max', 'first_word', 'entropy', 'len_min_word', 'is_empty', 'name_dist', 'is_binary', 'frequency_1qo', 'words_cnt_max', 'incompleteness', 'frequency_3qo', 'cardinality', 'uniqueness', 'frequency_5qo', 'frequency_7qo', 'words_cnt_min', 'words_cnt_avg', 'frequency_sd']\n",
      "Number of features for catboost -> Original = 36, new = 33\n",
      "catboost\n",
      "Fit time: 3.088189 | Score time: 0.006004\n",
      "R2: 0.940246 | MAE: 0.001192 | RMSE: 0.008055 | MedAE: 0.000218\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, regressor in zip(best_names, best_regressors):\n",
    "  model_no_syntactic = joblib.load(models_path / \"no_syntactic\" / f\"{name}_no_syntactic.pkl\")\n",
    "\n",
    "  feature_importances = model_no_syntactic.feature_importances_\n",
    "  predictors = merged_no_dummies.drop(columns=['quality', \"pct_numeric\", \"pct_alphanumeric\", \"pct_alphabetic\", \"pct_non_alphanumeric\", \"pct_date_time\", \"pct_unknown\",\n",
    "                                      \"pct_phones\", \"pct_email\", \"pct_url\", \"pct_ip\", \"pct_username\", \"pct_phrases\", \"pct_general\", \"pct_date\", \"pct_time\",\n",
    "                                      \"pct_date_time_specific\", \"pct_others\"], axis=1)\n",
    "\n",
    "  # Match feature importances with corresponding feature names\n",
    "  feature_names = list(predictors.columns)\n",
    "  feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "  # Sort the feature importances in descending order\n",
    "  sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  selected_metrics_fs_no_syntactic = []\n",
    "  for feature, importance in sorted_feature_importances:\n",
    "    if importance > 0.0001:\n",
    "      selected_metrics_fs_no_syntactic.append(feature)\n",
    "\n",
    "  target = merged_no_dummies['quality']\n",
    "  predictors = merged_no_dummies[selected_metrics_fs_no_syntactic]\n",
    "  print(feature_names)\n",
    "  print(f\"Number of features for {name} -> Original = {len(feature_names)}, new = {len(selected_metrics_fs_no_syntactic)}\")\n",
    "  test_list_of_regressors(predictors, target, [name], [regressor])\n",
    "\n",
    "  # selected_metrics_fs_no_syntactic = ['name_dist', 'frequency_max', 'uniqueness', 'first_word', 'frequency_4qo', 'freq_word_containment', 'len_avg_word', 'words_cnt_max',\n",
    "  #                                     'frequency_6qo', 'len_max_word', 'frequency_min', 'frequency_3qo', 'is_empty', 'frequency_iqr', 'entropy', 'val_pct_std',\n",
    "  #                                     'words_cnt_min', 'cardinality', 'words_cnt_sd', 'val_pct_max', 'len_min_word', 'words_cnt_avg']\n",
    "\n",
    "  model = regressor.fit(predictors, target)\n",
    "  folder_path = models_path / \"no_syntactic_fs\"\n",
    "  folder_path.mkdir(parents=True, exist_ok=True)\n",
    "  joblib.dump(regressor, folder_path / f\"{name}_no_syntactic_fs.pkl\")\n",
    "\n",
    "\n",
    "# Gradient Boosting -> models 1-2 -> from 0.016195 to 0.016114, 81 to 39 features\n",
    "#                   -> models 3-4 -> from 0.016299 to 0.016298, 36 to 30 features\n",
    "# Extra Trees       -> models 1-2 -> from 0.009650 to 0.009142, 81 to 54 features\n",
    "#                   -> models 3-4 -> from 0.008569 to 0.008568, 36 to 33 features\n",
    "# XGBoosting        -> models 1-2 -> from 0.008856 to 0.008714, 81 to 50 features\n",
    "#                   -> models 3-4 -> from 0.008252 to 0.008197, 36 to 32 features\n",
    "# CatBoost          -> models 1-2 -> from 0.008206 to 0.007993, 81 to 58 features\n",
    "#                   -> models 3-4 -> from 0.008011 to 0.008055, 36 to 33 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = merged_all['quality']\n",
    "X = merged_all.drop(columns=['quality'], axis=1)\n",
    "original_features = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed low-variance features: ['pct_time', 'pct_email', 'is_empty_2', 'pct_ip', 'pct_date', 'pct_url', 'pct_general', 'pct_date_time_specific', 'is_empty', 'is_binary', 'pct_date_time', 'pct_phones', 'datatype__pct_date_time', 'datatype_2__pct_date_time', 'specific_type__pct_date', 'specific_type__pct_email', 'specific_type__pct_general', 'specific_type__pct_time', 'specific_type__pct_url', 'specific_type_2__pct_date', 'specific_type_2__pct_email', 'specific_type_2__pct_general', 'specific_type_2__pct_time', 'specific_type_2__pct_url']\n",
      "81\n",
      "Removed highly correlated features: ['constancy', 'pct_alphanumeric', 'frequency_4qo', 'val_pct_min', 'frequency_1qo', 'words_cnt_max', 'incompleteness', 'frequency_3qo', 'frequency_5qo', 'frequency_7qo', 'words_cnt_avg', 'specific_type__pct_others', 'specific_type_2__pct_others']\n",
      "57\n",
      "Removed low mutual information features: []\n",
      "44\n",
      "Index(['pct_alphabetic', 'len_avg_word', 'pct_username', 'val_pct_max',\n",
      "       'pct_others', 'last_word', 'len_max_word', 'frequency_iqr',\n",
      "       'frequency_2qo', 'frequency_avg', 'frequency_min', 'frequency_6qo',\n",
      "       'freq_word_containment', 'number_words', 'val_pct_std',\n",
      "       'freq_word_soundex_containment', 'words_cnt_sd', 'frequency_max',\n",
      "       'pct_phrases', 'first_word', 'entropy', 'len_min_word',\n",
      "       'pct_non_alphanumeric', 'name_dist', 'pct_unknown', 'pct_numeric',\n",
      "       'cardinality', 'uniqueness', 'words_cnt_min', 'frequency_sd',\n",
      "       'datatype__pct_alphabetic', 'datatype__pct_alphanumeric',\n",
      "       'datatype__pct_non_alphanumeric', 'datatype__pct_numeric',\n",
      "       'datatype__pct_unknown', 'datatype_2__pct_alphabetic',\n",
      "       'datatype_2__pct_alphanumeric', 'datatype_2__pct_non_alphanumeric',\n",
      "       'datatype_2__pct_numeric', 'datatype_2__pct_unknown',\n",
      "       'specific_type__pct_phrases', 'specific_type__pct_username',\n",
      "       'specific_type_2__pct_phrases', 'specific_type_2__pct_username'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_regression\n",
    "\n",
    "# 1.1 Threshold: 0.01 => keep features with variance > 1%\n",
    "var_thresh = VarianceThreshold(threshold=0.01)\n",
    "var_thresh.fit(X)\n",
    "\n",
    "low_variance_removed = X.columns[~var_thresh.get_support()].tolist()\n",
    "print(\"Removed low-variance features:\", low_variance_removed)\n",
    "print(len(X.columns))\n",
    "\n",
    "X = X[X.columns[var_thresh.get_support()]] # Update X\n",
    "\n",
    "# 1.2 Compute absolute correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)) # Upper triangle mask (to avoid duplicate pairs)\n",
    "\n",
    "high_corr_removed = [column for column in upper.columns if any(upper[column] > 0.9)] # Find features with correlation > 0.9\n",
    "print(\"Removed highly correlated features:\", high_corr_removed)\n",
    "print(len(X.columns))\n",
    "\n",
    "X = X.drop(columns=high_corr_removed)\n",
    "\n",
    "# 1.3 Compute mutual information scores\n",
    "mi_scores = mutual_info_regression(X, y)\n",
    "mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "low_mi_removed = mi_scores[mi_scores < 0.01].index.tolist() # Threshold: MI score must be greater than 0.01\n",
    "print(\"Removed low mutual information features:\", low_mi_removed)\n",
    "print(len(X.columns))\n",
    "\n",
    "X_post_filter_methods = X[mi_scores[mi_scores >= 0.01].index]\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Index(['freq_word_containment', 'uniqueness', 'frequency_2qo', 'first_word',\n",
      "       'frequency_min', 'val_pct_max', 'frequency_iqr', 'name_dist',\n",
      "       'val_pct_std', 'frequency_6qo', 'words_cnt_min', 'pct_username',\n",
      "       'specific_type_2__pct_username', 'freq_word_soundex_containment',\n",
      "       'pct_alphabetic', 'datatype_2__pct_alphanumeric', 'frequency_max',\n",
      "       'specific_type__pct_username', 'len_max_word', 'last_word',\n",
      "       'len_min_word', 'pct_phrases', 'frequency_avg', 'len_avg_word',\n",
      "       'cardinality', 'pct_non_alphanumeric', 'datatype_2__pct_alphabetic',\n",
      "       'pct_unknown', 'entropy', 'datatype__pct_unknown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "gb_model = GradientBoostingRegressor(random_state=211199, n_estimators=50)\n",
    "gb_model.fit(X, y)\n",
    "\n",
    "feature_importances = gb_model.feature_importances_\n",
    "feature_names = list(X.columns) # Match feature importances with corresponding feature names\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True) # Sort the feature importances in descending order\n",
    "\n",
    "selected_metrics = []\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    if importance > 0.0001:\n",
    "      selected_metrics.append(feature)\n",
    "\n",
    "X = X[selected_metrics]\n",
    "print(len(X.columns))\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Optimal number of features: 22\n",
      "Selected features: ['freq_word_containment', 'uniqueness', 'frequency_2qo', 'first_word', 'frequency_min', 'val_pct_max', 'frequency_iqr', 'name_dist', 'val_pct_std', 'frequency_6qo', 'words_cnt_min', 'pct_username', 'specific_type_2__pct_username', 'freq_word_soundex_containment', 'pct_alphabetic', 'datatype_2__pct_alphanumeric', 'frequency_max', 'len_max_word', 'last_word', 'pct_phrases', 'len_avg_word', 'cardinality']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 3\n",
    "gb_model = GradientBoostingRegressor(random_state=211199, n_estimators=50)\n",
    "rfecv = RFECV(estimator=gb_model, step=1, cv=KFold(5), scoring='r2', verbose=3)\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected = X.columns[rfecv.support_]\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(f\"Selected features: {list(selected)}\")\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=211199, n_estimators=50)\n",
    "gb_model.fit(X[selected], y)\n",
    "folder_path = models_path / \"fs_deep\"\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(gb_model, folder_path / f\"gradient_boosting_fs_deep.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Index(['freq_word_containment', 'uniqueness', 'freq_word_soundex_containment',\n",
      "       'name_dist', 'frequency_6qo', 'datatype_2__pct_alphanumeric',\n",
      "       'frequency_2qo', 'len_max_word', 'first_word',\n",
      "       'specific_type_2__pct_username', 'frequency_min', 'val_pct_std',\n",
      "       'val_pct_max', 'words_cnt_min', 'frequency_iqr', 'last_word',\n",
      "       'pct_username', 'frequency_avg', 'frequency_sd', 'frequency_max',\n",
      "       'datatype_2__pct_alphabetic', 'len_avg_word', 'pct_alphabetic',\n",
      "       'datatype__pct_alphabetic', 'len_min_word', 'pct_unknown', 'entropy',\n",
      "       'cardinality', 'words_cnt_sd', 'number_words',\n",
      "       'specific_type__pct_username', 'pct_non_alphanumeric',\n",
      "       'specific_type__pct_phrases', 'pct_phrases', 'pct_others',\n",
      "       'specific_type_2__pct_phrases', 'datatype__pct_unknown',\n",
      "       'datatype__pct_non_alphanumeric', 'datatype_2__pct_unknown',\n",
      "       'datatype__pct_alphanumeric'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "et_model = ExtraTreesRegressor(random_state=211199, n_estimators=50)\n",
    "et_model.fit(X_post_filter_methods, y)\n",
    "\n",
    "feature_importances = et_model.feature_importances_\n",
    "feature_names = list(X_post_filter_methods.columns) # Match feature importances with corresponding feature names\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True) # Sort the feature importances in descending order\n",
    "\n",
    "selected_metrics = []\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    if importance > 0.0001:\n",
    "      selected_metrics.append(feature)\n",
    "\n",
    "X = X_post_filter_methods[selected_metrics]\n",
    "print(len(X.columns))\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Optimal number of features: 28\n",
      "Selected features: ['freq_word_containment', 'uniqueness', 'freq_word_soundex_containment', 'name_dist', 'frequency_6qo', 'datatype_2__pct_alphanumeric', 'frequency_2qo', 'len_max_word', 'first_word', 'specific_type_2__pct_username', 'frequency_min', 'val_pct_std', 'val_pct_max', 'words_cnt_min', 'frequency_iqr', 'last_word', 'pct_username', 'frequency_avg', 'frequency_sd', 'frequency_max', 'datatype_2__pct_alphabetic', 'len_avg_word', 'pct_alphabetic', 'datatype__pct_alphabetic', 'pct_unknown', 'entropy', 'cardinality', 'words_cnt_sd']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Projects\\\\freyja_repo\\\\data\\\\models\\\\fs_deep\\\\extra_trees_fs_deep.pkl']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 3\n",
    "et_model = ExtraTreesRegressor(random_state=211199, n_estimators=50)\n",
    "rfecv = RFECV(estimator=et_model, step=1, cv=KFold(5), scoring='r2', verbose=3)\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected = X.columns[rfecv.support_]\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(f\"Selected features: {list(selected)}\")\n",
    "\n",
    "et_model = ExtraTreesRegressor(random_state=211199, n_estimators=50)\n",
    "et_model.fit(X[selected], y)\n",
    "folder_path = models_path / \"fs_deep\"\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(et_model, folder_path / f\"extra_trees_fs_deep.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_regression, RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def feature_selection_pipeline(dataset, model_typology_name):\n",
    "\n",
    "    y = dataset[\"quality\"]\n",
    "    X = dataset.drop(columns=[\"quality\"], axis=1)\n",
    "    original_features = X.columns.tolist()\n",
    "\n",
    "    print(f\"Original feature count: {len(original_features)}\")\n",
    "\n",
    "    # 1.1 Variance Threshold\n",
    "    var_thresh = VarianceThreshold(threshold = 0.01)\n",
    "    var_thresh.fit(X)\n",
    "    low_variance_removed = X.columns[~var_thresh.get_support()].tolist()\n",
    "    X = X[X.columns[var_thresh.get_support()]]\n",
    "    print(f\"Removed low-variance features: {low_variance_removed}\")\n",
    "    print(f\"Remaining features: {len(X.columns)}\")\n",
    "\n",
    "    # 1.2 Correlation Filter\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_removed = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "    X = X.drop(columns=high_corr_removed)\n",
    "    print(f\"Removed highly correlated features: {high_corr_removed}\")\n",
    "    print(f\"Remaining features: {len(X.columns)}\")\n",
    "\n",
    "    # 1.3 Mutual Information\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "    low_mi_removed = mi_scores[mi_scores < 0.01].index.tolist()\n",
    "    X_post_filter_methods = X[mi_scores[mi_scores >= 0.01].index]\n",
    "    print(f\"Removed low mutual information features: {low_mi_removed}\")\n",
    "    print(f\"Remaining features: {len(X_post_filter_methods.columns)}\")\n",
    "\n",
    "    # Utility function for importance + RFECV steps\n",
    "    def run_model_selection(model, model_name):\n",
    "        nonlocal X_post_filter_methods, y\n",
    "\n",
    "        # Feature Importance Filter\n",
    "        model.fit(X_post_filter_methods, y)\n",
    "        feature_importances = model.feature_importances_\n",
    "        feature_names = list(X_post_filter_methods.columns)\n",
    "        feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "        sorted_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        selected_metrics = [f for f, imp in sorted_importances if imp > 0.0001]\n",
    "\n",
    "        X_temp = X_post_filter_methods[selected_metrics]\n",
    "        print(f\"{model_name} - Selected {len(X_temp.columns)} features after importance filtering\")\n",
    "\n",
    "        # RFECV\n",
    "        rfecv = RFECV(estimator=model, step=1, cv=KFold(5), scoring='r2',verbose=3)\n",
    "        rfecv.fit(X_temp, y)\n",
    "        selected = X_temp.columns[rfecv.support_]\n",
    "        print(f\"{model_name} - Optimal number of features: {rfecv.n_features_}\")\n",
    "        print(f\"{model_name} - Selected features: {list(selected)}\")\n",
    "\n",
    "        # Train final model\n",
    "        model.fit(X_temp[selected], y)\n",
    "        folder_path = Path(models_path) / model_typology_name\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "        model_file = folder_path / f\"{model_name}_{model_typology_name}.pkl\"\n",
    "        joblib.dump(model, model_file)\n",
    "        print(f\"Saved model to: {model_file}\")\n",
    "\n",
    "        return list(selected)\n",
    "\n",
    "    run_model_selection(GradientBoostingRegressor(random_state=21111999, n_estimators=50), \"gradient_boosting\")\n",
    "    run_model_selection(ExtraTreesRegressor(random_state=21111999, n_estimators=50), \"extra_trees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature count: 22\n",
      "Removed low-variance features: ['is_empty']\n",
      "Remaining features: 21\n",
      "Removed highly correlated features: ['frequency_3qo', 'words_cnt_sd']\n",
      "Remaining features: 19\n",
      "Removed low mutual information features: []\n",
      "Remaining features: 19\n",
      "gradient_boosting - Selected 18 features after importance filtering\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "gradient_boosting - Optimal number of features: 8\n",
      "gradient_boosting - Selected features: ['freq_word_containment', 'uniqueness', 'frequency_4qo', 'first_word', 'frequency_min', 'val_pct_std', 'frequency_iqr', 'val_pct_max']\n",
      "Saved model to: C:\\Projects\\freyja_repo\\data\\models\\no_syntactic_fs_deep\\gradient_boosting_no_syntactic_fs_deep.pkl\n",
      "extra_trees - Selected 19 features after importance filtering\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "extra_trees - Optimal number of features: 14\n",
      "extra_trees - Selected features: ['freq_word_containment', 'uniqueness', 'name_dist', 'frequency_min', 'frequency_6qo', 'val_pct_std', 'val_pct_max', 'frequency_iqr', 'first_word', 'frequency_4qo', 'frequency_max', 'len_max_word', 'words_cnt_min', 'cardinality']\n",
      "Saved model to: C:\\Projects\\freyja_repo\\data\\models\\no_syntactic_fs_deep\\extra_trees_no_syntactic_fs_deep.pkl\n"
     ]
    }
   ],
   "source": [
    "no_syntactic_features = merged_no_dummies[['quality', 'name_dist', 'frequency_max', 'uniqueness', 'first_word', 'frequency_4qo', 'freq_word_containment', 'len_avg_word', 'words_cnt_max',\n",
    "                                'frequency_6qo', 'len_max_word', 'frequency_min', 'frequency_3qo', 'is_empty', 'frequency_iqr', 'entropy', 'val_pct_std',\n",
    "                                'words_cnt_min', 'cardinality', 'words_cnt_sd', 'val_pct_max', 'len_min_word', 'words_cnt_avg']]\n",
    "\n",
    "feature_selection_pipeline(merged_all, \"all_fs_deep\")\n",
    "feature_selection_pipeline(no_syntactic_features, \"no_syntactic_fs_deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Projects\\\\freyja_repo\\\\data\\\\models\\\\no_syntactic_fs_deep\\\\gradient_boosting_no_syntactic_fs_deep.pkl']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = merged_no_dummies['quality']\n",
    "predictors = merged_no_dummies[['name_dist', 'frequency_max', 'uniqueness', 'first_word', 'frequency_4qo', 'freq_word_containment', 'len_avg_word', 'words_cnt_max',\n",
    "                                'frequency_6qo', 'len_max_word', 'frequency_min', 'frequency_3qo', 'is_empty', 'frequency_iqr', 'entropy', 'val_pct_std',\n",
    "                                'words_cnt_min', 'cardinality', 'words_cnt_sd', 'val_pct_max', 'len_min_word', 'words_cnt_avg']]\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=21111999, n_estimators=50)\n",
    "gb_model.fit(predictors, y)\n",
    "folder_path = Path(models_path) / \"no_syntactic_fs_deep\"\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "model_file = folder_path / f\"gradient_boosting_no_syntactic_fs_deep.pkl\"\n",
    "joblib.dump(gb_model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQHVWZM7qcZo"
   },
   "source": [
    "# Benchmark evaluation\n",
    "\n",
    "Once we have define all the models, we will evaluate each of the five selected benchmarks with all of them, with the goal of discerning which is the best one. To do so, all the distances for all query columns of each benchmark have been obtained and stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb-lbfUVSJHZ"
   },
   "source": [
    "## Preparation\n",
    "Load all the model and define the functions to prepare the data for the models. This data preparation depends on the features defined for each model. We also present the function used to obtain the metrics from the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "error",
     "timestamp": 1736438929909,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "VDQPdy3WHVQ3",
    "outputId": "969caa2f-defd-4878-d983-8a9dc4bb706d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gradient_boosting_all', 'gradient_boosting_all_fs', 'gradient_boosting_no_syntactic', 'gradient_boosting_no_syntactic_fs', 'gradient_boosting_all_fs_deep', 'gradient_boosting_no_syntactic_fs_deep', 'extra_trees_all', 'extra_trees_all_fs', 'extra_trees_no_syntactic', 'extra_trees_no_syntactic_fs', 'extra_trees_all_fs_deep', 'extra_trees_no_syntactic_fs_deep', 'xgboosting_all', 'xgboosting_all_fs', 'xgboosting_no_syntactic', 'xgboosting_no_syntactic_fs', 'catboost_all', 'catboost_all_fs', 'catboost_no_syntactic', 'catboost_no_syntactic_fs'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "typologies = [\"all\", \"all_fs\", \"no_syntactic\", \"no_syntactic_fs\", \"all_fs_deep\", \"no_syntactic_fs_deep\"]\n",
    "\n",
    "for name in best_names:\n",
    "    for type in typologies:\n",
    "        try:\n",
    "            models[f\"{name}_{type}\"] = joblib.load(models_path / type / f\"{name}_{type}.pkl\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "print(models.keys())\n",
    "print(len(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "J9kqsOF3FVUk"
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_model(distances, model, type_of_model):\n",
    "  distances = distances.drop(columns=['dataset_name', 'dataset_name_2', 'attribute_name', 'attribute_name_2'], axis=1)\n",
    "\n",
    "  maybe_missing_columns = [\"datatype__pct_alphabetic\", \"datatype__pct_date_time\", \"datatype__pct_non_alphanumeric\", \"datatype__pct_numeric\", \"datatype__pct_unknown\", \"datatype__pct_alphanumeric\",\n",
    "                            \"specific_type__pct_date\", \"specific_type__pct_email\", \"specific_type__pct_phrases\", 'specific_type_2__pct_username', 'datatype_2__pct_alphanumeric', 'datatype_2__pct_alphabetic',\n",
    "                            'specific_type_2__pct_phrases', \"specific_type__pct_general\", \"specific_type__pct_others\", \"specific_type__pct_time\", \"specific_type__pct_url\", \"specific_type__pct_username\",\n",
    "                            'datatype_2__pct_date_time', 'specific_type_2__pct_date', 'specific_type_2__pct_email', 'specific_type_2__pct_general', 'specific_type_2__pct_url', 'datatype_2__pct_non_alphanumeric',\n",
    "                            'datatype_2__pct_numeric', 'datatype_2__pct_unknown', 'specific_type_2__pct_others', 'specific_type_2__pct_time']\n",
    "\n",
    "  if \"no_syntactic_fs\" in type_of_model or \"custom\" in type_of_model:\n",
    "    pass\n",
    "  elif (\"all\" in type_of_model):\n",
    "    distances = pd.concat([distances.drop('datatype', axis=1), pd.get_dummies(distances['datatype'], prefix='datatype_', dtype=int)], axis=1)\n",
    "    distances = pd.concat([distances.drop('datatype_2', axis=1), pd.get_dummies(distances['datatype_2'], prefix='datatype_2_', dtype=int)], axis=1)\n",
    "    distances = pd.concat([distances.drop('specific_type', axis=1), pd.get_dummies(distances['specific_type'], prefix='specific_type_', dtype=int)], axis=1)\n",
    "    distances = pd.concat([distances.drop('specific_type_2', axis=1), pd.get_dummies(distances['specific_type_2'], prefix='specific_type_2_', dtype=int)], axis=1)\n",
    "\n",
    "    for column in maybe_missing_columns:\n",
    "      if column not in distances.columns:\n",
    "        distances[column] = 0\n",
    "  else:\n",
    "    distances = distances.drop(columns=['datatype', \"datatype_2\", \"specific_type\", \"specific_type_2\"], axis=1, errors='ignore')\n",
    "    distances = distances.drop(columns=[\"pct_numeric\", \"pct_alphanumeric\", \"pct_alphabetic\", \"pct_non_alphanumeric\", \"pct_date_time\", \"pct_unknown\", \"pct_phones\", \"pct_email\", \"pct_url\", \"pct_ip\",\n",
    "                                        \"pct_username\", \"pct_phrases\", \"pct_general\", \"pct_date\", \"pct_time\", \"pct_date_time_specific\", \"pct_others\"], axis=1, errors='ignore')\n",
    "\n",
    "  if 'is_empty_2' not in distances.columns:\n",
    "    distances['is_empty_2'] = 0\n",
    "\n",
    "  # Arrange the columns as in the model\n",
    "  if \"catboost\" in type_of_model:\n",
    "    distances = distances[model.feature_names_] \n",
    "  else:\n",
    "    distances = distances[model.feature_names_in_] \n",
    "  return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTTtg8E9Q0-T"
   },
   "outputs": [],
   "source": [
    "def compute_and_evaluate_ranking(model, model_type, k, step, ground_truth_path, distances_folder_path):\n",
    "  # Read the ground truth and obtain, for every target column, the amount of candidate columns that it has a join with. This will allow us to calculate the recall,\n",
    "  # as it indicates the maximum possible joins, regardless of the value of k\n",
    "  ground_truth = pd.read_csv(ground_truth_path, header = 0)\n",
    "  pair_counts = ground_truth.groupby(['target_ds', 'target_attr']).size().reset_index(name='joins_count')\n",
    "\n",
    "  # Initialize the matrix of metrics\n",
    "  num_observations = int(k / step)\n",
    "  precision = [0] * num_observations\n",
    "  recall = [0] * num_observations\n",
    "  max_recall = [0] * num_observations\n",
    "  MAP = [0] * num_observations\n",
    "\n",
    "  # Initialize execution time\n",
    "  total_time = 0\n",
    "\n",
    "  for _, row in tqdm(pair_counts.iterrows(), total=len(pair_counts)):\n",
    "      dataset = row['target_ds']\n",
    "      attribute = row['target_attr']\n",
    "      count = row['joins_count']\n",
    "\n",
    "      st = time.time()\n",
    "\n",
    "      # Read the distances and do some preprocessing\n",
    "      distances = pd.read_csv(distances_folder_path + 'distances_' + dataset.replace(\".csv\", \"_profile_\") + attribute.replace(\"/\", \"_\").replace(\": \",\"_\").replace(\"'\",\"_\") + \".csv\", header = 0, encoding='latin1', on_bad_lines=\"skip\")\n",
    "\n",
    "      dataset_names = distances[\"dataset_name_2\"] # We store dataset and attribute names to be used to evaluate the ranking\n",
    "      attribute_names = distances[\"attribute_name_2\"]\n",
    "      distances = prepare_data_for_model(distances, model, model_type)\n",
    "\n",
    "      # # Use the model to predict\n",
    "      # y_pred = model.predict(distances)\n",
    "      # distances[\"predictions\"] = y_pred\n",
    "\n",
    "      # Use the model to predict (preventing some weird lines that might have slipped in)\n",
    "      distances_numeric = distances.apply(pd.to_numeric, errors='coerce') # Convert everything to float, invalid parsing becomes NaN\n",
    "      valid_rows = distances_numeric.dropna(axis=0, how='any') # Keep track of valid rows\n",
    "      y_pred = model.predict(valid_rows) # Predict only on valid rows\n",
    "      distances.loc[valid_rows.index, \"predictions\"] = y_pred # Assign predictions back only to the valid rows\n",
    "\n",
    "      distances[\"target_ds\"] = dataset_names\n",
    "      distances[\"target_attr\"] = attribute_names\n",
    "\n",
    "      total_time += (time.time() - st) # In the time assessment we do not consider the evaluation of the ranking\n",
    "\n",
    "      # Precompute a lookup set of valid (candidate_ds, candidate_attr) for this query\n",
    "      valid_pairs = set(\n",
    "          ground_truth.loc[\n",
    "              (ground_truth['target_ds'] == dataset) &\n",
    "              (ground_truth['target_attr'] == attribute),\n",
    "              ['candidate_ds', 'candidate_attr']\n",
    "          ].itertuples(index=False, name=None)\n",
    "      )\n",
    "\n",
    "      # For every k that we want to assess the ranking of, we get the top k joins and check how many appear in the grpund truth\n",
    "      for k_iter in range(1, num_observations + 1):\n",
    "        count_sem = 0\n",
    "        ap = 0\n",
    "        count_positions = 0\n",
    "\n",
    "        top_k_joins = distances.sort_values(by='predictions', ascending=False).head(k_iter * step)\n",
    "\n",
    "        for position in top_k_joins.itertuples(index=False):\n",
    "            pair = (position.target_ds, position.target_attr)\n",
    "            if pair in valid_pairs: \n",
    "                count_sem += 1\n",
    "                ap += count_sem / (count_positions + 1)\n",
    "            count_positions += 1\n",
    "\n",
    "\n",
    "        precision[k_iter - 1] += count_sem / (k_iter * step)\n",
    "        if count_sem != 0:\n",
    "            MAP[k_iter - 1] += ap / count_sem\n",
    "        recall[k_iter - 1] += count_sem / count\n",
    "        max_recall[k_iter - 1] += (k_iter * step) / count\n",
    "\n",
    "  print(\"AVERAGE time to load the distances and execute the model:\")\n",
    "  print(\"----%.2f----\" % (total_time / len(pair_counts)))\n",
    "\n",
    "  print(\"Precisions:\", [round(element / len(pair_counts), 4) for element in precision])\n",
    "  print(\"Recall:\", [round(element / len(pair_counts), 4) for element in recall])\n",
    "  print(\"Max recall:\", [round(element / len(pair_counts), 4) for element in max_recall])\n",
    "  print(\"Recall percentage:\", [round((recall_iter / len(pair_counts)) / (max_recall_iter / len(pair_counts)), 4) for recall_iter, max_recall_iter in zip(recall, max_recall)])\n",
    "  print(\"MAP:\", [round(element / len(pair_counts), 4) for element in MAP])\n",
    "\n",
    "  return [round(element / len(pair_counts), 4) for element in precision]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYVafCqwHcFK"
   },
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(k, step, ground_truth_path, distances_folder_path, models=models):\n",
    "    results = {}\n",
    "\n",
    "    for model_type, model in models.items():\n",
    "        print(f\"Model {model_type}\")\n",
    "        precision_scores = compute_and_evaluate_ranking(\n",
    "            model, model_type, k, step, ground_truth_path, distances_folder_path\n",
    "        )\n",
    "        results[model_type] = precision_scores\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "    # Compute average precisions\n",
    "    avg_precisions = {\n",
    "        model_name: sum(precisions) / len(precisions)\n",
    "        for model_name, precisions in results.items()\n",
    "    }\n",
    "\n",
    "    # Sort models from best to worst\n",
    "    ranked_models = sorted(avg_precisions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print ranked results\n",
    "    print(\"\\n==================== MODEL RANKINGS ====================\")\n",
    "    for rank, (model_name, avg_precision) in enumerate(ranked_models, start=1):\n",
    "        print(f\"{rank:2d}. Model: {model_name:20s} | Avg Precision: {avg_precision:.4f}\")\n",
    "\n",
    "    print(\"========================================================\")\n",
    "\n",
    "    # return ranked_models, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oibWXdIE4oN"
   },
   "source": [
    "### Santos Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [1.0, 0.98, 0.96, 0.955, 0.948, 0.9467, 0.9429, 0.9375, 0.9333, 0.928]\n",
      "Recall: [0.0757, 0.1484, 0.2186, 0.2893, 0.3589, 0.4296, 0.4988, 0.5671, 0.6342, 0.7001]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9797, 0.9625, 0.9551, 0.9482, 0.9457, 0.9412, 0.9362, 0.9307, 0.9247]\n",
      "MAP: [1.0, 1.0, 1.0, 0.9967, 0.9955, 0.9944, 0.9939, 0.9937, 0.9856, 0.9825]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.05----\n",
      "Precisions: [1.0, 0.98, 0.96, 0.955, 0.948, 0.9467, 0.9429, 0.9375, 0.9333, 0.928]\n",
      "Recall: [0.0757, 0.1484, 0.2186, 0.2893, 0.3589, 0.4296, 0.4988, 0.5671, 0.6342, 0.7001]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9797, 0.9625, 0.9551, 0.9482, 0.9457, 0.9412, 0.9362, 0.9307, 0.9247]\n",
      "MAP: [1.0, 1.0, 1.0, 0.9967, 0.9955, 0.9944, 0.9939, 0.9937, 0.9856, 0.9825]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [1.0, 0.97, 0.94, 0.935, 0.932, 0.9267, 0.9229, 0.9175, 0.9111, 0.9]\n",
      "Recall: [0.0757, 0.1467, 0.2133, 0.2821, 0.3509, 0.4187, 0.4863, 0.5524, 0.6164, 0.6768]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9687, 0.939, 0.9314, 0.9269, 0.9218, 0.9175, 0.912, 0.9046, 0.894]\n",
      "MAP: [1.0, 1.0, 1.0, 0.9933, 0.9918, 0.9916, 0.9909, 0.9905, 0.9898, 0.9896]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [1.0, 0.97, 0.94, 0.935, 0.932, 0.9267, 0.9229, 0.9175, 0.9111, 0.9]\n",
      "Recall: [0.0757, 0.1467, 0.2133, 0.2821, 0.3509, 0.4187, 0.4863, 0.5524, 0.6164, 0.6768]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9687, 0.939, 0.9314, 0.9269, 0.9218, 0.9175, 0.912, 0.9046, 0.894]\n",
      "MAP: [1.0, 1.0, 1.0, 0.9933, 0.9918, 0.9916, 0.9909, 0.9905, 0.9898, 0.9896]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_all_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.05----\n",
      "Precisions: [1.0, 0.98, 0.9733, 0.965, 0.96, 0.9567, 0.9543, 0.95, 0.9467, 0.938]\n",
      "Recall: [0.0757, 0.1487, 0.2211, 0.2921, 0.3619, 0.4318, 0.5017, 0.5702, 0.6397, 0.7039]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9817, 0.9735, 0.9644, 0.9561, 0.9506, 0.9467, 0.9414, 0.9388, 0.9297]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9944, 0.9928, 0.9923, 0.9921, 0.9922, 0.9906, 0.9895]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [1.0, 0.98, 0.9733, 0.97, 0.964, 0.96, 0.96, 0.96, 0.9556, 0.952]\n",
      "Recall: [0.0757, 0.1484, 0.221, 0.2936, 0.3653, 0.4363, 0.5082, 0.5801, 0.6497, 0.719]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9797, 0.973, 0.9696, 0.9651, 0.9604, 0.9588, 0.9577, 0.9535, 0.9496]\n",
      "MAP: [1.0, 1.0, 1.0, 1.0, 1.0, 0.9933, 0.9906, 0.9895, 0.9891, 0.9887]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.07----\n",
      "Precisions: [1.0, 0.97, 0.9667, 0.96, 0.956, 0.9533, 0.9514, 0.9475, 0.94, 0.932]\n",
      "Recall: [0.0757, 0.1462, 0.2181, 0.2883, 0.3585, 0.4287, 0.499, 0.5688, 0.6343, 0.6983]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9654, 0.9601, 0.952, 0.9471, 0.9438, 0.9415, 0.9391, 0.9309, 0.9223]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9961, 0.9961, 0.9962, 0.9964, 0.989, 0.9889, 0.9887]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.07----\n",
      "Precisions: [1.0, 0.96, 0.9533, 0.935, 0.928, 0.93, 0.9286, 0.9275, 0.9222, 0.912]\n",
      "Recall: [0.0757, 0.1445, 0.2147, 0.2816, 0.3495, 0.4197, 0.4885, 0.5573, 0.6228, 0.6844]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9543, 0.9454, 0.9299, 0.9232, 0.9239, 0.9217, 0.9201, 0.9139, 0.904]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9967, 0.9957, 0.9913, 0.9903, 0.9886, 0.9879, 0.9877]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [1.0, 0.96, 0.9467, 0.94, 0.932, 0.9267, 0.9257, 0.925, 0.9222, 0.916]\n",
      "Recall: [0.0757, 0.1456, 0.2154, 0.2842, 0.352, 0.4199, 0.4887, 0.5579, 0.6253, 0.6895]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9613, 0.9483, 0.9384, 0.93, 0.9243, 0.9221, 0.9211, 0.9177, 0.9107]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9944, 0.9939, 0.9932, 0.9922, 0.9844, 0.9765, 0.9744]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [1.0, 0.98, 0.96, 0.955, 0.952, 0.9433, 0.94, 0.935, 0.9267, 0.916]\n",
      "Recall: [0.0757, 0.149, 0.2194, 0.29, 0.3603, 0.4281, 0.4969, 0.5643, 0.6283, 0.6901]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9843, 0.966, 0.9577, 0.9517, 0.9424, 0.9376, 0.9316, 0.9221, 0.9114]\n",
      "MAP: [1.0, 1.0, 1.0, 0.9933, 0.9857, 0.9856, 0.985, 0.9848, 0.9849, 0.9851]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.07----\n",
      "Precisions: [0.96, 0.93, 0.9133, 0.91, 0.908, 0.9133, 0.9143, 0.915, 0.9133, 0.91]\n",
      "Recall: [0.0738, 0.1421, 0.209, 0.2773, 0.3447, 0.4149, 0.4837, 0.5525, 0.6194, 0.685]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [0.9748, 0.9386, 0.9203, 0.9158, 0.9106, 0.9134, 0.9127, 0.9122, 0.909, 0.9047]\n",
      "MAP: [0.96, 0.96, 0.96, 0.9583, 0.9663, 0.9671, 0.9687, 0.9702, 0.9706, 0.9713]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [0.98, 0.97, 0.9533, 0.945, 0.94, 0.94, 0.94, 0.94, 0.9378, 0.93]\n",
      "Recall: [0.0748, 0.1477, 0.2175, 0.2868, 0.3556, 0.4258, 0.4968, 0.567, 0.6365, 0.7011]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [0.9874, 0.9754, 0.9578, 0.9471, 0.9394, 0.9374, 0.9374, 0.9361, 0.9341, 0.9261]\n",
      "MAP: [0.98, 0.99, 0.9867, 0.985, 0.9841, 0.9831, 0.9793, 0.9791, 0.9776, 0.977]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.08----\n",
      "Precisions: [1.0, 0.96, 0.9333, 0.92, 0.904, 0.8933, 0.8857, 0.88, 0.8756, 0.868]\n",
      "Recall: [0.0757, 0.145, 0.2124, 0.2783, 0.3408, 0.4031, 0.4651, 0.5275, 0.5893, 0.6484]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9577, 0.9352, 0.919, 0.9002, 0.8874, 0.8776, 0.8708, 0.8649, 0.8564]\n",
      "MAP: [1.0, 1.0, 1.0, 0.9967, 0.9955, 0.9879, 0.9852, 0.9839, 0.9821, 0.9813]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.07----\n",
      "Precisions: [1.0, 0.96, 0.9333, 0.915, 0.896, 0.8833, 0.8686, 0.8475, 0.8356, 0.824]\n",
      "Recall: [0.0757, 0.1447, 0.2105, 0.2754, 0.3369, 0.3974, 0.4548, 0.5072, 0.562, 0.6153]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9555, 0.9266, 0.9093, 0.8899, 0.8748, 0.8581, 0.8374, 0.8247, 0.8128]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9944, 0.9929, 0.9911, 0.9901, 0.99, 0.989, 0.9886]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [0.98, 0.95, 0.9333, 0.925, 0.912, 0.9033, 0.8886, 0.875, 0.8556, 0.842]\n",
      "Recall: [0.0748, 0.1443, 0.2129, 0.2804, 0.3452, 0.4098, 0.4692, 0.5268, 0.5796, 0.6333]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [0.9874, 0.953, 0.9373, 0.9261, 0.9119, 0.9021, 0.8853, 0.8698, 0.8506, 0.8364]\n",
      "MAP: [0.98, 0.99, 0.9917, 0.9911, 0.9845, 0.9836, 0.9832, 0.9758, 0.9758, 0.973]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [1.0, 0.95, 0.9333, 0.915, 0.9, 0.8867, 0.8743, 0.8575, 0.84, 0.822]\n",
      "Recall: [0.0757, 0.1443, 0.2122, 0.2761, 0.3383, 0.3995, 0.4597, 0.5145, 0.5665, 0.6154]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.953, 0.9342, 0.9117, 0.8938, 0.8793, 0.8675, 0.8495, 0.8313, 0.8128]\n",
      "MAP: [1.0, 1.0, 0.9933, 0.9911, 0.9911, 0.9889, 0.9867, 0.9859, 0.9856, 0.9852]\n",
      "------------------------------------------------------\n",
      "Model catboost_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [1.0, 0.96, 0.9467, 0.925, 0.92, 0.9033, 0.8943, 0.8875, 0.88, 0.868]\n",
      "Recall: [0.0757, 0.1456, 0.2149, 0.2788, 0.3456, 0.4073, 0.4707, 0.5335, 0.5948, 0.6521]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9613, 0.9462, 0.9206, 0.9129, 0.8966, 0.8881, 0.8809, 0.8729, 0.8613]\n",
      "MAP: [1.0, 1.0, 0.9933, 0.9928, 0.9841, 0.9841, 0.9799, 0.9755, 0.9743, 0.9661]\n",
      "------------------------------------------------------\n",
      "Model catboost_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [1.0, 0.94, 0.92, 0.91, 0.9, 0.89, 0.8829, 0.8675, 0.8533, 0.838]\n",
      "Recall: [0.0757, 0.1431, 0.2094, 0.2758, 0.3398, 0.4037, 0.466, 0.5228, 0.5781, 0.6308]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.945, 0.922, 0.9106, 0.8977, 0.8887, 0.8792, 0.8631, 0.8485, 0.8332]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9961, 0.9901, 0.9835, 0.9798, 0.9793, 0.9774, 0.9764]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [1.0, 0.98, 0.9533, 0.94, 0.932, 0.9267, 0.9086, 0.8975, 0.8844, 0.872]\n",
      "Recall: [0.0757, 0.1479, 0.2163, 0.2835, 0.3507, 0.4183, 0.4776, 0.538, 0.5955, 0.6522]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.977, 0.9525, 0.9361, 0.9265, 0.9208, 0.9011, 0.8883, 0.874, 0.8615]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9933, 0.9912, 0.9898, 0.9891, 0.9878, 0.9874, 0.9863]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [1.0, 0.97, 0.9467, 0.93, 0.928, 0.92, 0.9057, 0.9025, 0.8911, 0.876]\n",
      "Recall: [0.0757, 0.1469, 0.2152, 0.2822, 0.351, 0.4161, 0.4769, 0.5419, 0.6016, 0.6564]\n",
      "Max recall: [0.0757, 0.1514, 0.2271, 0.3028, 0.3786, 0.4543, 0.53, 0.6057, 0.6814, 0.7571]\n",
      "Recall percentage: [1.0, 0.9703, 0.9477, 0.9318, 0.9271, 0.9161, 0.8999, 0.8947, 0.8829, 0.867]\n",
      "MAP: [1.0, 1.0, 0.9933, 0.99, 0.9842, 0.9821, 0.9813, 0.9795, 0.9791, 0.9785]\n",
      "------------------------------------------------------\n",
      "\n",
      "==================== MODEL RANKINGS ====================\n",
      " 1. Model: gradient_boosting_no_syntactic_fs_deep | Avg Precision: 0.9675\n",
      " 2. Model: gradient_boosting_all_fs_deep | Avg Precision: 0.9624\n",
      " 3. Model: extra_trees_all      | Avg Precision: 0.9577\n",
      " 4. Model: gradient_boosting_all | Avg Precision: 0.9531\n",
      " 5. Model: gradient_boosting_all_fs | Avg Precision: 0.9531\n",
      " 6. Model: extra_trees_no_syntactic_fs | Avg Precision: 0.9508\n",
      " 7. Model: extra_trees_no_syntactic_fs_deep | Avg Precision: 0.9476\n",
      " 8. Model: extra_trees_all_fs   | Avg Precision: 0.9397\n",
      " 9. Model: extra_trees_no_syntactic | Avg Precision: 0.9394\n",
      "10. Model: gradient_boosting_no_syntactic | Avg Precision: 0.9355\n",
      "11. Model: gradient_boosting_no_syntactic_fs | Avg Precision: 0.9355\n",
      "12. Model: catboost_no_syntactic | Avg Precision: 0.9294\n",
      "13. Model: catboost_no_syntactic_fs | Avg Precision: 0.9270\n",
      "14. Model: extra_trees_all_fs_deep | Avg Precision: 0.9187\n",
      "15. Model: catboost_all         | Avg Precision: 0.9185\n",
      "16. Model: xgboosting_all       | Avg Precision: 0.9120\n",
      "17. Model: xgboosting_no_syntactic | Avg Precision: 0.9065\n",
      "18. Model: catboost_all_fs      | Avg Precision: 0.9002\n",
      "19. Model: xgboosting_no_syntactic_fs | Avg Precision: 0.8979\n",
      "20. Model: xgboosting_all_fs    | Avg Precision: 0.8963\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "step = 1\n",
    "ground_truth_path = 'C:/Projects/benchmarks/santos_small/santos_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/benchmarks/santos_small/distances_all_metrics/'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-x_FOB6VOWH"
   },
   "source": [
    "### TUS Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 697305,
     "status": "ok",
     "timestamp": 1722271470487,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "tKbvCZi7VP1j",
    "outputId": "4226856a-5a43-47ea-b235-9601696b4718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.13----\n",
      "Precisions: [0.974, 0.8885, 0.816, 0.8105, 0.815, 0.819]\n",
      "Recall: [0.1072, 0.1944, 0.2667, 0.3524, 0.4421, 0.5323]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9798, 0.8885, 0.8126, 0.8053, 0.8082, 0.8109]\n",
      "MAP: [0.9859, 0.9807, 0.9742, 0.9491, 0.932, 0.9244]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.12----\n",
      "Precisions: [0.974, 0.8885, 0.816, 0.8105, 0.815, 0.819]\n",
      "Recall: [0.1072, 0.1944, 0.2667, 0.3524, 0.4421, 0.5323]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9798, 0.8885, 0.8126, 0.8053, 0.8082, 0.8109]\n",
      "MAP: [0.9859, 0.9807, 0.9742, 0.9491, 0.932, 0.9244]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.10----\n",
      "Precisions: [0.977, 0.8365, 0.68, 0.5818, 0.519, 0.4787]\n",
      "Recall: [0.1074, 0.1783, 0.2128, 0.2372, 0.2597, 0.2831]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9816, 0.8149, 0.6482, 0.5421, 0.4748, 0.4312]\n",
      "MAP: [0.9893, 0.9837, 0.9806, 0.979, 0.9781, 0.9758]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.10----\n",
      "Precisions: [0.977, 0.8365, 0.68, 0.5818, 0.519, 0.4787]\n",
      "Recall: [0.1074, 0.1783, 0.2128, 0.2372, 0.2597, 0.2831]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9816, 0.8149, 0.6482, 0.5421, 0.4748, 0.4312]\n",
      "MAP: [0.9893, 0.9837, 0.9806, 0.979, 0.9781, 0.9758]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_all_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.12----\n",
      "Precisions: [0.973, 0.9135, 0.8603, 0.8732, 0.8894, 0.8967]\n",
      "Recall: [0.1071, 0.2015, 0.2849, 0.3853, 0.4903, 0.5931]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9786, 0.9207, 0.868, 0.8804, 0.8963, 0.9035]\n",
      "MAP: [0.9862, 0.9802, 0.9735, 0.9461, 0.928, 0.9206]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.10----\n",
      "Precisions: [0.976, 0.911, 0.8543, 0.865, 0.8734, 0.8837]\n",
      "Recall: [0.1073, 0.2008, 0.2831, 0.3819, 0.482, 0.5849]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9811, 0.9177, 0.8625, 0.8726, 0.8811, 0.891]\n",
      "MAP: [0.9863, 0.9814, 0.973, 0.9434, 0.9261, 0.918]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.16----\n",
      "Precisions: [0.977, 0.849, 0.7313, 0.665, 0.6082, 0.564]\n",
      "Recall: [0.1074, 0.1827, 0.2305, 0.2731, 0.3072, 0.3383]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.982, 0.835, 0.7023, 0.6241, 0.5616, 0.5154]\n",
      "MAP: [0.9842, 0.9802, 0.978, 0.9732, 0.9696, 0.9668]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.15----\n",
      "Precisions: [0.972, 0.849, 0.7317, 0.6485, 0.5948, 0.553]\n",
      "Recall: [0.107, 0.1824, 0.2301, 0.2658, 0.301, 0.3311]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9778, 0.8337, 0.7011, 0.6074, 0.5502, 0.5043]\n",
      "MAP: [0.98, 0.977, 0.9748, 0.9731, 0.9702, 0.9675]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.13----\n",
      "Precisions: [0.974, 0.8285, 0.7007, 0.6358, 0.595, 0.5568]\n",
      "Recall: [0.1072, 0.1766, 0.2168, 0.2556, 0.2939, 0.3253]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9794, 0.8071, 0.6604, 0.5841, 0.5372, 0.4956]\n",
      "MAP: [0.9771, 0.9759, 0.9751, 0.9676, 0.9638, 0.963]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.13----\n",
      "Precisions: [0.978, 0.828, 0.6997, 0.631, 0.5882, 0.5468]\n",
      "Recall: [0.1075, 0.1765, 0.2165, 0.2537, 0.2905, 0.3194]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9828, 0.8068, 0.6596, 0.5796, 0.531, 0.4865]\n",
      "MAP: [0.9827, 0.9793, 0.9775, 0.9691, 0.9647, 0.9642]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.15----\n",
      "Precisions: [0.98, 0.866, 0.7543, 0.6688, 0.6072, 0.5602]\n",
      "Recall: [0.1077, 0.1872, 0.2392, 0.2749, 0.3054, 0.3324]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.984, 0.8555, 0.7287, 0.6282, 0.5582, 0.5064]\n",
      "MAP: [0.9882, 0.983, 0.9801, 0.9786, 0.978, 0.9775]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.13----\n",
      "Precisions: [0.973, 0.9025, 0.8293, 0.7963, 0.7612, 0.7155]\n",
      "Recall: [0.1071, 0.199, 0.275, 0.3521, 0.4168, 0.4614]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9785, 0.9096, 0.8379, 0.8045, 0.7619, 0.7029]\n",
      "MAP: [0.9815, 0.9779, 0.9767, 0.9707, 0.9667, 0.9659]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.14----\n",
      "Precisions: [0.977, 0.848, 0.7333, 0.665, 0.6146, 0.5855]\n",
      "Recall: [0.1075, 0.1824, 0.231, 0.2752, 0.3124, 0.3528]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9821, 0.8334, 0.7039, 0.6289, 0.5711, 0.5374]\n",
      "MAP: [0.9825, 0.9787, 0.9771, 0.9742, 0.9726, 0.9539]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.14----\n",
      "Precisions: [0.978, 0.815, 0.6583, 0.5743, 0.527, 0.497]\n",
      "Recall: [0.1075, 0.172, 0.2016, 0.2287, 0.2572, 0.2865]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9824, 0.7858, 0.6143, 0.5225, 0.4701, 0.4364]\n",
      "MAP: [0.9765, 0.9764, 0.9747, 0.9734, 0.9608, 0.9537]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.12----\n",
      "Precisions: [0.973, 0.86, 0.7013, 0.6195, 0.5438, 0.4848]\n",
      "Recall: [0.107, 0.1866, 0.2293, 0.2704, 0.294, 0.3102]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9783, 0.8528, 0.6986, 0.6178, 0.5375, 0.4725]\n",
      "MAP: [0.9765, 0.9751, 0.9737, 0.9704, 0.9682, 0.9678]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.12----\n",
      "Precisions: [0.976, 0.853, 0.6767, 0.5885, 0.5224, 0.4678]\n",
      "Recall: [0.1073, 0.1848, 0.2213, 0.2575, 0.283, 0.3003]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9808, 0.8444, 0.6742, 0.5883, 0.5174, 0.4574]\n",
      "MAP: [0.9849, 0.9799, 0.9772, 0.9732, 0.9686, 0.967]\n",
      "------------------------------------------------------\n",
      "Model catboost_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.12----\n",
      "Precisions: [0.974, 0.856, 0.7323, 0.7113, 0.7016, 0.6887]\n",
      "Recall: [0.1071, 0.1857, 0.233, 0.2936, 0.3557, 0.4136]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9791, 0.8488, 0.71, 0.6707, 0.6502, 0.63]\n",
      "MAP: [0.9756, 0.9753, 0.9675, 0.9264, 0.9132, 0.9107]\n",
      "------------------------------------------------------\n",
      "Model catboost_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.12----\n",
      "Precisions: [0.976, 0.8425, 0.7133, 0.6463, 0.6124, 0.5908]\n",
      "Recall: [0.1073, 0.1812, 0.2239, 0.2628, 0.3054, 0.3504]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9811, 0.8282, 0.6821, 0.6004, 0.5582, 0.5337]\n",
      "MAP: [0.9783, 0.9765, 0.9755, 0.9614, 0.9506, 0.9388]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.10----\n",
      "Precisions: [0.972, 0.8625, 0.7487, 0.6758, 0.6092, 0.5683]\n",
      "Recall: [0.107, 0.1869, 0.2393, 0.2822, 0.3132, 0.3465]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9781, 0.8543, 0.729, 0.6448, 0.5725, 0.5278]\n",
      "MAP: [0.9757, 0.9758, 0.975, 0.9682, 0.9608, 0.9534]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.10----\n",
      "Precisions: [0.974, 0.867, 0.762, 0.705, 0.6524, 0.6115]\n",
      "Recall: [0.1072, 0.1881, 0.2447, 0.2976, 0.3363, 0.3711]\n",
      "Max recall: [0.1094, 0.2188, 0.3282, 0.4377, 0.5471, 0.6565]\n",
      "Recall percentage: [0.9794, 0.8597, 0.7456, 0.6799, 0.6147, 0.5653]\n",
      "MAP: [0.9768, 0.9759, 0.9745, 0.9641, 0.9575, 0.953]\n",
      "------------------------------------------------------\n",
      "\n",
      "==================== MODEL RANKINGS ====================\n",
      " 1. Model: gradient_boosting_all_fs_deep | Avg Precision: 0.9010\n",
      " 2. Model: gradient_boosting_no_syntactic_fs_deep | Avg Precision: 0.8939\n",
      " 3. Model: gradient_boosting_all | Avg Precision: 0.8538\n",
      " 4. Model: gradient_boosting_all_fs | Avg Precision: 0.8538\n",
      " 5. Model: extra_trees_no_syntactic_fs_deep | Avg Precision: 0.8296\n",
      " 6. Model: catboost_all         | Avg Precision: 0.7773\n",
      " 7. Model: catboost_no_syntactic_fs | Avg Precision: 0.7620\n",
      " 8. Model: catboost_no_syntactic | Avg Precision: 0.7394\n",
      " 9. Model: extra_trees_all_fs_deep | Avg Precision: 0.7394\n",
      "10. Model: xgboosting_all       | Avg Precision: 0.7372\n",
      "11. Model: extra_trees_all      | Avg Precision: 0.7324\n",
      "12. Model: catboost_all_fs      | Avg Precision: 0.7302\n",
      "13. Model: extra_trees_all_fs   | Avg Precision: 0.7248\n",
      "14. Model: extra_trees_no_syntactic | Avg Precision: 0.7151\n",
      "15. Model: extra_trees_no_syntactic_fs | Avg Precision: 0.7119\n",
      "16. Model: xgboosting_no_syntactic | Avg Precision: 0.6971\n",
      "17. Model: xgboosting_no_syntactic_fs | Avg Precision: 0.6807\n",
      "18. Model: gradient_boosting_no_syntactic | Avg Precision: 0.6788\n",
      "19. Model: gradient_boosting_no_syntactic_fs | Avg Precision: 0.6788\n",
      "20. Model: xgboosting_all_fs    | Avg Precision: 0.6749\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 60\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_small/tus_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/benchmarks/tus_small/distances_all_metrics/'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VM0vyNGqYXfQ"
   },
   "source": [
    "### TUS Big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2994324,
     "status": "ok",
     "timestamp": 1722275450938,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "YuKfumOJYZu0",
    "outputId": "4fd7528b-e50d-41f6-ae76-04da47cdc727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:56<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.44----\n",
      "Precisions: [0.97, 0.9615, 0.942, 0.9245, 0.915, 0.906]\n",
      "Recall: [0.05, 0.0981, 0.1404, 0.179, 0.2182, 0.2545]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9471, 0.9296, 0.8871, 0.8483, 0.827, 0.804]\n",
      "MAP: [0.983, 0.9787, 0.9745, 0.9717, 0.9671, 0.9645]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:48<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.41----\n",
      "Precisions: [0.972, 0.9615, 0.9417, 0.924, 0.9128, 0.903]\n",
      "Recall: [0.0501, 0.0981, 0.1404, 0.1791, 0.2181, 0.2545]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9489, 0.9296, 0.8868, 0.8485, 0.8268, 0.8039]\n",
      "MAP: [0.985, 0.9804, 0.9755, 0.9727, 0.9682, 0.9657]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:42<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.36----\n",
      "Precisions: [0.984, 0.9705, 0.9437, 0.907, 0.8766, 0.8513]\n",
      "Recall: [0.051, 0.0996, 0.1425, 0.1768, 0.2085, 0.2377]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9667, 0.944, 0.9003, 0.8378, 0.7903, 0.7509]\n",
      "MAP: [0.9901, 0.9858, 0.9826, 0.9806, 0.9751, 0.9721]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:40<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.34----\n",
      "Precisions: [0.985, 0.972, 0.9433, 0.9073, 0.8772, 0.8515]\n",
      "Recall: [0.051, 0.0997, 0.1425, 0.1769, 0.2084, 0.2376]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9668, 0.9446, 0.9, 0.8381, 0.7901, 0.7506]\n",
      "MAP: [0.9917, 0.9875, 0.9841, 0.9819, 0.9758, 0.9729]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_all_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:43<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.40----\n",
      "Precisions: [0.957, 0.956, 0.9507, 0.946, 0.9344, 0.9217]\n",
      "Recall: [0.0486, 0.0963, 0.1421, 0.1869, 0.2257, 0.2623]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9214, 0.9125, 0.8978, 0.8856, 0.8554, 0.8285]\n",
      "MAP: [0.9747, 0.9679, 0.9638, 0.9626, 0.9612, 0.9591]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.34----\n",
      "Precisions: [0.987, 0.986, 0.9833, 0.9688, 0.957, 0.949]\n",
      "Recall: [0.0513, 0.1024, 0.1512, 0.1951, 0.2366, 0.2777]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9721, 0.9708, 0.9553, 0.9244, 0.8967, 0.8771]\n",
      "MAP: [0.9926, 0.9898, 0.989, 0.9889, 0.9868, 0.9846]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.55----\n",
      "Precisions: [0.971, 0.964, 0.939, 0.9108, 0.8884, 0.8645]\n",
      "Recall: [0.0498, 0.0964, 0.1383, 0.1737, 0.2081, 0.2395]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9434, 0.9133, 0.8739, 0.8229, 0.7888, 0.7565]\n",
      "MAP: [0.974, 0.9777, 0.9751, 0.9743, 0.9709, 0.9687]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.54----\n",
      "Precisions: [0.981, 0.974, 0.949, 0.9185, 0.8882, 0.8663]\n",
      "Recall: [0.049, 0.0945, 0.1351, 0.1699, 0.2009, 0.2324]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9294, 0.8955, 0.8533, 0.8049, 0.7617, 0.7339]\n",
      "MAP: [0.9867, 0.9862, 0.9868, 0.9871, 0.9859, 0.9838]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.46----\n",
      "Precisions: [0.981, 0.97, 0.954, 0.9317, 0.913, 0.898]\n",
      "Recall: [0.0498, 0.0959, 0.1393, 0.1744, 0.2094, 0.2436]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9435, 0.9083, 0.88, 0.8263, 0.7939, 0.7693]\n",
      "MAP: [0.9935, 0.9864, 0.9837, 0.9811, 0.9761, 0.9718]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:51<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.44----\n",
      "Precisions: [0.97, 0.962, 0.931, 0.9, 0.8668, 0.8382]\n",
      "Recall: [0.0489, 0.0959, 0.1348, 0.1672, 0.1946, 0.2204]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9276, 0.9086, 0.8515, 0.7921, 0.7375, 0.6961]\n",
      "MAP: [0.99, 0.9835, 0.9813, 0.98, 0.9777, 0.9745]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_all_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:59<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.51----\n",
      "Precisions: [0.959, 0.947, 0.9183, 0.8942, 0.8662, 0.8387]\n",
      "Recall: [0.0505, 0.0951, 0.1329, 0.1681, 0.2004, 0.2278]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9565, 0.9013, 0.8397, 0.7964, 0.7595, 0.7196]\n",
      "MAP: [0.9657, 0.9667, 0.9673, 0.9651, 0.9636, 0.9631]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:47<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.43----\n",
      "Precisions: [0.979, 0.9675, 0.9473, 0.9172, 0.8952, 0.8797]\n",
      "Recall: [0.0492, 0.0949, 0.1369, 0.1718, 0.2043, 0.2366]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9328, 0.8997, 0.8651, 0.8138, 0.7743, 0.7472]\n",
      "MAP: [0.9945, 0.9921, 0.9908, 0.9898, 0.9842, 0.9798]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.41----\n",
      "Precisions: [0.988, 0.975, 0.9403, 0.9105, 0.8858, 0.8547]\n",
      "Recall: [0.0523, 0.1011, 0.1405, 0.175, 0.2087, 0.2358]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9909, 0.9582, 0.8879, 0.8292, 0.7909, 0.7448]\n",
      "MAP: [0.9998, 0.9977, 0.9954, 0.9865, 0.9835, 0.9803]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.40----\n",
      "Precisions: [0.984, 0.9625, 0.9247, 0.8878, 0.8586, 0.8303]\n",
      "Recall: [0.0521, 0.0978, 0.1347, 0.1664, 0.1981, 0.2271]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9866, 0.9271, 0.8508, 0.7887, 0.751, 0.7173]\n",
      "MAP: [0.9956, 0.9929, 0.9901, 0.9826, 0.9768, 0.9716]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:44<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.34----\n",
      "Precisions: [0.984, 0.9645, 0.943, 0.92, 0.9018, 0.8818]\n",
      "Recall: [0.0519, 0.0986, 0.1412, 0.1789, 0.215, 0.2465]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.984, 0.9339, 0.8917, 0.8475, 0.815, 0.7785]\n",
      "MAP: [0.9935, 0.9935, 0.9864, 0.9804, 0.9775, 0.9749]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.34----\n",
      "Precisions: [0.989, 0.9685, 0.944, 0.923, 0.907, 0.8823]\n",
      "Recall: [0.052, 0.0989, 0.1412, 0.1799, 0.2178, 0.2491]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9859, 0.9369, 0.8922, 0.8523, 0.8256, 0.7868]\n",
      "MAP: [0.9984, 0.9976, 0.9924, 0.987, 0.98, 0.9775]\n",
      "------------------------------------------------------\n",
      "Model catboost_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.40----\n",
      "Precisions: [0.991, 0.9725, 0.954, 0.9385, 0.9208, 0.909]\n",
      "Recall: [0.0514, 0.099, 0.1425, 0.1821, 0.2201, 0.2583]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9734, 0.9386, 0.9001, 0.8629, 0.8345, 0.8158]\n",
      "MAP: [0.9974, 0.9933, 0.9883, 0.9842, 0.9803, 0.977]\n",
      "------------------------------------------------------\n",
      "Model catboost_all_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.40----\n",
      "Precisions: [0.983, 0.967, 0.947, 0.9318, 0.9182, 0.8987]\n",
      "Recall: [0.0511, 0.0986, 0.1411, 0.1819, 0.2221, 0.2562]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9687, 0.934, 0.8914, 0.8621, 0.8419, 0.8092]\n",
      "MAP: [0.9962, 0.9911, 0.9862, 0.9818, 0.9736, 0.9704]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:41<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.34----\n",
      "Precisions: [0.981, 0.961, 0.935, 0.9108, 0.879, 0.8548]\n",
      "Recall: [0.0509, 0.0965, 0.1378, 0.1759, 0.2072, 0.239]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.9651, 0.9142, 0.8703, 0.8334, 0.7853, 0.755]\n",
      "MAP: [0.9969, 0.9918, 0.9842, 0.9784, 0.9736, 0.9676]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:40<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.33----\n",
      "Precisions: [0.984, 0.9715, 0.9457, 0.9193, 0.89, 0.8643]\n",
      "Recall: [0.0507, 0.0979, 0.1378, 0.1732, 0.2053, 0.2373]\n",
      "Max recall: [0.0528, 0.1055, 0.1583, 0.2111, 0.2638, 0.3166]\n",
      "Recall percentage: [0.96, 0.9274, 0.8707, 0.8207, 0.7782, 0.7497]\n",
      "MAP: [0.9978, 0.9905, 0.9867, 0.9846, 0.9831, 0.9772]\n",
      "------------------------------------------------------\n",
      "\n",
      "==================== MODEL RANKINGS ====================\n",
      " 1. Model: gradient_boosting_no_syntactic_fs_deep | Avg Precision: 0.9718\n",
      " 2. Model: catboost_all         | Avg Precision: 0.9476\n",
      " 3. Model: gradient_boosting_all_fs_deep | Avg Precision: 0.9443\n",
      " 4. Model: extra_trees_no_syntactic | Avg Precision: 0.9413\n",
      " 5. Model: catboost_all_fs      | Avg Precision: 0.9409\n",
      " 6. Model: gradient_boosting_all | Avg Precision: 0.9365\n",
      " 7. Model: gradient_boosting_all_fs | Avg Precision: 0.9358\n",
      " 8. Model: xgboosting_no_syntactic_fs | Avg Precision: 0.9356\n",
      " 9. Model: xgboosting_no_syntactic | Avg Precision: 0.9325\n",
      "10. Model: extra_trees_no_syntactic_fs_deep | Avg Precision: 0.9310\n",
      "11. Model: extra_trees_all_fs   | Avg Precision: 0.9295\n",
      "12. Model: catboost_no_syntactic_fs | Avg Precision: 0.9291\n",
      "13. Model: xgboosting_all       | Avg Precision: 0.9257\n",
      "14. Model: extra_trees_all      | Avg Precision: 0.9230\n",
      "15. Model: gradient_boosting_no_syntactic_fs | Avg Precision: 0.9227\n",
      "16. Model: gradient_boosting_no_syntactic | Avg Precision: 0.9222\n",
      "17. Model: catboost_no_syntactic | Avg Precision: 0.9203\n",
      "18. Model: extra_trees_no_syntactic_fs | Avg Precision: 0.9113\n",
      "19. Model: xgboosting_all_fs    | Avg Precision: 0.9080\n",
      "20. Model: extra_trees_all_fs_deep | Avg Precision: 0.9039\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 60\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_big/tus_big_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/benchmarks/tus_big/distances_all_metrics/'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnqQe3V23ARd"
   },
   "source": [
    "### D3L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404500,
     "status": "ok",
     "timestamp": 1722350932795,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "PC4Gfm_l3DQ8",
    "outputId": "203cd8f0-f08b-4f9c-8141-d0e36037080b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.733, 0.673, 0.5917, 0.5038, 0.4414, 0.3947, 0.3597, 0.3319, 0.3106, 0.2931]\n",
      "Recall: [0.0516, 0.0949, 0.1252, 0.1423, 0.1558, 0.1671, 0.1776, 0.1872, 0.1969, 0.2064]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7375, 0.6783, 0.5965, 0.5084, 0.4452, 0.3979, 0.3625, 0.3343, 0.3127, 0.295]\n",
      "MAP: [0.724, 0.7601, 0.7701, 0.7709, 0.7698, 0.7702, 0.7701, 0.7707, 0.7705, 0.7709]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.733, 0.6725, 0.5913, 0.5035, 0.4418, 0.3948, 0.3596, 0.3318, 0.3103, 0.2927]\n",
      "Recall: [0.0516, 0.0949, 0.1252, 0.1422, 0.156, 0.1672, 0.1775, 0.1871, 0.1968, 0.2062]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7375, 0.6778, 0.5962, 0.5081, 0.4458, 0.3981, 0.3624, 0.3342, 0.3125, 0.2947]\n",
      "MAP: [0.724, 0.7599, 0.77, 0.7707, 0.7697, 0.7702, 0.7702, 0.7708, 0.7705, 0.7709]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.814, 0.821, 0.842, 0.8155, 0.7896, 0.7953, 0.8014, 0.7894, 0.7709, 0.7546]\n",
      "Recall: [0.057, 0.1151, 0.1769, 0.228, 0.2755, 0.3322, 0.3898, 0.438, 0.4806, 0.522]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.815, 0.8227, 0.8426, 0.8147, 0.7874, 0.7911, 0.7957, 0.7824, 0.7631, 0.7459]\n",
      "MAP: [0.7453, 0.7972, 0.8129, 0.8219, 0.8246, 0.8233, 0.8254, 0.8288, 0.8325, 0.8361]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [0.797, 0.7435, 0.6803, 0.6165, 0.568, 0.55, 0.5477, 0.541, 0.531, 0.5258]\n",
      "Recall: [0.0558, 0.1043, 0.1434, 0.1734, 0.1998, 0.2315, 0.2681, 0.3019, 0.3325, 0.3651]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7969, 0.7453, 0.6829, 0.6196, 0.5711, 0.5513, 0.5474, 0.5393, 0.5279, 0.5218]\n",
      "MAP: [0.798, 0.8132, 0.8037, 0.796, 0.7841, 0.7585, 0.7406, 0.7327, 0.7278, 0.7237]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [0.744, 0.689, 0.6183, 0.568, 0.5478, 0.5435, 0.5496, 0.5444, 0.5359, 0.5317]\n",
      "Recall: [0.0521, 0.0966, 0.1302, 0.1592, 0.1915, 0.2275, 0.268, 0.3028, 0.3347, 0.3684]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7451, 0.6901, 0.62, 0.5686, 0.5472, 0.5418, 0.547, 0.5409, 0.5315, 0.5265]\n",
      "MAP: [0.7558, 0.7698, 0.7611, 0.7468, 0.7295, 0.7093, 0.6941, 0.6902, 0.6889, 0.6861]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [0.653, 0.712, 0.709, 0.7033, 0.698, 0.6625, 0.6454, 0.6361, 0.6238, 0.6152]\n",
      "Recall: [0.0461, 0.1003, 0.1497, 0.1976, 0.2442, 0.2771, 0.314, 0.3529, 0.3887, 0.4255]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.6592, 0.7164, 0.7133, 0.7059, 0.6978, 0.66, 0.641, 0.6304, 0.6171, 0.6081]\n",
      "MAP: [0.6067, 0.6752, 0.7023, 0.7208, 0.733, 0.7374, 0.7412, 0.7434, 0.7459, 0.7456]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.06----\n",
      "Precisions: [0.762, 0.6905, 0.652, 0.5943, 0.5394, 0.5033, 0.461, 0.4269, 0.393, 0.3669]\n",
      "Recall: [0.0535, 0.0974, 0.138, 0.1678, 0.1904, 0.2125, 0.2268, 0.2399, 0.2484, 0.2576]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7641, 0.6956, 0.6575, 0.5996, 0.5442, 0.5061, 0.4629, 0.4286, 0.3944, 0.368]\n",
      "MAP: [0.7655, 0.7793, 0.7721, 0.7652, 0.7554, 0.7416, 0.7348, 0.7292, 0.7265, 0.7195]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.05----\n",
      "Precisions: [0.75, 0.6775, 0.6357, 0.5743, 0.5276, 0.4995, 0.4571, 0.424, 0.3894, 0.3625]\n",
      "Recall: [0.0527, 0.0955, 0.1345, 0.162, 0.186, 0.2109, 0.2248, 0.2382, 0.246, 0.2544]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7526, 0.6824, 0.6406, 0.5787, 0.5316, 0.5022, 0.459, 0.4255, 0.3906, 0.3636]\n",
      "MAP: [0.7581, 0.7743, 0.7647, 0.7557, 0.7404, 0.7229, 0.7193, 0.7136, 0.7117, 0.7073]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.759, 0.696, 0.5897, 0.492, 0.4346, 0.4072, 0.3971, 0.3891, 0.3843, 0.3742]\n",
      "Recall: [0.0533, 0.098, 0.1249, 0.1391, 0.1533, 0.1721, 0.1953, 0.2181, 0.2418, 0.261]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7621, 0.7001, 0.5947, 0.4968, 0.4381, 0.4098, 0.3987, 0.3896, 0.3839, 0.373]\n",
      "MAP: [0.7261, 0.7646, 0.7697, 0.7686, 0.7564, 0.7349, 0.7124, 0.6978, 0.6869, 0.679]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.777, 0.7075, 0.6273, 0.5443, 0.4966, 0.481, 0.4779, 0.4714, 0.4654, 0.4567]\n",
      "Recall: [0.0546, 0.0996, 0.1329, 0.154, 0.1753, 0.2029, 0.2343, 0.2633, 0.2917, 0.3174]\n",
      "Max recall: [0.07, 0.14, 0.2099, 0.2799, 0.3499, 0.4199, 0.4899, 0.5598, 0.6298, 0.6998]\n",
      "Recall percentage: [0.7797, 0.7116, 0.633, 0.55, 0.5009, 0.4832, 0.4783, 0.4702, 0.4632, 0.4536]\n",
      "MAP: [0.7888, 0.8067, 0.8019, 0.796, 0.7807, 0.7505, 0.7293, 0.7205, 0.7156, 0.7139]\n",
      "------------------------------------------------------\n",
      "\n",
      "==================== MODEL RANKINGS ====================\n",
      " 1. Model: gradient_boosting_no_syntactic_fs_deep | Avg Precision: 0.7994\n",
      " 2. Model: extra_trees_no_syntactic_fs_deep | Avg Precision: 0.6658\n",
      " 3. Model: extra_trees_no_syntactic | Avg Precision: 0.6101\n",
      " 4. Model: extra_trees_no_syntactic_fs | Avg Precision: 0.5872\n",
      " 5. Model: catboost_no_syntactic_fs | Avg Precision: 0.5505\n",
      " 6. Model: xgboosting_no_syntactic | Avg Precision: 0.5389\n",
      " 7. Model: xgboosting_no_syntactic_fs | Avg Precision: 0.5298\n",
      " 8. Model: catboost_no_syntactic | Avg Precision: 0.4923\n",
      " 9. Model: gradient_boosting_no_syntactic | Avg Precision: 0.4633\n",
      "10. Model: gradient_boosting_no_syntactic_fs | Avg Precision: 0.4631\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/d3l/d3l_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/benchmarks/d3l/distances/'\n",
    "\n",
    "custom_models = {}\n",
    "custom_typologies = [\"no_syntactic\", \"no_syntactic_fs\", \"no_syntactic_fs_deep\"]\n",
    "\n",
    "for name in best_names:\n",
    "    for type in custom_typologies:\n",
    "        try:\n",
    "            custom_models[f\"{name}_{type}\"] = joblib.load(models_path / type / f\"{name}_{type}.pkl\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, custom_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VqebH-e7ctX"
   },
   "source": [
    "### Freyja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5114,
     "status": "ok",
     "timestamp": 1722350938831,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "Kwv7xFdL7a_v",
    "outputId": "9480de4c-9da3-49f0-eca5-a101b557d14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 32.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.01----\n",
      "Precisions: [1.0, 0.94, 0.9333, 0.925, 0.92, 0.9233, 0.9086, 0.89, 0.86, 0.842]\n",
      "Recall: [0.0309, 0.0579, 0.0863, 0.1139, 0.1419, 0.1709, 0.1962, 0.2199, 0.2395, 0.261]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9359, 0.9301, 0.9213, 0.9182, 0.9213, 0.9066, 0.8889, 0.8606, 0.8441]\n",
      "MAP: [1.0, 1.0, 0.9833, 0.975, 0.9649, 0.9578, 0.9535, 0.9513, 0.9489, 0.9439]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 38.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.01----\n",
      "Precisions: [1.0, 0.94, 0.9267, 0.92, 0.916, 0.9233, 0.9086, 0.89, 0.86, 0.842]\n",
      "Recall: [0.0309, 0.0579, 0.0856, 0.1132, 0.1412, 0.1709, 0.1962, 0.2199, 0.2395, 0.261]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9359, 0.9227, 0.9157, 0.9137, 0.9213, 0.9066, 0.8889, 0.8606, 0.8441]\n",
      "MAP: [1.0, 1.0, 0.9867, 0.9739, 0.9628, 0.9555, 0.9517, 0.9498, 0.9476, 0.9428]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 44.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.01----\n",
      "Precisions: [1.0, 0.97, 0.96, 0.95, 0.952, 0.9467, 0.9429, 0.93, 0.9267, 0.926]\n",
      "Recall: [0.0309, 0.0598, 0.0888, 0.1171, 0.147, 0.1757, 0.2039, 0.23, 0.2581, 0.2868]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9673, 0.957, 0.9469, 0.9512, 0.947, 0.9423, 0.9299, 0.9275, 0.9276]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9944, 0.9858, 0.9837, 0.9812, 0.9792, 0.9769, 0.9665]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 26.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.02----\n",
      "Precisions: [1.0, 0.95, 0.9467, 0.935, 0.92, 0.9167, 0.9086, 0.9, 0.8844, 0.868]\n",
      "Recall: [0.0309, 0.0586, 0.0876, 0.1153, 0.142, 0.1697, 0.1961, 0.2227, 0.2464, 0.2691]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9474, 0.9447, 0.9322, 0.9184, 0.9147, 0.9063, 0.9002, 0.8853, 0.8704]\n",
      "MAP: [1.0, 1.0, 0.99, 0.9828, 0.9736, 0.9689, 0.9663, 0.9564, 0.954, 0.9503]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 30.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.01----\n",
      "Precisions: [1.0, 0.97, 0.94, 0.935, 0.932, 0.93, 0.9257, 0.9125, 0.8889, 0.87]\n",
      "Recall: [0.0309, 0.06, 0.0873, 0.1156, 0.1442, 0.1726, 0.2006, 0.2264, 0.2482, 0.27]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9697, 0.9412, 0.9346, 0.9329, 0.9305, 0.9271, 0.9155, 0.892, 0.8733]\n",
      "MAP: [1.0, 1.0, 0.9933, 0.9878, 0.9737, 0.9673, 0.9639, 0.9629, 0.9615, 0.9585]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 38.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.01----\n",
      "Precisions: [1.0, 0.95, 0.92, 0.925, 0.936, 0.93, 0.9143, 0.915, 0.9067, 0.898]\n",
      "Recall: [0.0309, 0.0586, 0.0853, 0.1142, 0.1444, 0.1721, 0.1977, 0.2264, 0.2529, 0.2784]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9481, 0.9191, 0.9235, 0.9343, 0.928, 0.9136, 0.9155, 0.9087, 0.9003]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9861, 0.9659, 0.9617, 0.9598, 0.9546, 0.9514, 0.9491]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 22.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.02----\n",
      "Precisions: [1.0, 0.95, 0.9533, 0.94, 0.912, 0.9033, 0.9029, 0.8625, 0.8489, 0.834]\n",
      "Recall: [0.0309, 0.0586, 0.0883, 0.1161, 0.141, 0.1675, 0.1953, 0.2132, 0.2365, 0.2586]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9484, 0.9521, 0.9388, 0.912, 0.9031, 0.9025, 0.862, 0.8501, 0.8365]\n",
      "MAP: [1.0, 1.0, 0.9867, 0.9828, 0.9741, 0.9692, 0.9637, 0.9618, 0.9567, 0.9526]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.02----\n",
      "Precisions: [1.0, 0.96, 0.9467, 0.925, 0.912, 0.8933, 0.8857, 0.855, 0.8356, 0.826]\n",
      "Recall: [0.0309, 0.0593, 0.0877, 0.1142, 0.1409, 0.1655, 0.1914, 0.211, 0.2324, 0.2558]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9585, 0.9454, 0.9231, 0.9117, 0.892, 0.8843, 0.853, 0.8353, 0.8272]\n",
      "MAP: [1.0, 1.0, 0.99, 0.985, 0.9738, 0.9648, 0.9575, 0.9544, 0.9511, 0.9462]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 32.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.01----\n",
      "Precisions: [1.0, 0.97, 0.9467, 0.935, 0.92, 0.91, 0.8971, 0.875, 0.8689, 0.858]\n",
      "Recall: [0.0309, 0.0599, 0.0875, 0.1152, 0.142, 0.1688, 0.1943, 0.2167, 0.2423, 0.2663]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9686, 0.9433, 0.9317, 0.9184, 0.91, 0.898, 0.876, 0.8707, 0.8613]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9878, 0.9749, 0.969, 0.9649, 0.9636, 0.9565, 0.9524]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 35.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.01----\n",
      "Precisions: [1.0, 0.94, 0.9, 0.88, 0.872, 0.8467, 0.8343, 0.8275, 0.8067, 0.798]\n",
      "Recall: [0.0309, 0.058, 0.0832, 0.1084, 0.1343, 0.1563, 0.1803, 0.2047, 0.2249, 0.2475]\n",
      "Max recall: [0.0309, 0.0618, 0.0928, 0.1237, 0.1546, 0.1855, 0.2164, 0.2473, 0.2783, 0.3092]\n",
      "Recall percentage: [1.0, 0.9383, 0.8968, 0.8766, 0.8685, 0.8426, 0.8332, 0.8274, 0.8083, 0.8006]\n",
      "MAP: [1.0, 1.0, 0.9967, 0.9911, 0.9597, 0.9471, 0.939, 0.929, 0.9236, 0.9139]\n",
      "------------------------------------------------------\n",
      "\n",
      "==================== MODEL RANKINGS ====================\n",
      " 1. Model: gradient_boosting_no_syntactic_fs_deep | Avg Precision: 0.9504\n",
      " 2. Model: extra_trees_no_syntactic_fs | Avg Precision: 0.9304\n",
      " 3. Model: extra_trees_no_syntactic_fs_deep | Avg Precision: 0.9295\n",
      " 4. Model: extra_trees_no_syntactic | Avg Precision: 0.9229\n",
      " 5. Model: catboost_no_syntactic | Avg Precision: 0.9181\n",
      " 6. Model: gradient_boosting_no_syntactic | Avg Precision: 0.9142\n",
      " 7. Model: gradient_boosting_no_syntactic_fs | Avg Precision: 0.9127\n",
      " 8. Model: xgboosting_no_syntactic | Avg Precision: 0.9107\n",
      " 9. Model: xgboosting_no_syntactic_fs | Avg Precision: 0.9039\n",
      "10. Model: catboost_no_syntactic_fs | Avg Precision: 0.8705\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "step = 1\n",
    "ground_truth_path = 'C:/Projects/benchmarks/freyja/freyja_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/benchmarks/freyja/distances/'\n",
    "\n",
    "custom_models = {}\n",
    "custom_typologies = [\"no_syntactic\", \"no_syntactic_fs\", \"no_syntactic_fs_deep\"]\n",
    "\n",
    "for name in best_names:\n",
    "    for type in custom_typologies:\n",
    "        try:\n",
    "            custom_models[f\"{name}_{type}\"] = joblib.load(models_path / type / f\"{name}_{type}.pkl\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, custom_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OM CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.46, 0.4, 0.348, 0.328, 0.304, 0.2813]\n",
      "Recall: [0.0722, 0.1254, 0.164, 0.206, 0.2384, 0.2647]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.4602, 0.3998, 0.3486, 0.3284, 0.304, 0.2813]\n",
      "MAP: [0.538, 0.5533, 0.5486, 0.54, 0.5386, 0.5382]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.02----\n",
      "Precisions: [0.452, 0.396, 0.3467, 0.331, 0.304, 0.2793]\n",
      "Recall: [0.0709, 0.1242, 0.1635, 0.2079, 0.2385, 0.2627]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.4523, 0.3958, 0.3474, 0.3313, 0.3041, 0.2792]\n",
      "MAP: [0.5471, 0.5613, 0.5526, 0.5433, 0.5433, 0.5434]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.02----\n",
      "Precisions: [0.552, 0.558, 0.5467, 0.539, 0.5272, 0.5267]\n",
      "Recall: [0.0864, 0.1748, 0.2567, 0.3378, 0.4133, 0.4953]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.5507, 0.5572, 0.5455, 0.5385, 0.527, 0.5263]\n",
      "MAP: [0.6074, 0.6191, 0.6005, 0.593, 0.5884, 0.5839]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.472, 0.496, 0.4947, 0.496, 0.4864, 0.4747]\n",
      "Recall: [0.074, 0.1557, 0.233, 0.3115, 0.3817, 0.4469]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.4717, 0.4962, 0.4953, 0.4965, 0.4868, 0.4749]\n",
      "MAP: [0.5368, 0.5472, 0.545, 0.543, 0.5436, 0.5447]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.496, 0.506, 0.512, 0.524, 0.5072, 0.488]\n",
      "Recall: [0.0776, 0.1584, 0.2411, 0.3288, 0.3971, 0.4586]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.4948, 0.5049, 0.5123, 0.524, 0.5064, 0.4873]\n",
      "MAP: [0.5536, 0.5592, 0.5512, 0.5551, 0.5554, 0.5585]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 24.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.496, 0.508, 0.484, 0.476, 0.4576, 0.4407]\n",
      "Recall: [0.0777, 0.1593, 0.2277, 0.2982, 0.3583, 0.4137]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.4955, 0.5077, 0.4839, 0.4753, 0.4569, 0.4396]\n",
      "MAP: [0.5696, 0.5736, 0.5701, 0.559, 0.5545, 0.5524]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.524, 0.508, 0.5067, 0.492, 0.4648, 0.4393]\n",
      "Recall: [0.082, 0.1593, 0.2389, 0.3087, 0.3644, 0.414]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.5228, 0.508, 0.5078, 0.492, 0.4647, 0.44]\n",
      "MAP: [0.5881, 0.5879, 0.5732, 0.5706, 0.566, 0.5646]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.488, 0.524, 0.4973, 0.475, 0.4424, 0.4127]\n",
      "Recall: [0.0765, 0.1643, 0.2345, 0.2985, 0.3476, 0.3896]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.4874, 0.5237, 0.4984, 0.4758, 0.4432, 0.414]\n",
      "MAP: [0.5507, 0.5647, 0.5642, 0.5621, 0.5572, 0.5512]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 24.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.516, 0.522, 0.476, 0.461, 0.4392, 0.4113]\n",
      "Recall: [0.0811, 0.1641, 0.2241, 0.2894, 0.3444, 0.3873]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.5171, 0.5232, 0.4764, 0.4614, 0.4391, 0.4115]\n",
      "MAP: [0.5929, 0.5772, 0.5695, 0.5638, 0.5586, 0.5508]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 26.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.468, 0.39, 0.356, 0.329, 0.3064, 0.28]\n",
      "Recall: [0.0735, 0.1225, 0.1677, 0.2068, 0.2409, 0.2642]\n",
      "Max recall: [0.1568, 0.3137, 0.4705, 0.6274, 0.7842, 0.941]\n",
      "Recall percentage: [0.4687, 0.3906, 0.3565, 0.3296, 0.3072, 0.2807]\n",
      "MAP: [0.5874, 0.5766, 0.5748, 0.5696, 0.5686, 0.5675]\n",
      "------------------------------------------------------\n",
      "\n",
      "==================== MODEL RANKINGS ====================\n",
      " 1. Model: gradient_boosting_no_syntactic_fs_deep | Avg Precision: 0.5416\n",
      " 2. Model: extra_trees_no_syntactic_fs | Avg Precision: 0.5055\n",
      " 3. Model: xgboosting_no_syntactic | Avg Precision: 0.4891\n",
      " 4. Model: extra_trees_no_syntactic | Avg Precision: 0.4866\n",
      " 5. Model: extra_trees_no_syntactic_fs_deep | Avg Precision: 0.4770\n",
      " 6. Model: xgboosting_no_syntactic_fs | Avg Precision: 0.4732\n",
      " 7. Model: catboost_no_syntactic | Avg Precision: 0.4709\n",
      " 8. Model: catboost_no_syntactic_fs | Avg Precision: 0.3549\n",
      " 9. Model: gradient_boosting_no_syntactic | Avg Precision: 0.3536\n",
      "10. Model: gradient_boosting_no_syntactic_fs | Avg Precision: 0.3515\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "step = 5\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_city_government/omnimatch_city_government_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/benchmarks/omnimatch_city_government/distances/'\n",
    "\n",
    "custom_models = {}\n",
    "custom_typologies = [\"no_syntactic\", \"no_syntactic_fs\", \"no_syntactic_fs_deep\"]\n",
    "\n",
    "for name in best_names:\n",
    "    for type in custom_typologies:\n",
    "        try:\n",
    "            custom_models[f\"{name}_{type}\"] = joblib.load(models_path / type / f\"{name}_{type}.pkl\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, custom_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OM CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.448, 0.436, 0.416, 0.403, 0.4152, 0.4207]\n",
      "Recall: [0.0645, 0.1256, 0.1791, 0.2316, 0.2983, 0.3626]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.4467, 0.4348, 0.4133, 0.401, 0.4131, 0.4185]\n",
      "MAP: [0.5071, 0.5224, 0.5244, 0.522, 0.5081, 0.5088]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.02----\n",
      "Precisions: [0.456, 0.43, 0.4133, 0.391, 0.4072, 0.4147]\n",
      "Recall: [0.0658, 0.1238, 0.1781, 0.2245, 0.2925, 0.3577]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.4556, 0.4288, 0.4111, 0.3887, 0.4051, 0.4128]\n",
      "MAP: [0.5062, 0.52, 0.5151, 0.5112, 0.5039, 0.5026]\n",
      "------------------------------------------------------\n",
      "Model gradient_boosting_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.02----\n",
      "Precisions: [0.564, 0.568, 0.5733, 0.567, 0.5648, 0.5573]\n",
      "Recall: [0.0809, 0.1623, 0.2457, 0.3246, 0.404, 0.4773]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.5599, 0.5619, 0.5672, 0.5619, 0.5595, 0.5508]\n",
      "MAP: [0.5724, 0.6127, 0.6097, 0.6086, 0.6097, 0.6065]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.536, 0.54, 0.496, 0.482, 0.4824, 0.4647]\n",
      "Recall: [0.0765, 0.1549, 0.2139, 0.2779, 0.3479, 0.4023]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.5294, 0.5365, 0.4937, 0.4811, 0.4819, 0.4642]\n",
      "MAP: [0.5771, 0.6062, 0.6016, 0.5859, 0.5747, 0.5671]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.512, 0.544, 0.5347, 0.529, 0.504, 0.4767]\n",
      "Recall: [0.0725, 0.1555, 0.2298, 0.3039, 0.3617, 0.4105]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.502, 0.5384, 0.5304, 0.526, 0.5009, 0.4737]\n",
      "MAP: [0.5562, 0.5884, 0.5845, 0.5823, 0.5724, 0.5665]\n",
      "------------------------------------------------------\n",
      "Model extra_trees_no_syntactic_fs_deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 25.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.492, 0.492, 0.4973, 0.509, 0.5152, 0.516]\n",
      "Recall: [0.0699, 0.1398, 0.2128, 0.2906, 0.3678, 0.4415]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.4842, 0.4841, 0.4912, 0.5031, 0.5094, 0.5095]\n",
      "MAP: [0.5311, 0.551, 0.5528, 0.5503, 0.5498, 0.5535]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.448, 0.438, 0.4147, 0.401, 0.3976, 0.3867]\n",
      "Recall: [0.0645, 0.1259, 0.1788, 0.2309, 0.2865, 0.3347]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.4463, 0.436, 0.4126, 0.3996, 0.3968, 0.3862]\n",
      "MAP: [0.4843, 0.5019, 0.4929, 0.4872, 0.4846, 0.4859]\n",
      "------------------------------------------------------\n",
      "Model xgboosting_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.04----\n",
      "Precisions: [0.412, 0.404, 0.4067, 0.397, 0.4008, 0.394]\n",
      "Recall: [0.0588, 0.1157, 0.175, 0.2283, 0.2886, 0.3404]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.4075, 0.4006, 0.4039, 0.3952, 0.3997, 0.3929]\n",
      "MAP: [0.4619, 0.488, 0.4762, 0.4665, 0.4623, 0.4643]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.524, 0.5, 0.4667, 0.452, 0.4232, 0.398]\n",
      "Recall: [0.0753, 0.1433, 0.2012, 0.2603, 0.3055, 0.3452]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.5215, 0.4963, 0.4645, 0.4507, 0.4231, 0.3984]\n",
      "MAP: [0.5574, 0.5763, 0.5773, 0.5561, 0.5472, 0.5417]\n",
      "------------------------------------------------------\n",
      "Model catboost_no_syntactic_fs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 25.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE time to load the distances and execute the model:\n",
      "----0.03----\n",
      "Precisions: [0.488, 0.466, 0.4467, 0.422, 0.4024, 0.3767]\n",
      "Recall: [0.0701, 0.1337, 0.1926, 0.2426, 0.2898, 0.3259]\n",
      "Max recall: [0.1444, 0.2888, 0.4332, 0.5776, 0.7221, 0.8665]\n",
      "Recall percentage: [0.4854, 0.4629, 0.4446, 0.4199, 0.4013, 0.3762]\n",
      "MAP: [0.5473, 0.5705, 0.5753, 0.5585, 0.5535, 0.5451]\n",
      "------------------------------------------------------\n",
      "\n",
      "==================== MODEL RANKINGS ====================\n",
      " 1. Model: gradient_boosting_no_syntactic_fs_deep | Avg Precision: 0.5657\n",
      " 2. Model: extra_trees_no_syntactic_fs | Avg Precision: 0.5167\n",
      " 3. Model: extra_trees_no_syntactic_fs_deep | Avg Precision: 0.5036\n",
      " 4. Model: extra_trees_no_syntactic | Avg Precision: 0.5002\n",
      " 5. Model: catboost_no_syntactic | Avg Precision: 0.4607\n",
      " 6. Model: catboost_no_syntactic_fs | Avg Precision: 0.4336\n",
      " 7. Model: gradient_boosting_no_syntactic | Avg Precision: 0.4231\n",
      " 8. Model: gradient_boosting_no_syntactic_fs | Avg Precision: 0.4187\n",
      " 9. Model: xgboosting_no_syntactic | Avg Precision: 0.4143\n",
      "10. Model: xgboosting_no_syntactic_fs | Avg Precision: 0.4024\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "step = 5\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_culture_recreation/omnimatch_culture_recreation_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/benchmarks/omnimatch_culture_recreation/distances/'\n",
    "\n",
    "custom_models = {}\n",
    "custom_typologies = [\"no_syntactic\", \"no_syntactic_fs\", \"no_syntactic_fs_deep\"]\n",
    "\n",
    "for name in best_names:\n",
    "    for type in custom_typologies:\n",
    "        try:\n",
    "            custom_models[f\"{name}_{type}\"] = joblib.load(models_path / type / f\"{name}_{type}.pkl\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, custom_models)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPphl2VZmBeT9V/QvoypH3j",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "freyja_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
