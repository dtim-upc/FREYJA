{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the final model\n",
    "Once we have obtained the continuous quality metric for join detection, we will use it to generate a predictive model that can efficiently, and in a very lightweight fashion, detect similarities between columns of a data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwFeRi_OjHzB",
    "outputId": "4bfea26a-41e2-41a1-833e-7314cb8bb917"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-svj3C4JrAFi"
   },
   "source": [
    "# Preparing the data\n",
    "\n",
    "We will:\n",
    "- Load the ground truth, which contains a subset of semantic joins, a subset of syntactic joins and a sample of the rest of joins.\n",
    "- Merge it with the distances. That is, for each selected pair \"add\" the distances between the metrics of their respective profiles\n",
    "- Remove unnecessary columns for the models (e.g. dataset and attribute names)\n",
    "\n",
    "**Important**: the `ground_truth_models.csv` file contains all the semantic and syntactic joins detected in the data lake + a sample of joins that do not have a relationship (i.e. containment < 0.1 and no semantic link), indicated with a *null* value in the relationships cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "error",
     "timestamp": 1753704098205,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "FZN3haMUrqTs",
    "outputId": "7aed1fd3-ce0c-4996-fcd5-a3110d5929a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of syntactic joins: 2703\n",
      "Number of semantic joins: 1701\n",
      "Number of unrelated pairs: 18206\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>containment</th>\n",
       "      <th>cardinality_proportion</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>multiset_jaccard</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22610.000000</td>\n",
       "      <td>2.261000e+04</td>\n",
       "      <td>22610.000000</td>\n",
       "      <td>22610.000000</td>\n",
       "      <td>22610.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.107225</td>\n",
       "      <td>2.154501e-01</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.004229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.242172</td>\n",
       "      <td>2.893796e-01</td>\n",
       "      <td>0.149750</td>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.033486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.251912e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.015228e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.189559e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>3.333333e-01</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.494872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        containment  cardinality_proportion       jaccard  multiset_jaccard  \\\n",
       "count  22610.000000            2.261000e+04  22610.000000      22610.000000   \n",
       "mean       0.107225            2.154501e-01      0.043204          0.010597   \n",
       "std        0.242172            2.893796e-01      0.149750          0.050149   \n",
       "min        0.000000            4.251912e-07      0.000000          0.000000   \n",
       "25%        0.000000            1.015228e-02      0.000000          0.000000   \n",
       "50%        0.000000            6.189559e-02      0.000000          0.000000   \n",
       "75%        0.045455            3.333333e-01      0.002380          0.000219   \n",
       "max        1.000000            1.000000e+00      1.000000          0.500000   \n",
       "\n",
       "            quality  \n",
       "count  22610.000000  \n",
       "mean       0.004229  \n",
       "std        0.033486  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000002  \n",
       "max        0.494872  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = pd.read_csv(f'C:/Projects/data/ground_truths/ground_truth_for_training_models.csv')\n",
    "ground_truth['relationship'] = ground_truth['relationship'].fillna('unrelated') # Pairs that are neither semantic or syntactic have a NaN. We change it by unrelated to prevent problems.\n",
    "\n",
    "count_syntactic = (ground_truth['relationship'] == 'syntactic').sum()\n",
    "count_semantic = (ground_truth['relationship'] == 'semantic').sum()\n",
    "count_unrelated = (ground_truth['relationship'] == 'unrelated').sum()\n",
    "\n",
    "print(f\"Number of syntactic joins: {count_syntactic}\")\n",
    "print(f\"Number of semantic joins: {count_semantic}\")\n",
    "print(f\"Number of unrelated pairs: {count_unrelated}\")\n",
    "\n",
    "ground_truth.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name_1</th>\n",
       "      <th>attribute_name_1</th>\n",
       "      <th>dataset_name_2</th>\n",
       "      <th>attribute_name_2</th>\n",
       "      <th>relationship</th>\n",
       "      <th>containment</th>\n",
       "      <th>cardinality_proportion</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>multiset_jaccard</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.757322</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>1.381513e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distributions_data_2016.csv</td>\n",
       "      <td>demographics</td>\n",
       "      <td>Tech_sector_diversity_demographics_2016.csv</td>\n",
       "      <td>raceEthnicity</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>1.041278e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA_cars_datasets.csv</td>\n",
       "      <td>country</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>4.659192e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World_countries_env_vars.csv</td>\n",
       "      <td>Country</td>\n",
       "      <td>world_city.csv</td>\n",
       "      <td>District</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.177892</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>6.364830e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>books_updated.csv</td>\n",
       "      <td>languageCode</td>\n",
       "      <td>countries_metadatacountries.csv</td>\n",
       "      <td>CountryCode</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>1.381148e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22605</th>\n",
       "      <td>pte_sulfo.csv</td>\n",
       "      <td>Set</td>\n",
       "      <td>AdventureWorks2014_shift.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22606</th>\n",
       "      <td>dataSpotifyClass.csv</td>\n",
       "      <td>song_title</td>\n",
       "      <td>netflix_titles.csv</td>\n",
       "      <td>description</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22607</th>\n",
       "      <td>pte_methoxy.csv</td>\n",
       "      <td>Arg0</td>\n",
       "      <td>countries_data.csv</td>\n",
       "      <td>1997</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22608</th>\n",
       "      <td>student-mat.csv</td>\n",
       "      <td>internet</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22609</th>\n",
       "      <td>ipums_la_98-small.csv</td>\n",
       "      <td>schltype</td>\n",
       "      <td>financial_order.csv</td>\n",
       "      <td>ksymbol</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>4.544962e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22610 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dataset_name_1 attribute_name_1  \\\n",
       "0      AdventureWorks2014_stateprovince.csv             Name   \n",
       "1               Distributions_data_2016.csv     demographics   \n",
       "2                     USA_cars_datasets.csv          country   \n",
       "3              World_countries_env_vars.csv          Country   \n",
       "4                         books_updated.csv     languageCode   \n",
       "...                                     ...              ...   \n",
       "22605                         pte_sulfo.csv              Set   \n",
       "22606                  dataSpotifyClass.csv       song_title   \n",
       "22607                       pte_methoxy.csv             Arg0   \n",
       "22608                       student-mat.csv         internet   \n",
       "22609                 ipums_la_98-small.csv         schltype   \n",
       "\n",
       "                                    dataset_name_2 attribute_name_2  \\\n",
       "0                                world_country.csv             Name   \n",
       "1      Tech_sector_diversity_demographics_2016.csv    raceEthnicity   \n",
       "2                                world_country.csv             Name   \n",
       "3                                   world_city.csv         District   \n",
       "4                  countries_metadatacountries.csv      CountryCode   \n",
       "...                                            ...              ...   \n",
       "22605                 AdventureWorks2014_shift.csv             Name   \n",
       "22606                           netflix_titles.csv      description   \n",
       "22607                           countries_data.csv             1997   \n",
       "22608         AdventureWorks2014_stateprovince.csv             Name   \n",
       "22609                          financial_order.csv          ksymbol   \n",
       "\n",
       "      relationship  containment  cardinality_proportion   jaccard  \\\n",
       "0        unrelated     0.044199                0.757322  0.019417   \n",
       "1        syntactic     0.230769                0.461538  0.187500   \n",
       "2         semantic     0.500000                0.008368  0.004167   \n",
       "3        unrelated     0.053498                0.177892  0.006250   \n",
       "4        syntactic     0.360000                0.101215  0.034091   \n",
       "...            ...          ...                     ...       ...   \n",
       "22605    unrelated     0.000000                0.120000  0.000000   \n",
       "22606    unrelated     0.000000                0.313414  0.000000   \n",
       "22607    unrelated     0.000000                0.004015  0.000000   \n",
       "22608    unrelated     0.000000                0.011050  0.000000   \n",
       "22609    unrelated     0.000000                0.750000  0.125000   \n",
       "\n",
       "       multiset_jaccard       quality  \n",
       "0              0.019048  1.381513e-03  \n",
       "1              0.000186  1.041278e-05  \n",
       "2              0.000365  4.659192e-07  \n",
       "3              0.002314  6.364830e-05  \n",
       "4              0.000878  1.381148e-05  \n",
       "...                 ...           ...  \n",
       "22605          0.000000  0.000000e+00  \n",
       "22606          0.000000  0.000000e+00  \n",
       "22607          0.000000  0.000000e+00  \n",
       "22608          0.000000  0.000000e+00  \n",
       "22609          0.000072  4.544962e-06  \n",
       "\n",
       "[22610 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_name_1</th>\n",
       "      <th>dataset_name_1</th>\n",
       "      <th>attribute_name_2</th>\n",
       "      <th>dataset_name_2</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>incompleteness</th>\n",
       "      <th>uniqueness</th>\n",
       "      <th>entropy</th>\n",
       "      <th>frequency_avg</th>\n",
       "      <th>frequency_min</th>\n",
       "      <th>...</th>\n",
       "      <th>number_words</th>\n",
       "      <th>words_cnt_max</th>\n",
       "      <th>words_cnt_min</th>\n",
       "      <th>words_cnt_avg</th>\n",
       "      <th>words_cnt_sd</th>\n",
       "      <th>first_word</th>\n",
       "      <th>last_word</th>\n",
       "      <th>is_empty</th>\n",
       "      <th>is_binary</th>\n",
       "      <th>name_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Name</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>-0.001747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.108910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.080856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002658</td>\n",
       "      <td>-0.040543</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demographics</td>\n",
       "      <td>Distributions_data_2016.csv</td>\n",
       "      <td>raceEthnicity</td>\n",
       "      <td>Tech_sector_diversity_demographics_2016.csv</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.135553</td>\n",
       "      <td>0.312345</td>\n",
       "      <td>0.460409</td>\n",
       "      <td>0.760277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>-0.020214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016396</td>\n",
       "      <td>-0.055986</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>country</td>\n",
       "      <td>USA_cars_datasets.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>-0.007140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.999200</td>\n",
       "      <td>-2.138181</td>\n",
       "      <td>0.468604</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024177</td>\n",
       "      <td>-0.121284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.043013</td>\n",
       "      <td>-0.141035</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Country</td>\n",
       "      <td>World_countries_env_vars.csv</td>\n",
       "      <td>District</td>\n",
       "      <td>world_city.csv</td>\n",
       "      <td>-0.033833</td>\n",
       "      <td>-0.000981</td>\n",
       "      <td>0.665114</td>\n",
       "      <td>-0.391313</td>\n",
       "      <td>-0.000744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055464</td>\n",
       "      <td>0.020214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019129</td>\n",
       "      <td>0.053482</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>languageCode</td>\n",
       "      <td>books_updated.csv</td>\n",
       "      <td>CountryCode</td>\n",
       "      <td>countries_metadatacountries.csv</td>\n",
       "      <td>-0.006688</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>-0.997500</td>\n",
       "      <td>-1.826932</td>\n",
       "      <td>0.133484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22605</th>\n",
       "      <td>Set</td>\n",
       "      <td>pte_sulfo.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>AdventureWorks2014_shift.csv</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.830738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22606</th>\n",
       "      <td>song_title</td>\n",
       "      <td>dataSpotifyClass.csv</td>\n",
       "      <td>description</td>\n",
       "      <td>netflix_titles.csv</td>\n",
       "      <td>-0.128763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.030943</td>\n",
       "      <td>-0.459097</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.594809</td>\n",
       "      <td>-0.525564</td>\n",
       "      <td>-14.020669</td>\n",
       "      <td>-1.844184</td>\n",
       "      <td>-0.086287</td>\n",
       "      <td>104</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22607</th>\n",
       "      <td>Arg0</td>\n",
       "      <td>pte_methoxy.csv</td>\n",
       "      <td>1997</td>\n",
       "      <td>countries_data.csv</td>\n",
       "      <td>-0.261564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148744</td>\n",
       "      <td>-0.609951</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22608</th>\n",
       "      <td>internet</td>\n",
       "      <td>student-mat.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.994937</td>\n",
       "      <td>-1.860018</td>\n",
       "      <td>0.073753</td>\n",
       "      <td>0.040177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>-0.040428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.040356</td>\n",
       "      <td>-0.100492</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22609</th>\n",
       "      <td>schltype</td>\n",
       "      <td>ipums_la_98-small.csv</td>\n",
       "      <td>ksymbol</td>\n",
       "      <td>financial_order.csv</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.496849</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>-0.152537</td>\n",
       "      <td>-0.206183</td>\n",
       "      <td>-0.136603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22610 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attribute_name_1                        dataset_name_1 attribute_name_2  \\\n",
       "0                 Name  AdventureWorks2014_stateprovince.csv             Name   \n",
       "1         demographics           Distributions_data_2016.csv    raceEthnicity   \n",
       "2              country                 USA_cars_datasets.csv             Name   \n",
       "3              Country          World_countries_env_vars.csv         District   \n",
       "4         languageCode                     books_updated.csv      CountryCode   \n",
       "...                ...                                   ...              ...   \n",
       "22605              Set                         pte_sulfo.csv             Name   \n",
       "22606       song_title                  dataSpotifyClass.csv      description   \n",
       "22607             Arg0                       pte_methoxy.csv             1997   \n",
       "22608         internet                       student-mat.csv             Name   \n",
       "22609         schltype                 ipums_la_98-small.csv          ksymbol   \n",
       "\n",
       "                                    dataset_name_2  cardinality  \\\n",
       "0                                world_country.csv    -0.001747   \n",
       "1      Tech_sector_diversity_demographics_2016.csv     0.000211   \n",
       "2                                world_country.csv    -0.007140   \n",
       "3                                   world_city.csv    -0.033833   \n",
       "4                  countries_metadatacountries.csv    -0.006688   \n",
       "...                                            ...          ...   \n",
       "22605                 AdventureWorks2014_shift.csv     0.000663   \n",
       "22606                           netflix_titles.csv    -0.128763   \n",
       "22607                           countries_data.csv    -0.261564   \n",
       "22608         AdventureWorks2014_stateprovince.csv    -0.005393   \n",
       "22609                          financial_order.csv    -0.000030   \n",
       "\n",
       "       incompleteness  uniqueness   entropy  frequency_avg  frequency_min  \\\n",
       "0            0.000000    0.000000 -0.108910       0.000000       0.000000   \n",
       "1            0.000000   -0.135553  0.312345       0.460409       0.760277   \n",
       "2            0.000000   -0.999200 -2.138181       0.468604       0.003709   \n",
       "3           -0.000981    0.665114 -0.391313      -0.000744       0.000000   \n",
       "4            0.108400   -0.997500 -1.826932       0.133484       0.000000   \n",
       "...               ...         ...       ...            ...            ...   \n",
       "22605        0.000000    0.000000  0.830738       0.000000       0.000000   \n",
       "22606        0.000000   -0.030943 -0.459097       0.000012       0.000000   \n",
       "22607        0.000000    0.148744 -0.609951      -0.000220       0.000000   \n",
       "22608        0.000000   -0.994937 -1.860018       0.073753       0.040177   \n",
       "22609        0.496849   -0.000217 -0.152537      -0.206183      -0.136603   \n",
       "\n",
       "       ...  number_words  words_cnt_max  words_cnt_min  words_cnt_avg  \\\n",
       "0      ...     -0.001025      -0.080856       0.000000      -0.002658   \n",
       "1      ...      0.180148      -0.020214       0.000000      -0.016396   \n",
       "2      ...      0.024177      -0.121284       0.000000      -0.043013   \n",
       "3      ...     -0.055464       0.020214       0.000000       0.019129   \n",
       "4      ...      0.097667       0.000000       0.000000       0.000000   \n",
       "...    ...           ...            ...            ...            ...   \n",
       "22605  ...      0.000248       0.000000       0.000000       0.000000   \n",
       "22606  ...     -1.594809      -0.525564     -14.020669      -1.844184   \n",
       "22607  ...     -0.225303       0.000000       0.000000       0.000000   \n",
       "22608  ...      0.001498      -0.040428       0.000000      -0.040356   \n",
       "22609  ...     -0.032909       0.000000       0.000000       0.000000   \n",
       "\n",
       "       words_cnt_sd  first_word  last_word  is_empty  is_binary  name_dist  \n",
       "0         -0.040543           8          7         0          0          0  \n",
       "1         -0.055986           4         13         0          0         12  \n",
       "2         -0.141035           8          7         0          1          7  \n",
       "3          0.053482          14          8         0          0          7  \n",
       "4          0.000000           2          2         0          0          8  \n",
       "...             ...         ...        ...       ...        ...        ...  \n",
       "22605      0.000000          39         36         0          0          4  \n",
       "22606     -0.086287         104        135         0          0          9  \n",
       "22607      0.000000          22          4         0          0          4  \n",
       "22608     -0.100492           4          6         0          1          7  \n",
       "22609      0.000000          19         12         0          0          8  \n",
       "\n",
       "[22610 rows x 39 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = pd.read_csv(f'C:/Projects/data/ground_truths/distances_for_training_models.csv')\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 19450,
     "status": "ok",
     "timestamp": 1736505249702,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "iXd8o58DTDUz",
    "outputId": "6edf9e8f-797c-4c1c-d4ae-477c74f60857"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name_1</th>\n",
       "      <th>attribute_name_1</th>\n",
       "      <th>dataset_name_2</th>\n",
       "      <th>attribute_name_2</th>\n",
       "      <th>relationship</th>\n",
       "      <th>containment</th>\n",
       "      <th>cardinality_proportion</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>multiset_jaccard</th>\n",
       "      <th>quality</th>\n",
       "      <th>...</th>\n",
       "      <th>number_words</th>\n",
       "      <th>words_cnt_max</th>\n",
       "      <th>words_cnt_min</th>\n",
       "      <th>words_cnt_avg</th>\n",
       "      <th>words_cnt_sd</th>\n",
       "      <th>first_word</th>\n",
       "      <th>last_word</th>\n",
       "      <th>is_empty</th>\n",
       "      <th>is_binary</th>\n",
       "      <th>name_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.757322</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>1.381513e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.080856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002658</td>\n",
       "      <td>-0.040543</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distributions_data_2016.csv</td>\n",
       "      <td>demographics</td>\n",
       "      <td>Tech_sector_diversity_demographics_2016.csv</td>\n",
       "      <td>raceEthnicity</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>1.041278e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>-0.020214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016396</td>\n",
       "      <td>-0.055986</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA_cars_datasets.csv</td>\n",
       "      <td>country</td>\n",
       "      <td>world_country.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>4.659192e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024177</td>\n",
       "      <td>-0.121284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.043013</td>\n",
       "      <td>-0.141035</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World_countries_env_vars.csv</td>\n",
       "      <td>Country</td>\n",
       "      <td>world_city.csv</td>\n",
       "      <td>District</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.177892</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>6.364830e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055464</td>\n",
       "      <td>0.020214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019129</td>\n",
       "      <td>0.053482</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>books_updated.csv</td>\n",
       "      <td>languageCode</td>\n",
       "      <td>countries_metadatacountries.csv</td>\n",
       "      <td>CountryCode</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>1.381148e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22607</th>\n",
       "      <td>pte_sulfo.csv</td>\n",
       "      <td>Set</td>\n",
       "      <td>AdventureWorks2014_shift.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22608</th>\n",
       "      <td>dataSpotifyClass.csv</td>\n",
       "      <td>song_title</td>\n",
       "      <td>netflix_titles.csv</td>\n",
       "      <td>description</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.594809</td>\n",
       "      <td>-0.525564</td>\n",
       "      <td>-14.020669</td>\n",
       "      <td>-1.844184</td>\n",
       "      <td>-0.086287</td>\n",
       "      <td>104</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22609</th>\n",
       "      <td>pte_methoxy.csv</td>\n",
       "      <td>Arg0</td>\n",
       "      <td>countries_data.csv</td>\n",
       "      <td>1997</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22610</th>\n",
       "      <td>student-mat.csv</td>\n",
       "      <td>internet</td>\n",
       "      <td>AdventureWorks2014_stateprovince.csv</td>\n",
       "      <td>Name</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>-0.040428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.040356</td>\n",
       "      <td>-0.100492</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22611</th>\n",
       "      <td>ipums_la_98-small.csv</td>\n",
       "      <td>schltype</td>\n",
       "      <td>financial_order.csv</td>\n",
       "      <td>ksymbol</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>4.544962e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22612 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dataset_name_1 attribute_name_1  \\\n",
       "0      AdventureWorks2014_stateprovince.csv             Name   \n",
       "1               Distributions_data_2016.csv     demographics   \n",
       "2                     USA_cars_datasets.csv          country   \n",
       "3              World_countries_env_vars.csv          Country   \n",
       "4                         books_updated.csv     languageCode   \n",
       "...                                     ...              ...   \n",
       "22607                         pte_sulfo.csv              Set   \n",
       "22608                  dataSpotifyClass.csv       song_title   \n",
       "22609                       pte_methoxy.csv             Arg0   \n",
       "22610                       student-mat.csv         internet   \n",
       "22611                 ipums_la_98-small.csv         schltype   \n",
       "\n",
       "                                    dataset_name_2 attribute_name_2  \\\n",
       "0                                world_country.csv             Name   \n",
       "1      Tech_sector_diversity_demographics_2016.csv    raceEthnicity   \n",
       "2                                world_country.csv             Name   \n",
       "3                                   world_city.csv         District   \n",
       "4                  countries_metadatacountries.csv      CountryCode   \n",
       "...                                            ...              ...   \n",
       "22607                 AdventureWorks2014_shift.csv             Name   \n",
       "22608                           netflix_titles.csv      description   \n",
       "22609                           countries_data.csv             1997   \n",
       "22610         AdventureWorks2014_stateprovince.csv             Name   \n",
       "22611                          financial_order.csv          ksymbol   \n",
       "\n",
       "      relationship  containment  cardinality_proportion   jaccard  \\\n",
       "0        unrelated     0.044199                0.757322  0.019417   \n",
       "1        syntactic     0.230769                0.461538  0.187500   \n",
       "2         semantic     0.500000                0.008368  0.004167   \n",
       "3        unrelated     0.053498                0.177892  0.006250   \n",
       "4        syntactic     0.360000                0.101215  0.034091   \n",
       "...            ...          ...                     ...       ...   \n",
       "22607    unrelated     0.000000                0.120000  0.000000   \n",
       "22608    unrelated     0.000000                0.313414  0.000000   \n",
       "22609    unrelated     0.000000                0.004015  0.000000   \n",
       "22610    unrelated     0.000000                0.011050  0.000000   \n",
       "22611    unrelated     0.000000                0.750000  0.125000   \n",
       "\n",
       "       multiset_jaccard       quality  ...  number_words  words_cnt_max  \\\n",
       "0              0.019048  1.381513e-03  ...     -0.001025      -0.080856   \n",
       "1              0.000186  1.041278e-05  ...      0.180148      -0.020214   \n",
       "2              0.000365  4.659192e-07  ...      0.024177      -0.121284   \n",
       "3              0.002314  6.364830e-05  ...     -0.055464       0.020214   \n",
       "4              0.000878  1.381148e-05  ...      0.097667       0.000000   \n",
       "...                 ...           ...  ...           ...            ...   \n",
       "22607          0.000000  0.000000e+00  ...      0.000248       0.000000   \n",
       "22608          0.000000  0.000000e+00  ...     -1.594809      -0.525564   \n",
       "22609          0.000000  0.000000e+00  ...     -0.225303       0.000000   \n",
       "22610          0.000000  0.000000e+00  ...      0.001498      -0.040428   \n",
       "22611          0.000072  4.544962e-06  ...     -0.032909       0.000000   \n",
       "\n",
       "       words_cnt_min  words_cnt_avg  words_cnt_sd  first_word  last_word  \\\n",
       "0           0.000000      -0.002658     -0.040543           8          7   \n",
       "1           0.000000      -0.016396     -0.055986           4         13   \n",
       "2           0.000000      -0.043013     -0.141035           8          7   \n",
       "3           0.000000       0.019129      0.053482          14          8   \n",
       "4           0.000000       0.000000      0.000000           2          2   \n",
       "...              ...            ...           ...         ...        ...   \n",
       "22607       0.000000       0.000000      0.000000          39         36   \n",
       "22608     -14.020669      -1.844184     -0.086287         104        135   \n",
       "22609       0.000000       0.000000      0.000000          22          4   \n",
       "22610       0.000000      -0.040356     -0.100492           4          6   \n",
       "22611       0.000000       0.000000      0.000000          19         12   \n",
       "\n",
       "       is_empty  is_binary  name_dist  \n",
       "0             0          0          0  \n",
       "1             0          0         12  \n",
       "2             0          1          7  \n",
       "3             0          0          7  \n",
       "4             0          0          8  \n",
       "...         ...        ...        ...  \n",
       "22607         0          0          4  \n",
       "22608         0          0          9  \n",
       "22609         0          0          4  \n",
       "22610         0          1          7  \n",
       "22611         0          0          8  \n",
       "\n",
       "[22612 rows x 45 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = pd.merge(ground_truth, distances, on=['dataset_name_1', 'dataset_name_2', 'attribute_name_1', 'attribute_name_2'])\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1736505249702,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "QK2djoYXzHGg",
    "outputId": "9e06a7cb-e845-425b-f520-bcddf3c1a12e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dataset_name_1', 'attribute_name_1', 'dataset_name_2',\n",
      "       'attribute_name_2', 'relationship', 'containment',\n",
      "       'cardinality_proportion', 'jaccard', 'multiset_jaccard', 'quality',\n",
      "       'cardinality', 'incompleteness', 'uniqueness', 'entropy',\n",
      "       'frequency_avg', 'frequency_min', 'frequency_max', 'frequency_sd',\n",
      "       'frequency_iqr', 'val_pct_min', 'val_pct_max', 'val_pct_std',\n",
      "       'constancy', 'frequency_1qo', 'frequency_2qo', 'frequency_3qo',\n",
      "       'frequency_4qo', 'frequency_5qo', 'frequency_6qo', 'frequency_7qo',\n",
      "       'freq_word_containment', 'freq_word_soundex_containment',\n",
      "       'len_max_word', 'len_min_word', 'len_avg_word', 'number_words',\n",
      "       'words_cnt_max', 'words_cnt_min', 'words_cnt_avg', 'words_cnt_sd',\n",
      "       'first_word', 'last_word', 'is_empty', 'is_binary', 'name_dist'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(joined.columns)\n",
    "\n",
    "joined.drop(['dataset_name_1', 'attribute_name_1', 'dataset_name_2', 'attribute_name_2', \n",
    "             'relationship', 'containment', 'cardinality_proportion', 'jaccard', 'multiset_jaccard'],\n",
    "             axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-hEqpoxmPwp"
   },
   "source": [
    "# Model selection\n",
    "\n",
    "Our goal is to define the best regressor model that can approximate the true value of the joinability metric defined ($MJ$ & $K$) by using profiles.\n",
    "\n",
    "We define four base models to do so: (i) all profile metrics, (ii) all profile metrics + lightweight feature selection, (iii) all profile metrics + lightweight feature selection and (iv) a \"custom\" set of features obtained after some preliminary testing.\n",
    "\n",
    "**Important note:** we have remove the \"datatype\" metrics from the profiles. These metrics measured whether the column fit into an URL, email, etc. These were removed due to their elevated cost of computation (much higher than all the other metrics combined) and given that they did not contribute to the models in meaningful ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWZf8rrtp6oH"
   },
   "source": [
    "### Model evaluation methodology\n",
    "\n",
    "We want to define a regression model. To do so, we will employ 17 base regressors (listed below) evaluated over a 10-split CV (test size = 30%) and 4 different metrics, of which we will primarily focus on the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rSdayOnY9GuS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "split = ShuffleSplit(n_splits=10, test_size=0.3, random_state=211199)\n",
    "\n",
    "def scoring(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return {\"R2\": r2_score(y, y_pred),\n",
    "            \"MAE\": mean_absolute_error(y, y_pred),\n",
    "            \"RMSE\": math.sqrt(mean_squared_error(y, y_pred)),\n",
    "            \"MedAE\": median_absolute_error(y, y_pred),}\n",
    "\n",
    "names = [\n",
    "    \"Linear Regression\",\n",
    "    \"Ridge Regression\",\n",
    "    \"Lasso Regression\",\n",
    "    \"ElasticNet Regression\",\n",
    "    \"Random Forests\",\n",
    "    \"Gradient Boosting\",\n",
    "    \"AdaBoost\",\n",
    "    \"Extra Trees\",\n",
    "    \"Histogram Gradient Boosting\",\n",
    "    \"XGBoosting\",\n",
    "    \"Light GBM\",\n",
    "    \"CatBoost\",\n",
    "    \"MLP Relu\",\n",
    "    \"MLP logistic\",\n",
    "    \"MLP Tanh\",\n",
    "    \"SVR Poly\",\n",
    "    \"SVR Rbf\"\n",
    "]\n",
    "\n",
    "regressors = [\n",
    "    LinearRegression(),\n",
    "    Ridge(random_state=211199),\n",
    "    Lasso(random_state=211199),\n",
    "    ElasticNet(random_state=211199),\n",
    "    RandomForestRegressor(random_state=211199, n_estimators=50),\n",
    "    GradientBoostingRegressor(random_state=211199, n_estimators=50),\n",
    "    AdaBoostRegressor(random_state=211199, n_estimators=50),\n",
    "    ExtraTreesRegressor(random_state=211199, n_estimators=50),\n",
    "    HistGradientBoostingRegressor(random_state=211199),\n",
    "    XGBRegressor(random_state=211199, n_estimators=50),\n",
    "    LGBMRegressor(random_state=211199, n_estimators=50, verbose=0),\n",
    "    CatBoostRegressor(random_state=211199, verbose=0),\n",
    "    MLPRegressor(random_state=211199, activation = 'relu'),\n",
    "    MLPRegressor(random_state=211199, activation = 'logistic'),\n",
    "    MLPRegressor(random_state=211199, activation = 'tanh'),\n",
    "    SVR(kernel=\"poly\"),\n",
    "    SVR(kernel=\"rbf\"),\n",
    "]\n",
    "\n",
    "def test_list_of_regressors(predictors, target, names, regressors):\n",
    "  for name, regressor in zip(names, regressors):\n",
    "    print(name)\n",
    "    test_regressor(regressor, predictors, target)\n",
    "\n",
    "def test_regressor(regressor, predictors, target):\n",
    "  scores = cross_validate(regressor, predictors, target, cv=split, scoring=scoring, verbose=0)\n",
    "  print(f\"Fit time: {scores['fit_time'].mean():.6f} | \"\n",
    "      f\"Score time: {scores['score_time'].mean():.6f}\")\n",
    "\n",
    "  print(f\"R2: {scores['test_R2'].mean():.6f} | \"\n",
    "      f\"MAE: {scores['test_MAE'].mean():.6f} | \"\n",
    "      f\"RMSE: {scores['test_RMSE'].mean():.6f} | \"\n",
    "      f\"MedAE: {scores['test_MedAE'].mean():.6f}\")\n",
    "  print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the regressors below are the ones that perform best in average. Hence, we define some functions to directly store the models associated with these base regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = 'C:/Projects/results_freyja/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_names = [\n",
    "        \"gradient_boosting\",\n",
    "        \"extra_trees\",\n",
    "        \"xgboosting\",\n",
    "        \"catboost\"\n",
    "    ]\n",
    "\n",
    "best_regressors = [\n",
    "    GradientBoostingRegressor(random_state=211199, n_estimators=50),\n",
    "    ExtraTreesRegressor(random_state=211199, n_estimators=50),\n",
    "    XGBRegressor(random_state=211199, n_estimators=50),\n",
    "    CatBoostRegressor(random_state=211199, verbose=0),\n",
    "]\n",
    "\n",
    "def store_models(target, predictors, model_typology):\n",
    "    for name, regressor in tqdm(zip(best_names, best_regressors), total=len(best_names)):\n",
    "        regressor.fit(predictors, target)\n",
    "        folder_path = Path(f\"{models_path}/{model_typology}\")\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(regressor, folder_path / f\"{name}_{model_typology}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvxm9qE3qJGI"
   },
   "source": [
    "### Model 1: All metrics\n",
    "\n",
    "Model that includes all metrics (likely to overfit and contain redundancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 884473,
     "status": "ok",
     "timestamp": 1719936430793,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "65h-CwQxEtkh",
    "outputId": "5ad7db70-3b28-486e-b966-da5e0ff13482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Fit time: 0.015084 | Score time: 0.003200\n",
      "R2: 0.245259 | MAE: 0.007351 | RMSE: 0.030250 | MedAE: 0.001155\n",
      "------------------------------\n",
      "Ridge Regression\n",
      "Fit time: 0.009687 | Score time: 0.003168\n",
      "R2: 0.245202 | MAE: 0.007344 | RMSE: 0.030251 | MedAE: 0.001145\n",
      "------------------------------\n",
      "Lasso Regression\n",
      "Fit time: 0.011201 | Score time: 0.003002\n",
      "R2: -0.000131 | MAE: 0.008125 | RMSE: 0.034828 | MedAE: 0.004196\n",
      "------------------------------\n",
      "ElasticNet Regression\n",
      "Fit time: 0.010195 | Score time: 0.003102\n",
      "R2: -0.000131 | MAE: 0.008125 | RMSE: 0.034828 | MedAE: 0.004196\n",
      "------------------------------\n",
      "Random Forests\n",
      "Fit time: 21.715015 | Score time: 0.048598\n",
      "R2: 0.880686 | MAE: 0.001303 | RMSE: 0.011876 | MedAE: 0.000002\n",
      "------------------------------\n",
      "Gradient Boosting\n",
      "Fit time: 4.717526 | Score time: 0.005962\n",
      "R2: 0.834697 | MAE: 0.002286 | RMSE: 0.014073 | MedAE: 0.000063\n",
      "------------------------------\n",
      "AdaBoost\n",
      "Fit time: 0.475718 | Score time: 0.007401\n",
      "R2: 0.752848 | MAE: 0.003422 | RMSE: 0.017281 | MedAE: 0.000407\n",
      "------------------------------\n",
      "Extra Trees\n",
      "Fit time: 2.976416 | Score time: 0.053000\n",
      "R2: 0.901158 | MAE: 0.001221 | RMSE: 0.010762 | MedAE: 0.000002\n",
      "------------------------------\n",
      "Histogram Gradient Boosting\n",
      "Fit time: 0.359368 | Score time: 0.008900\n",
      "R2: 0.888570 | MAE: 0.001525 | RMSE: 0.011496 | MedAE: 0.000057\n",
      "------------------------------\n",
      "XGBoosting\n",
      "Fit time: 0.122606 | Score time: 0.011619\n",
      "R2: 0.887489 | MAE: 0.001447 | RMSE: 0.011533 | MedAE: 0.000059\n",
      "------------------------------\n",
      "Light GBM\n",
      "Fit time: 0.063807 | Score time: 0.004797\n",
      "R2: 0.885940 | MAE: 0.001494 | RMSE: 0.011608 | MedAE: 0.000058\n",
      "------------------------------\n",
      "CatBoost\n",
      "Fit time: 2.883288 | Score time: 0.005601\n",
      "R2: 0.902826 | MAE: 0.001413 | RMSE: 0.010665 | MedAE: 0.000149\n",
      "------------------------------\n",
      "MLP Relu\n",
      "Fit time: 1.408369 | Score time: 0.005435\n",
      "R2: -35.535527 | MAE: 0.058777 | RMSE: 0.145505 | MedAE: 0.031236\n",
      "------------------------------\n",
      "MLP logistic\n",
      "Fit time: 1.211128 | Score time: 0.029555\n",
      "R2: 0.236199 | MAE: 0.011062 | RMSE: 0.030424 | MedAE: 0.005463\n",
      "------------------------------\n",
      "MLP Tanh\n",
      "Fit time: 0.924036 | Score time: 0.009300\n",
      "R2: -0.316655 | MAE: 0.022480 | RMSE: 0.039924 | MedAE: 0.013515\n",
      "------------------------------\n",
      "SVR Poly\n",
      "Fit time: 0.129302 | Score time: 0.029697\n",
      "R2: -7.524210 | MAE: 0.099705 | RMSE: 0.101480 | MedAE: 0.100199\n",
      "------------------------------\n",
      "SVR Rbf\n",
      "Fit time: 0.237390 | Score time: 0.119323\n",
      "R2: -7.067989 | MAE: 0.097048 | RMSE: 0.098740 | MedAE: 0.097497\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_MJ = joined['quality']\n",
    "predictors = joined.drop(columns=['quality'], axis=1)\n",
    "\n",
    "test_list_of_regressors(predictors, y_MJ, names, regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24557,
     "status": "ok",
     "timestamp": 1736505300961,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "UoOTDQT3RCD8",
    "outputId": "0c7de7ff-81e0-4ed7-9a5d-7078fdcaed1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:14<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "target = joined['quality']\n",
    "predictors = joined.drop(columns=['quality'], axis=1)\n",
    "\n",
    "store_models(target, predictors, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2PKzyskqn8O"
   },
   "source": [
    "### Model 2: All metrics + Lightweight Feature Selection\n",
    "\n",
    "Model 1 + lightweight feature selection process to reduce overfitting and redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1736505312623,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -60
    },
    "id": "sYV37nOWt30H",
    "outputId": "00ad5623-af82-483e-9c2c-bf8b143a942d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features for gradient_boosting -> Original = 35, new = 19\n",
      "gradient_boosting\n",
      "Fit time: 2.621250 | Score time: 0.005131\n",
      "R2: 0.835561 | MAE: 0.002280 | RMSE: 0.014037 | MedAE: 0.000065\n",
      "------------------------------\n",
      "Number of features for extra_trees -> Original = 35, new = 33\n",
      "extra_trees\n",
      "Fit time: 2.975584 | Score time: 0.054783\n",
      "R2: 0.901720 | MAE: 0.001226 | RMSE: 0.010735 | MedAE: 0.000002\n",
      "------------------------------\n",
      "Number of features for xgboosting -> Original = 35, new = 31\n",
      "xgboosting\n",
      "Fit time: 0.123266 | Score time: 0.011395\n",
      "R2: 0.888968 | MAE: 0.001443 | RMSE: 0.011468 | MedAE: 0.000064\n",
      "------------------------------\n",
      "Number of features for catboost -> Original = 35, new = 33\n",
      "catboost\n",
      "Fit time: 3.008903 | Score time: 0.005605\n",
      "R2: 0.900992 | MAE: 0.001417 | RMSE: 0.010765 | MedAE: 0.000154\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, regressor in zip(best_names, best_regressors):\n",
    "  model_all = joblib.load(f\"{models_path}/all/{name}_all.pkl\")\n",
    "\n",
    "  feature_importances = model_all.feature_importances_\n",
    "  predictors = joined.drop(columns=['quality'], axis=1)\n",
    "\n",
    "  # Match feature importances with corresponding feature names\n",
    "  feature_names = list(predictors.columns)\n",
    "  feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "  # Sort the feature importances in descending order\n",
    "  sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  selected_metrics_fs_all = []\n",
    "  for feature, importance in sorted_feature_importances:\n",
    "    if importance > 0.0001:\n",
    "      selected_metrics_fs_all.append(feature)\n",
    "\n",
    "  target = joined['quality']\n",
    "  predictors = joined[selected_metrics_fs_all]\n",
    "  print(f\"Number of features for {name} -> Original = {len(feature_names)}, new = {len(selected_metrics_fs_all)}\")\n",
    "  test_list_of_regressors(predictors, target, [name], [regressor])\n",
    "\n",
    "  model = regressor.fit(predictors, target)\n",
    "  folder_path = Path(f\"{models_path}/all_fs_simple\")\n",
    "  folder_path.mkdir(parents=True, exist_ok=True)\n",
    "  joblib.dump(regressor, f\"{folder_path}/{name}_all_fs_simple.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: In-depth feature selection\n",
    "\n",
    "Model 1 + multi-layer feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_regression, RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def feature_selection_pipeline(dataset, model_typology_name):\n",
    "    y = dataset[\"quality\"]\n",
    "    X = dataset.drop(columns=[\"quality\"], axis=1)\n",
    "    original_features = X.columns.tolist()\n",
    "\n",
    "    print(f\"Original feature count: {len(original_features)}\")\n",
    "\n",
    "    # 1.1 Variance Threshold\n",
    "    var_thresh = VarianceThreshold(threshold = 0.01)\n",
    "    var_thresh.fit(X)\n",
    "    low_variance_removed = X.columns[~var_thresh.get_support()].tolist()\n",
    "    X = X[X.columns[var_thresh.get_support()]]\n",
    "    print(f\"Removed low-variance features: {low_variance_removed}\")\n",
    "    print(f\"Remaining features: {len(X.columns)}\")\n",
    "\n",
    "    # 1.2 Correlation Filter\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_removed = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "    X = X.drop(columns=high_corr_removed)\n",
    "    print(f\"Removed highly correlated features: {high_corr_removed}\")\n",
    "    print(f\"Remaining features: {len(X.columns)}\")\n",
    "\n",
    "    # 1.3 Mutual Information\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "    low_mi_removed = mi_scores[mi_scores < 0.01].index.tolist()\n",
    "    X_post_filter_methods = X[mi_scores[mi_scores >= 0.01].index]\n",
    "    print(f\"Removed low mutual information features: {low_mi_removed}\")\n",
    "    print(f\"Remaining features: {len(X_post_filter_methods.columns)}\")\n",
    "\n",
    "    # Utility function for importance + RFECV steps\n",
    "    def run_model_selection(model, model_name):\n",
    "        nonlocal X_post_filter_methods, y\n",
    "\n",
    "        # Feature Importance Filter\n",
    "        model.fit(X_post_filter_methods, y)\n",
    "        feature_importances = model.feature_importances_\n",
    "        feature_names = list(X_post_filter_methods.columns)\n",
    "        feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "        sorted_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        selected_metrics = [f for f, imp in sorted_importances if imp > 0.0001]\n",
    "\n",
    "        X_temp = X_post_filter_methods[selected_metrics]\n",
    "        print(f\"{model_name} - Selected {len(X_temp.columns)} features after importance filtering\")\n",
    "\n",
    "        # RFECV\n",
    "        rfecv = RFECV(estimator=model, step=1, cv=KFold(5), scoring='r2',verbose=3)\n",
    "        rfecv.fit(X_temp, y)\n",
    "        selected = X_temp.columns[rfecv.support_]\n",
    "        print(f\"{model_name} - Optimal number of features: {rfecv.n_features_}\")\n",
    "        print(f\"{model_name} - Selected features: {list(selected)}\")\n",
    "\n",
    "        # Train final model\n",
    "        model.fit(X_temp[selected], y)\n",
    "        folder_path = Path(models_path) / model_typology_name\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "        model_file = folder_path / f\"{model_name}_{model_typology_name}.pkl\"\n",
    "        joblib.dump(model, model_file)\n",
    "        print(f\"Saved model to: {model_file}\")\n",
    "\n",
    "        return list(selected)\n",
    "\n",
    "    run_model_selection(GradientBoostingRegressor(random_state=21111999, n_estimators=50), \"gradient_boosting\")\n",
    "    run_model_selection(ExtraTreesRegressor(random_state=21111999, n_estimators=50), \"extra_trees\")\n",
    "    run_model_selection(XGBRegressor(random_state=211199, n_estimators=50), \"xgboosting\")\n",
    "    run_model_selection(CatBoostRegressor(random_state=211199, verbose=0), \"catboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature count: 35\n",
      "Removed low-variance features: ['is_empty']\n",
      "Remaining features: 34\n",
      "Removed highly correlated features: ['constancy', 'frequency_1qo', 'frequency_2qo', 'frequency_3qo', 'frequency_4qo', 'frequency_6qo', 'frequency_7qo', 'words_cnt_sd']\n",
      "Remaining features: 26\n",
      "Removed low mutual information features: []\n",
      "Remaining features: 26\n",
      "gradient_boosting - Selected 18 features after importance filtering\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "gradient_boosting - Optimal number of features: 7\n",
      "gradient_boosting - Selected features: ['freq_word_containment', 'frequency_min', 'frequency_avg', 'number_words', 'frequency_max', 'first_word', 'last_word']\n",
      "Saved model to: C:\\Projects\\results_freyja\\models\\all_fs_deep\\gradient_boosting_all_fs_deep.pkl\n",
      "extra_trees - Selected 25 features after importance filtering\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 25 features.\n",
      "extra_trees - Optimal number of features: 24\n",
      "extra_trees - Selected features: ['freq_word_containment', 'uniqueness', 'number_words', 'name_dist', 'freq_word_soundex_containment', 'val_pct_max', 'first_word', 'frequency_5qo', 'frequency_max', 'val_pct_std', 'frequency_avg', 'val_pct_min', 'frequency_min', 'frequency_iqr', 'last_word', 'len_max_word', 'entropy', 'len_avg_word', 'frequency_sd', 'incompleteness', 'cardinality', 'len_min_word', 'words_cnt_max', 'words_cnt_avg']\n",
      "Saved model to: C:\\Projects\\results_freyja\\models\\all_fs_deep\\extra_trees_all_fs_deep.pkl\n",
      "xgboosting - Selected 24 features after importance filtering\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "xgboosting - Optimal number of features: 6\n",
      "xgboosting - Selected features: ['frequency_min', 'freq_word_containment', 'frequency_avg', 'frequency_max', 'len_max_word', 'first_word']\n",
      "Saved model to: C:\\Projects\\results_freyja\\models\\all_fs_deep\\xgboosting_all_fs_deep.pkl\n",
      "catboost - Selected 24 features after importance filtering\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "catboost - Optimal number of features: 15\n",
      "catboost - Selected features: ['frequency_avg', 'freq_word_containment', 'frequency_min', 'first_word', 'frequency_max', 'last_word', 'number_words', 'len_max_word', 'len_avg_word', 'name_dist', 'frequency_5qo', 'cardinality', 'freq_word_soundex_containment', 'val_pct_min', 'entropy']\n",
      "Saved model to: C:\\Projects\\results_freyja\\models\\all_fs_deep\\catboost_all_fs_deep.pkl\n"
     ]
    }
   ],
   "source": [
    "feature_selection_pipeline(joined, \"all_fs_deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Custom\n",
    "Model defined via a custom set of metrics obtained after initial testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metrics_fs_no_syntactic = ['name_dist', 'frequency_max', 'uniqueness', 'first_word', 'frequency_4qo', 'freq_word_containment', 'len_avg_word', 'words_cnt_max',\n",
    "                                      'frequency_6qo', 'len_max_word', 'frequency_min', 'frequency_3qo', 'is_empty', 'frequency_iqr', 'entropy', 'val_pct_std',\n",
    "                                      'words_cnt_min', 'cardinality', 'words_cnt_sd', 'val_pct_max', 'len_min_word', 'words_cnt_avg']\n",
    "\n",
    "for name, regressor in zip(best_names, best_regressors):\n",
    "  target = joined['quality']\n",
    "  predictors = joined[selected_metrics_fs_no_syntactic]\n",
    "\n",
    "\n",
    "  model = regressor.fit(predictors, target)\n",
    "  folder_path = Path(f\"{models_path}/custom\")\n",
    "  folder_path.mkdir(parents=True, exist_ok=True)\n",
    "  joblib.dump(regressor, folder_path / f\"{name}_custom.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQHVWZM7qcZo"
   },
   "source": [
    "# Benchmark evaluation\n",
    "\n",
    "Once we have define all the models, we will evaluate each of the seven selected benchmarks with all of them, with the goal of discerning which is the best one. To do so, all the distances for all query columns of each benchmark have been obtained and stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = {} # variable to store the individual results of the benchmarks, to later decide the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.core.model.evaluate_benchmark_performance as ebp\n",
    "\n",
    "models_path = 'C:/Projects/results_freyja/models'\n",
    "\n",
    "def evaluate_models(k, step, ground_truth_path, distances_folder_path, benchmark_name):\n",
    "    results = {}\n",
    "\n",
    "    typologies = [\"all\", \"all_fs_simple\", \"all_fs_deep\", \"custom\"]\n",
    "\n",
    "    for name, type in tqdm(product(best_names, typologies), total=len(best_names)*len(typologies), desc=\"Processing models\"):\n",
    "        try:\n",
    "            config = ebp.ModelExecutionConfig(\n",
    "                k = k,\n",
    "                step = step,\n",
    "                ground_truth_path = Path(ground_truth_path),\n",
    "                distances_folder_path = Path(distances_folder_path),\n",
    "                model_path = Path(f\"{models_path}/{type}/{name}_{type}.pkl\")\n",
    "            )\n",
    "            model_execution = ebp.ModelExecution(config)\n",
    "            model_results = model_execution.evaluate_benchmark(use_tqdm=False)\n",
    "\n",
    "            results[f\"{name}_{type}\"] = model_results['precision']\n",
    "            # print(f\"Model {name}_{type} --------------\")\n",
    "            # print(f\"Precision scores: {model_results['precision']}\")\n",
    "            # print(f\"Recall scores: {model_results['recall']}\")\n",
    "            # print(f\"MAP scores: {model_results['map']}\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    # Compute average precisions\n",
    "    avg_precisions = {model_name: sum(precisions) / len(precisions) for model_name, precisions in results.items()}\n",
    "\n",
    "    # Sort models from best to worst\n",
    "    ranked_models = sorted(avg_precisions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print ranked results\n",
    "    print(\"\\n==================== MODEL RANKINGS (top 5) ====================\")\n",
    "    for rank, (model_name, avg_precision) in enumerate(ranked_models[:5], start=1):\n",
    "        print(f\"{rank:2d}. Model: {model_name:20s} | Avg Precision: {avg_precision:.4f}\")\n",
    "    \n",
    "    benchmark_results[benchmark_name] = ranked_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding the best base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oibWXdIE4oN"
   },
   "source": [
    "### Santos Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 16/16 [00:33<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_custom | Avg Precision: 0.9586\n",
      " 2. Model: gradient_boosting_all | Avg Precision: 0.9490\n",
      " 3. Model: gradient_boosting_all_fs_simple | Avg Precision: 0.9443\n",
      " 4. Model: extra_trees_all      | Avg Precision: 0.9386\n",
      " 5. Model: extra_trees_all_fs_simple | Avg Precision: 0.9369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "step = 1\n",
    "ground_truth_path = 'C:/Projects/benchmarks/santos_small/santos_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/santos_small'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, \"santos_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-x_FOB6VOWH"
   },
   "source": [
    "### TUS Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 697305,
     "status": "ok",
     "timestamp": 1722271470487,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "tKbvCZi7VP1j",
    "outputId": "4226856a-5a43-47ea-b235-9601696b4718"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 16/16 [02:28<00:00,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_custom | Avg Precision: 0.8875\n",
      " 2. Model: catboost_all_fs_deep | Avg Precision: 0.8676\n",
      " 3. Model: catboost_custom      | Avg Precision: 0.8425\n",
      " 4. Model: gradient_boosting_all | Avg Precision: 0.8356\n",
      " 5. Model: catboost_all_fs_simple | Avg Precision: 0.8272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 60\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_small/tus_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/tus_small'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, \"tus_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VM0vyNGqYXfQ"
   },
   "source": [
    "### TUS Big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2994324,
     "status": "ok",
     "timestamp": 1722275450938,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "YuKfumOJYZu0",
    "outputId": "4fd7528b-e50d-41f6-ae76-04da47cdc727"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 16/16 [08:18<00:00, 31.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_custom | Avg Precision: 0.9267\n",
      " 2. Model: gradient_boosting_all | Avg Precision: 0.9155\n",
      " 3. Model: gradient_boosting_all_fs_simple | Avg Precision: 0.9143\n",
      " 4. Model: extra_trees_all      | Avg Precision: 0.9012\n",
      " 5. Model: extra_trees_all_fs_deep | Avg Precision: 0.9011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 60\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_big/tus_big_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/tus_big'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, \"tus_big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnqQe3V23ARd"
   },
   "source": [
    "### D3L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404500,
     "status": "ok",
     "timestamp": 1722350932795,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "PC4Gfm_l3DQ8",
    "outputId": "203cd8f0-f08b-4f9c-8141-d0e36037080b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 16/16 [01:26<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_custom | Avg Precision: 0.7788\n",
      " 2. Model: catboost_custom      | Avg Precision: 0.6327\n",
      " 3. Model: catboost_all_fs_deep | Avg Precision: 0.6283\n",
      " 4. Model: xgboosting_custom    | Avg Precision: 0.6259\n",
      " 5. Model: gradient_boosting_all_fs_simple | Avg Precision: 0.6054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/d3l/d3l_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/d3l'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, \"d3l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VqebH-e7ctX"
   },
   "source": [
    "### Freyja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5114,
     "status": "ok",
     "timestamp": 1722350938831,
     "user": {
      "displayName": "Marc Maynou Yelamos",
      "userId": "15390060321136749570"
     },
     "user_tz": -120
    },
    "id": "Kwv7xFdL7a_v",
    "outputId": "9480de4c-9da3-49f0-eca5-a101b557d14d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 16/16 [00:15<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_custom | Avg Precision: 0.9624\n",
      " 2. Model: gradient_boosting_all | Avg Precision: 0.9387\n",
      " 3. Model: gradient_boosting_all_fs_simple | Avg Precision: 0.9363\n",
      " 4. Model: gradient_boosting_all_fs_deep | Avg Precision: 0.9213\n",
      " 5. Model: xgboosting_custom    | Avg Precision: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "step = 1\n",
    "ground_truth_path = 'C:/Projects/benchmarks/freyja/freyja_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/freyja'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, \"freyja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OM CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 16/16 [00:22<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_custom | Avg Precision: 0.5763\n",
      " 2. Model: catboost_custom      | Avg Precision: 0.5552\n",
      " 3. Model: extra_trees_all_fs_deep | Avg Precision: 0.5526\n",
      " 4. Model: extra_trees_all      | Avg Precision: 0.5483\n",
      " 5. Model: extra_trees_custom   | Avg Precision: 0.5448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "step = 5\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_city_government/omnimatch_city_government_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/omnimatch_city_government'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, \"omnimatch_city_government\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OM CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 16/16 [00:23<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_custom | Avg Precision: 0.5996\n",
      " 2. Model: extra_trees_all      | Avg Precision: 0.5490\n",
      " 3. Model: extra_trees_all_fs_simple | Avg Precision: 0.5444\n",
      " 4. Model: gradient_boosting_all_fs_simple | Avg Precision: 0.5439\n",
      " 5. Model: gradient_boosting_all | Avg Precision: 0.5436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "step = 5\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_culture_recreation/omnimatch_culture_recreation_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/omnimatch_culture_recreation'\n",
    "\n",
    "evaluate_models(k, step, ground_truth_path, distances_folder_path, \"omnimatch_culture_recreation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 models with average scores:\n",
      "gradient_boosting_custom            0.8128\n",
      "gradient_boosting_all               0.7505\n",
      "gradient_boosting_all_fs_simple     0.7456\n",
      "\n",
      "Scores per benchmark for top 3 models:\n",
      "\n",
      "Benchmark: santos_small\n",
      "gradient_boosting_custom            0.9585700000000001\n",
      "gradient_boosting_all               0.94899\n",
      "gradient_boosting_all_fs_simple     0.9443400000000001\n",
      "\n",
      "Benchmark: tus_small\n",
      "gradient_boosting_custom            0.8875333333333333\n",
      "gradient_boosting_all               0.8356166666666667\n",
      "gradient_boosting_all_fs_simple     0.8213666666666666\n",
      "\n",
      "Benchmark: tus_big\n",
      "gradient_boosting_custom            0.9267166666666666\n",
      "gradient_boosting_all               0.9154833333333333\n",
      "gradient_boosting_all_fs_simple     0.9142833333333334\n",
      "\n",
      "Benchmark: d3l\n",
      "gradient_boosting_custom            0.7788399999999999\n",
      "gradient_boosting_all               0.5924699999999999\n",
      "gradient_boosting_all_fs_simple     0.60544\n",
      "\n",
      "Benchmark: freyja\n",
      "gradient_boosting_custom            0.9624200000000002\n",
      "gradient_boosting_all               0.9386699999999999\n",
      "gradient_boosting_all_fs_simple     0.9363300000000001\n",
      "\n",
      "Benchmark: omnimatch_city_government\n",
      "gradient_boosting_custom            0.5762833333333334\n",
      "gradient_boosting_all               0.47905000000000003\n",
      "gradient_boosting_all_fs_simple     0.4537\n",
      "\n",
      "Benchmark: omnimatch_culture_recreation\n",
      "gradient_boosting_custom            0.5995666666666667\n",
      "gradient_boosting_all               0.5435666666666666\n",
      "gradient_boosting_all_fs_simple     0.5438500000000001\n"
     ]
    }
   ],
   "source": [
    "def obtain_best_overall_model(benchmark_results):\n",
    "    # Accumulate scores\n",
    "    scores_sum = defaultdict(float)\n",
    "    scores_count = defaultdict(int)\n",
    "\n",
    "    for benchmark, results in benchmark_results.items():\n",
    "        for model, score in results:\n",
    "            scores_sum[model] += score\n",
    "            scores_count[model] += 1\n",
    "\n",
    "    # Compute average score per model\n",
    "    avg_scores = {\n",
    "        model: scores_sum[model] / scores_count[model]\n",
    "        for model in scores_sum\n",
    "    }\n",
    "\n",
    "    # Get top 3 models\n",
    "    top_3 = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    print(\"Top 3 models with average scores:\")\n",
    "    for model, score in top_3:\n",
    "        print(f\"{model:35s} {score:.4f}\")\n",
    "\n",
    "    print(\"\\nScores per benchmark for top 3 models:\")\n",
    "    for benchmark, results in benchmark_results.items():\n",
    "        print(f\"\\nBenchmark: {benchmark}\")\n",
    "        results_dict = dict(results)\n",
    "        for model, _ in top_3:\n",
    "            score = results_dict.get(model, None)\n",
    "            print(f\"{model:35s} {score}\")\n",
    "\n",
    "obtain_best_overall_model(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "The best base model is clearly the gradient boosting, specially that with the custom metrics. The most likely explanation is its reduced overfitting, which makes it able to adapt to many benchmarks, as opposed to, for example, xgboost.\n",
    "\n",
    "Now, we will fine-tune the model to extract even more effectivenes out of the join detection process. We will generate 48 models from the initial gradient boosting, following a grid search over relevant parameters. Each model will be evaluated on each of the seven benchmarks, and we will obtain the best overall model (same as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [04:31<00:00,  5.65s/it]\n"
     ]
    }
   ],
   "source": [
    "models_path = \"C:/Projects/results_freyja/models/fine_tuning\"\n",
    "folder_path = Path(models_path)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "selected_metrics = [\n",
    "    'name_dist', 'frequency_max', 'uniqueness', 'first_word',\n",
    "    'frequency_4qo', 'freq_word_containment', 'len_avg_word',\n",
    "    'words_cnt_max', 'frequency_6qo', 'len_max_word',\n",
    "    'frequency_min', 'frequency_3qo', 'is_empty',\n",
    "    'frequency_iqr', 'entropy', 'val_pct_std',\n",
    "    'words_cnt_min', 'cardinality', 'words_cnt_sd',\n",
    "    'val_pct_max', 'len_min_word', 'words_cnt_avg'\n",
    "]\n",
    "\n",
    "predictors = joined[selected_metrics]\n",
    "target = joined['quality']\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [25, 50, 100],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"min_samples_leaf\": [1, 10]\n",
    "}\n",
    "\n",
    "# Wrap the product iterator with tqdm\n",
    "for values in tqdm(list(product(*param_grid.values())), total=len(list(product(*param_grid.values())))):\n",
    "    params = dict(zip(param_grid.keys(), values))\n",
    "\n",
    "    regressor = GradientBoostingRegressor(random_state=211199, **params)\n",
    "    regressor.fit(predictors, target)\n",
    "\n",
    "    model_name = (\n",
    "        f\"gradient_boosting_\"\n",
    "        f\"ne{params['n_estimators']}_\"\n",
    "        f\"lr{params['learning_rate']}_\"\n",
    "        f\"md{params['max_depth']}_\"\n",
    "        f\"ss{params['subsample']}_\"\n",
    "        f\"msl{params['min_samples_leaf']}.pkl\"\n",
    "    )\n",
    "\n",
    "    joblib.dump(regressor, folder_path / model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results_fine_tuning = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, benchmark_name):\n",
    "    results = {}\n",
    "    fine_tuned_models_path = 'C:/Projects/results_freyja/models/fine_tuning'\n",
    "\n",
    "    for filename in tqdm(os.listdir(fine_tuned_models_path), desc=\"Processing models\"):\n",
    "        model_name = os.path.splitext(filename)[0]\n",
    "        file_path = os.path.join(fine_tuned_models_path, filename)\n",
    "        try:\n",
    "            config = ebp.ModelExecutionConfig(\n",
    "                k = k,\n",
    "                step = step,\n",
    "                ground_truth_path = Path(ground_truth_path),\n",
    "                distances_folder_path = Path(distances_folder_path),\n",
    "                model_path = Path(file_path)\n",
    "            )\n",
    "            model_execution = ebp.ModelExecution(config)\n",
    "\n",
    "            model_results = model_execution.evaluate_benchmark(use_tqdm=False)\n",
    "            results[model_name] = model_results['precision']\n",
    "            # print(f\"Model {model_name} --------------\")\n",
    "            # print(f\"Precision scores: {model_results['precision']}\")\n",
    "            # print(f\"Recall scores: {model_results['recall']}\")\n",
    "            # print(f\"MAP scores: {model_results['map']}\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    # Compute average precisions\n",
    "    avg_precisions = {model_name: sum(precisions) / len(precisions) for model_name, precisions in results.items()}\n",
    "\n",
    "    # Sort models from best to worst\n",
    "    ranked_models = sorted(avg_precisions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print ranked results\n",
    "    print(\"\\n==================== MODEL RANKINGS (top 5) ====================\")\n",
    "    for rank, (model_name, avg_precision) in enumerate(ranked_models[:5], start=1):\n",
    "        print(f\"{rank:2d}. Model: {model_name:20s} | Avg Precision: {avg_precision:.4f}\")\n",
    "    \n",
    "    benchmark_results_fine_tuning[benchmark_name] = ranked_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:21<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_ne25_lr0.1_md3_ss1.0_msl1 | Avg Precision: 0.9719\n",
      " 2. Model: gradient_boosting_ne50_lr0.05_md3_ss1.0_msl1 | Avg Precision: 0.9714\n",
      " 3. Model: gradient_boosting_ne50_lr0.05_md3_ss1.0_msl10 | Avg Precision: 0.9712\n",
      " 4. Model: gradient_boosting_ne25_lr0.1_md3_ss1.0_msl10 | Avg Precision: 0.9706\n",
      " 5. Model: gradient_boosting_ne50_lr0.05_md3_ss0.8_msl1 | Avg Precision: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "step = 1\n",
    "ground_truth_path = 'C:/Projects/benchmarks/santos_small/santos_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/santos_small'\n",
    "\n",
    "evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, \"santos_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 48/48 [06:16<00:00,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 | Avg Precision: 0.8958\n",
      " 2. Model: gradient_boosting_ne50_lr0.05_md5_ss0.8_msl1 | Avg Precision: 0.8950\n",
      " 3. Model: gradient_boosting_ne50_lr0.05_md5_ss0.8_msl10 | Avg Precision: 0.8944\n",
      " 4. Model: gradient_boosting_ne100_lr0.1_md3_ss0.8_msl10 | Avg Precision: 0.8936\n",
      " 5. Model: gradient_boosting_ne25_lr0.1_md5_ss1.0_msl1 | Avg Precision: 0.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 60\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_small/tus_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/tus_small'\n",
    "\n",
    "evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, \"tus_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 48/48 [22:46<00:00, 28.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_ne25_lr0.1_md3_ss1.0_msl10 | Avg Precision: 0.9381\n",
      " 2. Model: gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 | Avg Precision: 0.9358\n",
      " 3. Model: gradient_boosting_ne50_lr0.05_md3_ss1.0_msl10 | Avg Precision: 0.9351\n",
      " 4. Model: gradient_boosting_ne25_lr0.1_md3_ss1.0_msl1 | Avg Precision: 0.9350\n",
      " 5. Model: gradient_boosting_ne50_lr0.05_md3_ss1.0_msl1 | Avg Precision: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 60\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_big/tus_big_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/tus_big'\n",
    "\n",
    "evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, \"tus_big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 48/48 [03:31<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 | Avg Precision: 0.8005\n",
      " 2. Model: gradient_boosting_ne50_lr0.05_md5_ss0.8_msl1 | Avg Precision: 0.7993\n",
      " 3. Model: gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 | Avg Precision: 0.7980\n",
      " 4. Model: gradient_boosting_ne50_lr0.1_md3_ss1.0_msl10 | Avg Precision: 0.7930\n",
      " 5. Model: gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 | Avg Precision: 0.7843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "step = 10\n",
    "ground_truth_path = 'C:/Projects/benchmarks/d3l/d3l_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/d3l'\n",
    "\n",
    "evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, \"d3l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 48/48 [00:34<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_ne50_lr0.1_md3_ss1.0_msl1 | Avg Precision: 0.9624\n",
      " 2. Model: gradient_boosting_ne25_lr0.1_md3_ss0.8_msl1 | Avg Precision: 0.9623\n",
      " 3. Model: gradient_boosting_ne25_lr0.1_md3_ss1.0_msl1 | Avg Precision: 0.9578\n",
      " 4. Model: gradient_boosting_ne100_lr0.05_md3_ss0.8_msl1 | Avg Precision: 0.9576\n",
      " 5. Model: gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 | Avg Precision: 0.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "step = 1\n",
    "ground_truth_path = 'C:/Projects/benchmarks/freyja/freyja_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/freyja'\n",
    "\n",
    "evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, \"freyja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 48/48 [00:53<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 | Avg Precision: 0.6111\n",
      " 2. Model: gradient_boosting_ne100_lr0.1_md3_ss0.8_msl10 | Avg Precision: 0.6018\n",
      " 3. Model: gradient_boosting_ne50_lr0.1_md3_ss0.8_msl1 | Avg Precision: 0.5990\n",
      " 4. Model: gradient_boosting_ne100_lr0.1_md3_ss1.0_msl10 | Avg Precision: 0.5947\n",
      " 5. Model: gradient_boosting_ne100_lr0.1_md3_ss1.0_msl1 | Avg Precision: 0.5942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "step = 5\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_city_government/omnimatch_city_government_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/omnimatch_city_government'\n",
    "\n",
    "evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, \"omnimatch_city_government\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 48/48 [00:53<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL RANKINGS (top 5) ====================\n",
      " 1. Model: gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 | Avg Precision: 0.6089\n",
      " 2. Model: gradient_boosting_ne100_lr0.05_md3_ss0.8_msl1 | Avg Precision: 0.6034\n",
      " 3. Model: gradient_boosting_ne50_lr0.05_md3_ss1.0_msl10 | Avg Precision: 0.5997\n",
      " 4. Model: gradient_boosting_ne50_lr0.1_md3_ss1.0_msl1 | Avg Precision: 0.5996\n",
      " 5. Model: gradient_boosting_ne25_lr0.1_md3_ss1.0_msl10 | Avg Precision: 0.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "step = 5\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_culture_recreation/omnimatch_culture_recreation_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/omnimatch_culture_recreation'\n",
    "\n",
    "evaluate_models_fine_tuned(k, step, ground_truth_path, distances_folder_path, \"omnimatch_culture_recreation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 models with average scores:\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.8176\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.8163\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.8157\n",
      "\n",
      "Scores per benchmark for top 3 models:\n",
      "\n",
      "Benchmark: santos_small\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.9577000000000002\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.9558500000000001\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.9512700000000001\n",
      "\n",
      "Benchmark: tus_small\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.8925500000000001\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.88755\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.8957833333333333\n",
      "\n",
      "Benchmark: tus_big\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.9279999999999999\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.9293166666666667\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.9357666666666667\n",
      "\n",
      "Benchmark: d3l\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.7980099999999999\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.7843100000000001\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.80046\n",
      "\n",
      "Benchmark: freyja\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.95328\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.9569899999999999\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.94967\n",
      "\n",
      "Benchmark: omnimatch_city_government\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.6111333333333332\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.5910333333333333\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.59415\n",
      "\n",
      "Benchmark: omnimatch_culture_recreation\n",
      "gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10 0.5823666666666667\n",
      "gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10 0.6088666666666667\n",
      "gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10 0.5828166666666666\n"
     ]
    }
   ],
   "source": [
    "obtain_best_overall_model(benchmark_results_fine_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gradient_boosting_ne100_lr0.05_md3_ss0.8_msl1` model is the best performing model across all benchmarks. Next, we print the specific results for this model for all benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 28.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_ne50_lr0.1_md5_ss1.0_msl10, benchmark santos_small\n",
      "Precision scores: [1.0, 0.99, 0.9667, 0.965, 0.956, 0.95, 0.9457, 0.94, 0.9356, 0.928]\n",
      "Recall scores: [1.0, 0.988, 0.968, 0.9659, 0.9583, 0.951, 0.9466, 0.9398, 0.9346, 0.9261]\n",
      "MAP scores: [1.0, 1.0, 1.0, 0.9967, 0.9961, 0.9952, 0.9931, 0.9917, 0.9909, 0.9905]\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_ne50_lr0.1_md5_ss1.0_msl10, benchmark tus_small\n",
      "Precision scores: [0.973, 0.902, 0.839, 0.8682, 0.8834, 0.8897]\n",
      "Recall scores: [0.9787, 0.9091, 0.847, 0.8758, 0.8906, 0.897]\n",
      "MAP scores: [0.9803, 0.9778, 0.9692, 0.928, 0.9155, 0.9117]\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_ne50_lr0.1_md5_ss1.0_msl10, benchmark tus_big\n",
      "Precision scores: [0.962, 0.9535, 0.935, 0.9198, 0.9042, 0.8935]\n",
      "Recall scores: [0.9151, 0.9099, 0.876, 0.8448, 0.8101, 0.7849]\n",
      "MAP scores: [0.9879, 0.9836, 0.9831, 0.9812, 0.9697, 0.9628]\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_ne50_lr0.1_md5_ss1.0_msl10, benchmark d3l\n",
      "Precision scores: [0.827, 0.8265, 0.824, 0.8108, 0.791, 0.7973, 0.7987, 0.7854, 0.7681, 0.7513]\n",
      "Recall scores: [0.8276, 0.826, 0.8218, 0.8079, 0.7877, 0.7921, 0.7914, 0.777, 0.7593, 0.7419]\n",
      "MAP scores: [0.7438, 0.8069, 0.8156, 0.8223, 0.8237, 0.8209, 0.8225, 0.8264, 0.8297, 0.8332]\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 69.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_ne50_lr0.1_md5_ss1.0_msl10, benchmark freyja\n",
      "Precision scores: [1.0, 0.98, 0.9667, 0.955, 0.948, 0.9467, 0.9457, 0.94, 0.9267, 0.924]\n",
      "Recall scores: [1.0, 0.9843, 0.9755, 0.9633, 0.9539, 0.9507, 0.949, 0.9424, 0.9276, 0.9246]\n",
      "MAP scores: [1.0, 1.0, 1.0, 0.99, 0.9853, 0.9797, 0.9764, 0.9745, 0.9736, 0.9712]\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 44.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_ne50_lr0.1_md5_ss1.0_msl10, benchmark omnimatch_city_government\n",
      "Precision scores: [0.624, 0.618, 0.5867, 0.607, 0.6184, 0.6127]\n",
      "Recall scores: [0.6226, 0.6181, 0.5868, 0.6073, 0.619, 0.6132]\n",
      "MAP scores: [0.622, 0.654, 0.6523, 0.6357, 0.6335, 0.6342]\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 43.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradient_boosting_ne50_lr0.1_md5_ss1.0_msl10, benchmark omnimatch_culture_recreation\n",
      "Precision scores: [0.508, 0.558, 0.6027, 0.604, 0.6128, 0.6087]\n",
      "Recall scores: [0.5042, 0.5559, 0.6, 0.6011, 0.6101, 0.6048]\n",
      "MAP scores: [0.5016, 0.5569, 0.5762, 0.5889, 0.598, 0.6053]\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_single_model(k, step, ground_truth_path, distances_folder_path, benchmark_name):\n",
    "    try:\n",
    "        config = ebp.ModelExecutionConfig(\n",
    "            k = k,\n",
    "            step = step,\n",
    "            ground_truth_path = Path(ground_truth_path),\n",
    "            distances_folder_path = Path(distances_folder_path),\n",
    "            model_path = Path(\"C:/Projects/FREYJA/app/core/model/gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10.pkl\")\n",
    "        )\n",
    "        model_execution = ebp.ModelExecution(config)\n",
    "\n",
    "        model_results = model_execution.evaluate_benchmark()\n",
    "        print(f\"Model {model_name}, benchmark {benchmark_name}\")\n",
    "        print(f\"Precision scores: {model_results['precision']}\")\n",
    "        print(f\"Recall scores: {model_results['recall']}\")\n",
    "        print(f\"MAP scores: {model_results['map']}\")\n",
    "        print(f\"-----------------------\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "ground_truth_path = 'C:/Projects/benchmarks/santos_small/santos_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/santos_small'\n",
    "evaluate_single_model(10, 1, ground_truth_path, distances_folder_path, \"santos_small\")\n",
    "\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_small/tus_small_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/tus_small'\n",
    "evaluate_single_model(60, 10, ground_truth_path, distances_folder_path, \"tus_small\")\n",
    "\n",
    "ground_truth_path = 'C:/Projects/benchmarks/tus_big/tus_big_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/tus_big'\n",
    "evaluate_single_model(60, 10, ground_truth_path, distances_folder_path, \"tus_big\")\n",
    "\n",
    "ground_truth_path = 'C:/Projects/benchmarks/d3l/d3l_ground_truth_sample.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/d3l'\n",
    "evaluate_single_model(100, 10, ground_truth_path, distances_folder_path, \"d3l\")\n",
    "\n",
    "ground_truth_path = 'C:/Projects/benchmarks/freyja/freyja_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/freyja'\n",
    "evaluate_single_model(10, 1, ground_truth_path, distances_folder_path, \"freyja\")\n",
    "\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_city_government/omnimatch_city_government_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/omnimatch_city_government'\n",
    "evaluate_single_model(30, 5, ground_truth_path, distances_folder_path, \"omnimatch_city_government\")\n",
    "\n",
    "ground_truth_path = 'C:/Projects/benchmarks/omnimatch_culture_recreation/omnimatch_culture_recreation_ground_truth.csv'\n",
    "distances_folder_path = 'C:/Projects/results_freyja/distances/omnimatch_culture_recreation'\n",
    "evaluate_single_model(30, 5, ground_truth_path, distances_folder_path, \"omnimatch_culture_recreation\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPphl2VZmBeT9V/QvoypH3j",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "freyja",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
